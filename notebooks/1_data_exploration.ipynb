{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rongxuan-Zhou/CS6120_project/blob/main/notebooks/enhanced_msmarco_data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced MSMARCO v1.1 Data Exploration\n",
        "\n",
        "This notebook provides a comprehensive exploration of the MS MARCO dataset v1.1, a large-scale information retrieval dataset used for text ranking and question answering tasks. The analysis focuses on understanding data characteristics essential for developing effective text retrieval systems.\n",
        "\n",
        "## Dataset Background\n",
        "\n",
        "The MS MARCO dataset was created by Microsoft Research as a Machine Reading Comprehension (MRC) dataset grounded in real-world behavior. It contains real queries sampled from Bing search logs, along with relevant passages and human-generated answers. This makes it particularly valuable for information retrieval and natural language processing tasks.\n",
        "\n",
        "**Key dataset features:**\n",
        "- Real user queries from Bing search logs (anonymized)\n",
        "- Multiple passages per query with human relevance judgments\n",
        "- Human-generated answers\n",
        "- Large scale (over 1 million queries in the full dataset)\n",
        "\n",
        "## Notebook Objectives\n",
        "\n",
        "Through this notebook, we will:\n",
        "1. Set up a robust environment for dataset exploration\n",
        "2. Load and examine the MSMARCO dataset with appropriate error handling\n",
        "3. Analyze text characteristics with visualization and statistical measures\n",
        "4. Compare corpus text features relevant to information retrieval\n",
        "5. Implement memory and performance optimizations for large-scale analysis\n",
        "6. Document insights for information retrieval system development\n",
        "\n",
        "## References\n",
        "- [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset](https://arxiv.org/abs/1611.09268)\n",
        "- [MS MARCO Official Website](https://microsoft.github.io/msmarco/)\n",
        "- [Hugging Face MS MARCO Dataset](https://huggingface.co/datasets/microsoft/ms_marco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment-setup"
      },
      "outputs": [],
      "source": [
        "# 1. Environment Setup\n",
        "# Check and install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "required_packages = {\n",
        "    'pandas': '1.3.0',\n",
        "    'matplotlib': '3.4.0',\n",
        "    'seaborn': '0.11.0',\n",
        "    'datasets': '2.0.0',\n",
        "    'psutil': '5.8.0',\n",
        "    'requests': '2.25.0',\n",
        "    'tqdm': '4.61.0',\n",
        "    'chardet': '4.0.0',  # For encoding detection\n",
        "    'plotly': '5.3.0',   # For interactive visualization\n",
        "    'nltk': '3.6.0'      # For text analysis\n",
        "}\n",
        "\n",
        "# Check installed packages and install missing ones\n",
        "installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "missing = {}\n",
        "\n",
        "print(\"Checking required packages...\")\n",
        "for package, min_version in required_packages.items():\n",
        "    if package not in installed:\n",
        "        print(f\"⚠️ {package} not found - will install\")\n",
        "        missing[package] = min_version\n",
        "    elif pkg_resources.parse_version(installed[package]) < pkg_resources.parse_version(min_version):\n",
        "        print(f\"⚠️ {package} {installed[package]} needs upgrade to {min_version}\")\n",
        "        missing[package] = min_version\n",
        "    else:\n",
        "        print(f\"✅ {package} {installed[package]}\")\n",
        "\n",
        "if missing:\n",
        "    print(\"\\nInstalling required packages...\")\n",
        "    for package, version in missing.items():\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{package}>={version}\"])\n",
        "    print(\"All required packages installed.\")\n",
        "else:\n",
        "    print(\"\\nAll required packages already installed.\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import psutil\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "import chardet\n",
        "import re\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Configure visualization settings\n",
        "plt.style.use('ggplot')\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "\n",
        "# Google Drive setup (for Colab execution)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    is_colab = True\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_PATH = \"/content/drive/MyDrive/CS6120_project\"\n",
        "except ImportError:\n",
        "    is_colab = False\n",
        "    PROJECT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
        "    print(f\"Running in local environment. Project path: {PROJECT_PATH}\")\n",
        "\n",
        "# Set up project directory structure if it doesn't exist\n",
        "os.makedirs(os.path.join(PROJECT_PATH, \"data\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(PROJECT_PATH, \"models\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(PROJECT_PATH, \"results\"), exist_ok=True)\n",
        "\n",
        "# Import custom modules if available\n",
        "SRC_PATH = os.path.join(PROJECT_PATH, \"src\")\n",
        "if os.path.exists(SRC_PATH):\n",
        "    # If running in Colab, copy to current directory for easier importing\n",
        "    if is_colab:\n",
        "        if os.path.exists(\"/content/src\"):\n",
        "            shutil.rmtree(\"/content/src\")\n",
        "        shutil.copytree(SRC_PATH, \"/content/src\")\n",
        "        sys.path.append(\"/content\")\n",
        "    else:\n",
        "        sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
        "    print(\"Successfully added source directory to path\")\n",
        "    \n",
        "    # Try to import custom modules\n",
        "    try:\n",
        "        from src.data_preparation import DataPreprocessor\n",
        "        preprocessor_available = True\n",
        "        print(\"Successfully imported DataPreprocessor\")\n",
        "    except ImportError:\n",
        "        preprocessor_available = False\n",
        "        print(\"DataPreprocessor not available - will implement data cleaning directly\")\n",
        "else:\n",
        "    preprocessor_available = False\n",
        "    print(\"Source directory not found - will implement data preprocessing directly\")\n",
        "\n",
        "# Memory monitoring\n",
        "def check_memory():\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"Memory Usage:\")\n",
        "    print(f\"  Total: {memory.total/1024**3:.2f} GB\")\n",
        "    print(f\"  Available: {memory.available/1024**3:.2f} GB\")\n",
        "    print(f\"  Used: {memory.used/1024**3:.2f} GB ({memory.percent}%)\")\n",
        "    \n",
        "    if memory.available < 2 * 1024**3:  # Less than 2GB available\n",
        "        print(\"⚠️ Warning: Low memory available - consider reducing sample size or using chunks\")\n",
        "    \n",
        "    return memory.available\n",
        "\n",
        "# Time tracking\n",
        "start_time = time.time()\n",
        "check_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "functions-definition"
      },
      "outputs": [],
      "source": [
        "# 2. Helper Functions for Data Analysis\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text, remove_urls=True, remove_html=True, keep_case=False):\n",
        "    \"\"\"Clean text by removing unwanted elements and normalizing\n",
        "    \n",
        "    Args:\n",
        "        text (str): Text to clean\n",
        "        remove_urls (bool): Whether to remove URLs from text\n",
        "        remove_html (bool): Whether to remove HTML tags\n",
        "        keep_case (bool): Whether to keep original case or convert to lowercase\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "    \n",
        "    if remove_urls:\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    \n",
        "    if remove_html:\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    if not keep_case:\n",
        "        text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Text feature extraction function\n",
        "def extract_text_features(text):\n",
        "    \"\"\"Extract various features from text for analysis\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary of extracted features\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return {\n",
        "            'length': 0,\n",
        "            'word_count': 0,\n",
        "            'avg_word_length': 0,\n",
        "            'sentence_count': 0,\n",
        "            'avg_sentence_length': 0,\n",
        "            'question_mark_count': 0,\n",
        "            'number_count': 0,\n",
        "            'special_char_count': 0\n",
        "        }\n",
        "    \n",
        "    # Basic features\n",
        "    length = len(text)\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    avg_word_length = sum(len(word) for word in words) / max(1, word_count)\n",
        "    \n",
        "    # Sentence features\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    sentence_count = len(sentences)\n",
        "    avg_sentence_length = length / max(1, sentence_count)\n",
        "    \n",
        "    # Character type features\n",
        "    question_mark_count = text.count('?')\n",
        "    number_count = sum(c.isdigit() for c in text)\n",
        "    special_char_count = sum(not c.isalnum() and not c.isspace() for c in text)\n",
        "    \n",
        "    return {\n",
        "        'length': length,\n",
        "        'word_count': word_count,\n",
        "        'avg_word_length': avg_word_length,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'question_mark_count': question_mark_count,\n",
        "        'number_count': number_count,\n",
        "        'special_char_count': special_char_count\n",
        "    }\n",
        "\n",
        "# Function to analyze dataset text\n",
        "def analyze_dataset_text(df, text_column='text', sample_size=None):\n",
        "    \"\"\"Perform comprehensive analysis on text in dataset\n",
        "    \n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing text\n",
        "        text_column (str): Column name containing text\n",
        "        sample_size (int, optional): Number of samples to analyze (for performance)\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with added text feature columns\n",
        "    \"\"\"\n",
        "    if len(df) == 0:\n",
        "        print(\"Empty dataframe provided. Nothing to analyze.\")\n",
        "        return df\n",
        "    \n",
        "    print(f\"Analyzing text features for {text_column}...\")\n",
        "    \n",
        "    if sample_size and len(df) > sample_size:\n",
        "        print(f\"Using sample of {sample_size} records from {len(df)} total\")\n",
        "        df_sample = df.sample(sample_size, random_state=42)\n",
        "    else:\n",
        "        print(f\"Analyzing all {len(df)} records\")\n",
        "        df_sample = df\n",
        "    \n",
        "    # Extract text features (with progress bar)\n",
        "    features = []\n",
        "    for text in tqdm(df_sample[text_column], desc=\"Extracting features\"):\n",
        "        features.append(extract_text_features(text))\n",
        "    \n",
        "    # Add features as new columns\n",
        "    for feature_name in features[0].keys():\n",
        "        if feature_name != 'length':  # Length is usually already present\n",
        "            df_sample[feature_name] = [feature[feature_name] for feature in features]\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(\"\\nText statistics:\")\n",
        "    stats_df = df_sample[[col for col in df_sample.columns \n",
        "                          if col in ['length', 'word_count', 'sentence_count', \n",
        "                                     'avg_word_length', 'avg_sentence_length']]].describe()\n",
        "    print(stats_df)\n",
        "    \n",
        "    return df_sample\n",
        "\n",
        "# Function to detect file encoding\n",
        "def detect_file_encoding(file_path, sample_size=10000):\n",
        "    \"\"\"Detect file encoding using chardet\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "        sample_size (int): Number of bytes to sample for detection\n",
        "        \n",
        "    Returns:\n",
        "        str: Detected encoding\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            rawdata = f.read(sample_size)\n",
        "        result = chardet.detect(rawdata)\n",
        "        return result['encoding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting encoding: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to read text file with multiple encoding fallbacks\n",
        "def read_text_file_safely(file_path):\n",
        "    \"\"\"Read text file with multiple encoding fallbacks\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "        \n",
        "    Returns:\n",
        "        list: Lines from the file\n",
        "    \"\"\"\n",
        "    # First try to detect encoding\n",
        "    detected_encoding = detect_file_encoding(file_path)\n",
        "    if detected_encoding:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=detected_encoding) as f:\n",
        "                return f.read().splitlines()\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed with detected encoding {detected_encoding}\")\n",
        "    \n",
        "    # Try common encodings\n",
        "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'ascii']\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as f:\n",
        "                return f.read().splitlines()\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    \n",
        "    # Last resort - read with errors='ignore'\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            return f.read().splitlines()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to optimize memory usage of DataFrame\n",
        "def optimize_dataframe_memory(df):\n",
        "    \"\"\"Optimize memory usage of DataFrame by choosing appropriate data types\n",
        "    \n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame to optimize\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: Optimized DataFrame\n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
        "    \n",
        "    # Optimize numeric columns\n",
        "    for col in df.select_dtypes(include=['int']):\n",
        "        col_min = df[col].min()\n",
        "        col_max = df[col].max()\n",
        "        \n",
        "        # Choose appropriate integer type\n",
        "        if col_min >= 0:\n",
        "            if col_max < 255:\n",
        "                df[col] = df[col].astype(np.uint8)\n",
        "            elif col_max < 65535:\n",
        "                df[col] = df[col].astype(np.uint16)\n",
        "            elif col_max < 4294967295:\n",
        "                df[col] = df[col].astype(np.uint32)\n",
        "        else:\n",
        "            if col_min > -128 and col_max < 127:\n",
        "                df[col] = df[col].astype(np.int8)\n",
        "            elif col_min > -32768 and col_max < 32767:\n",
        "                df[col] = df[col].astype(np.int16)\n",
        "            elif col_min > -2147483648 and col_max < 2147483647:\n",
        "                df[col] = df[col].astype(np.int32)\n",
        "    \n",
        "    # Optimize float columns\n",
        "    for col in df.select_dtypes(include=['float']):\n",
        "        df[col] = df[col].astype(np.float32)\n",
        "    \n",
        "    # Optimize object columns that could be categorical\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        if col != 'text':  # Don't convert main text to category\n",
        "            n_unique = df[col].nunique()\n",
        "            if n_unique < len(df) * 0.5:  # If less than 50% unique values\n",
        "                df[col] = df[col].astype('category')\n",
        "    \n",
        "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
        "    print(f\"Reduced by {(start_mem - end_mem) / start_mem * 100:.1f}%\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Function to compare dataframes\n",
        "def compare_datasets(df1, df2, name1=\"Dataset 1\", name2=\"Dataset 2\", common_columns=None):\n",
        "    \"\"\"Compare statistics between two datasets\n",
        "    \n",
        "    Args:\n",
        "        df1 (pandas.DataFrame): First DataFrame\n",
        "        df2 (pandas.DataFrame): Second DataFrame\n",
        "        name1 (str): Name for first DataFrame\n",
        "        name2 (str): Name for second DataFrame\n",
        "        common_columns (list): List of columns to compare (if None, uses intersection)\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with comparison statistics\n",
        "    \"\"\"\n",
        "    if common_columns is None:\n",
        "        common_columns = list(set(df1.columns) & set(df2.columns))\n",
        "        if not common_columns:\n",
        "            print(\"No common columns to compare\")\n",
        "            return None\n",
        "    \n",
        "    print(f\"Comparing {name1} and {name2} on {len(common_columns)} common columns\")\n",
        "    \n",
        "    comparison = {}\n",
        "    for col in common_columns:\n",
        "        if col in df1.columns and col in df2.columns and pd.api.types.is_numeric_dtype(df1[col]):\n",
        "            comparison[col] = {\n",
        "                f\"{name1} Mean\": df1[col].mean(),\n",
        "                f\"{name2} Mean\": df2[col].mean(),\n",
        "                f\"{name1} Median\": df1[col].median(),\n",
        "                f\"{name2} Median\": df2[col].median(),\n",
        "                f\"{name1} Std\": df1[col].std(),\n",
        "                f\"{name2} Std\": df2[col].std(),\n",
        "                \"Difference %\": ((df2[col].mean() - df1[col].mean()) / df1[col].mean() * 100) if df1[col].mean() != 0 else float('inf')\n",
        "            }\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison).T\n",
        "    return comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msmarco-loading"
      },
      "outputs": [],
      "source": [
        "# 3. MSMARCO Dataset Loading with Robust Error Handling\n",
        "\n",
        "def load_msmarco_dataset(max_samples=10000, version=\"v1.1\", timeout=300):\n",
        "    \"\"\"Load MS MARCO dataset with comprehensive error handling\n",
        "    \n",
        "    Args:\n",
        "        max_samples (int): Maximum number of samples to load\n",
        "        version (str): Dataset version to load\n",
        "        timeout (int): Timeout in seconds for dataset loading\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (msmarco_dataset, msmarco_df)\n",
        "    \"\"\"\n",
        "    # Check for cached version first\n",
        "    cache_path = os.path.join(PROJECT_PATH, \"data\", f\"msmarco_{version}_sample.csv\")\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            print(f\"Loading cached MSMARCO dataset from {cache_path}\")\n",
        "            msmarco_df = pd.read_csv(cache_path)\n",
        "            print(f\"Successfully loaded {len(msmarco_df)} records from cache\")\n",
        "            return None, msmarco_df\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load from cache: {e}\")\n",
        "    \n",
        "    # No cache, load from Hugging Face\n",
        "    print(f\"Loading MSMARCO {version} dataset from Hugging Face...\")\n",
        "    msmarco_dataset = None\n",
        "    msmarco_df = None\n",
        "    \n",
        "    try:\n",
        "        # Try different dataset configurations\n",
        "        dataset_configs = [\n",
        "            (\"ms_marco\", version),\n",
        "            (\"microsoft/ms_marco\", version),\n",
        "            (\"ms_marco\", None),\n",
        "            (\"microsoft/ms_marco\", None)\n",
        "        ]\n",
        "        \n",
        "        for dataset_name, dataset_version in dataset_configs:\n",
        "            try:\n",
        "                config_str = f\" (version: {dataset_version})\" if dataset_version else \"\"\n",
        "                print(f\"Attempting to load {dataset_name}{config_str}...\")\n",
        "                \n",
        "                # Set loading timeout\n",
        "                if dataset_version:\n",
        "                    msmarco_dataset = load_dataset(dataset_name, dataset_version)\n",
        "                else:\n",
        "                    msmarco_dataset = load_dataset(dataset_name)\n",
        "                    \n",
        "                if 'train' in msmarco_dataset and len(msmarco_dataset['train']) > 0:\n",
        "                    print(f\"Successfully loaded {dataset_name}{config_str}\")\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load {dataset_name}{config_str}: {str(e)[:100]}...\")\n",
        "                continue\n",
        "        \n",
        "        if msmarco_dataset is None:\n",
        "            raise ValueError(\"Could not load any version of MSMARCO dataset\")\n",
        "        \n",
        "        # Extract passage text samples from the dataset\n",
        "        docs = []\n",
        "        sample_size = min(max_samples, len(msmarco_dataset['train']))\n",
        "        print(f\"Extracting {sample_size} samples from dataset...\")\n",
        "        \n",
        "        for doc in tqdm(msmarco_dataset['train'].select(range(sample_size))):\n",
        "            # Handle different dataset structures\n",
        "            if 'passages' in doc and 'passage_text' in doc['passages'] and len(doc['passages']['passage_text']) > 0:\n",
        "                docs.append({\n",
        "                    'text': doc['passages']['passage_text'][0],\n",
        "                    'length': len(doc['passages']['passage_text'][0])\n",
        "                })\n",
        "            elif 'passage' in doc and len(doc['passage']) > 0:\n",
        "                docs.append({\n",
        "                    'text': doc['passage'],\n",
        "                    'length': len(doc['passage'])\n",
        "                })\n",
        "            # Add queries if available\n",
        "            if 'query' in doc:\n",
        "                docs[-1]['query'] = doc['query']\n",
        "                docs[-1]['query_length'] = len(doc['query'])\n",
        "        \n",
        "        # Create DataFrame\n",
        "        msmarco_df = pd.DataFrame(docs)\n",
        "        \n",
        "        if len(msmarco_df) == 0:\n",
        "            raise ValueError(\"No valid records extracted from dataset\")\n",
        "            \n",
        "        # Save to cache for future use\n",
        "        try:\n",
        "            print(f\"Saving dataset to cache at {cache_path}\")\n",
        "            msmarco_df.to_csv(cache_path, index=False)\n",
        "            print(\"Successfully saved to cache\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save to cache: {e}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MSMARCO dataset: {e}\")\n",
        "        # Create empty DataFrame as fallback\n",
        "        msmarco_df = pd.DataFrame(columns=['text', 'length'])\n",
        "        print(\"Created empty dataframe as fallback\")\n",
        "    \n",
        "    return msmarco_dataset, msmarco_df\n",
        "\n",
        "# Load MSMARCO dataset\n",
        "msmarco_dataset, msmarco_df = load_msmarco_dataset(max_samples=10000)\n",
        "\n",
        "# Display basic info about loaded data\n",
        "if len(msmarco_df) > 0:\n",
        "    print(\"\\nMSMARCO Dataset Statistics:\")\n",
        "    print(f\"Number of records: {len(msmarco_df)}\")\n",
        "    print(f\"Columns: {msmarco_df.columns.tolist()}\")\n",
        "    print(\"\\nText length statistics:\")\n",
        "    print(msmarco_df['length'].describe())\n",
        "    \n",
        "    # Display sample passages\n",
        "    print(\"\\nSample Passages:\")\n",
        "    for i, (_, row) in enumerate(msmarco_df.sample(3).iterrows()):\n",
        "        print(f\"Passage {i+1} ({row['length']} chars):\")\n",
        "        print(f\"  {row['text'][:100]}...\")\n",
        "        if 'query' in row:\n",
        "            print(f\"  Query: {row['query']}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"Failed to load MSMARCO dataset or no valid records found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twitter-loading"
      },
      "outputs": [],
      "source": [
        "# 4. Optional: Twitter Dataset Loading for Comparison\n",
        "# This allows comparison between MSMARCO (formal text) and social media (informal text)\n",
        "\n",
        "def load_twitter_dataset(max_samples=10000, timeout=300):\n",
        "    \"\"\"Load Twitter dataset with comprehensive error handling\n",
        "    \n",
        "    Args:\n",
        "        max_samples (int): Maximum number of samples to load\n",
        "        timeout (int): Timeout in seconds for dataset loading\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with Twitter data\n",
        "    \"\"\"\n",
        "    # Check for cached version first\n",
        "    cache_path = os.path.join(PROJECT_PATH, \"data\", \"twitter_sample.csv\")\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            print(f\"Loading cached Twitter dataset from {cache_path}\")\n",
        "            twitter_df = pd.read_csv(cache_path)\n",
        "            print(f\"Successfully loaded {len(twitter_df)} records from cache\")\n",
        "            return twitter_df\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load from cache: {e}\")\n",
        "    \n",
        "    # No cache available, try to download the dataset\n",
        "    print(\"Attempting to load Twitter dataset...\")\n",
        "    \n",
        "    try:\n",
        "        # Twitter dataset URL\n",
        "        twitter_url = \"https://archive.org/download/twitter_cikm_2010/twitter_cikm_2010.zip\"\n",
        "        twitter_zip = os.path.join(PROJECT_PATH, \"data\", \"twitter.zip\")\n",
        "        \n",
        "        if not os.path.exists(twitter_zip):\n",
        "            print(\"Downloading Twitter data...\")\n",
        "            response = requests.get(twitter_url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            with open(twitter_zip, 'wb') as f:\n",
        "                for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "                    f.write(chunk)\n",
        "        \n",
        "        # Extract the data\n",
        "        import zipfile\n",
        "        extract_dir = os.path.join(PROJECT_PATH, \"data\", \"twitter_data\")\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "        \n",
        "        print(\"Extracting Twitter data...\")\n",
        "        with zipfile.ZipFile(twitter_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        \n",
        "        # Find the extracted files\n",
        "        tweets = []\n",
        "        for root, dirs, files in os.walk(extract_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".txt\") and \"tweet\" in file.lower():\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    print(f\"Processing file: {file}\")\n",
        "                    \n",
        "                    # Read with encoding detection and fallbacks\n",
        "                    lines = read_text_file_safely(file_path)\n",
        "                    print(f\"Loaded {len(lines)} lines from {file}\")\n",
        "                    \n",
        "                    # Limit to max_samples\n",
        "                    tweets.extend(lines[:max_samples])\n",
        "                    if len(tweets) >= max_samples:\n",
        "                        tweets = tweets[:max_samples]\n",
        "                        break\n",
        "            if len(tweets) >= max_samples:\n",
        "                break\n",
        "        \n",
        "        if not tweets:\n",
        "            raise ValueError(\"No tweets found in the extracted files\")\n",
        "        \n",
        "        # Clean tweets\n",
        "        print(\"Cleaning Twitter data...\")\n",
        "        cleaned_tweets = []\n",
        "        for tweet in tqdm(tweets):\n",
        "            if isinstance(tweet, str) and tweet.strip():\n",
        "                # Use custom preprocessor if available, otherwise use local function\n",
        "                if preprocessor_available:\n",
        "                    cleaned_text = preprocessor.clean_text(tweet)\n",
        "                else:\n",
        "                    cleaned_text = clean_text(tweet)\n",
        "                cleaned_tweets.append(cleaned_text)\n",
        "        \n",
        "        # Create DataFrame\n",
        "        twitter_df = pd.DataFrame({\n",
        "            'text': cleaned_tweets,\n",
        "            'length': [len(tweet) for tweet in cleaned_tweets]\n",
        "        })\n",
        "        \n",
        "        # Save to cache for future use\n",
        "        print(f\"Saving {len(twitter_df)} tweets to cache\")\n",
        "        twitter_df.to_csv(cache_path, index=False)\n",
        "        \n",
        "        return twitter_df\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Twitter dataset: {e}\")\n",
        "        # Create empty DataFrame as fallback\n",
        "        return pd.DataFrame(columns=['text', 'length'])\n",
        "\n",
        "# Load Twitter dataset\n",
        "use_twitter_comparison = True  # Set to False to skip Twitter data loading\n",
        "\n",
        "if use_twitter_comparison:\n",
        "    twitter_df = load_twitter_dataset(max_samples=10000)\n",
        "    \n",
        "    if len(twitter_df) > 0:\n",
        "        print(\"\\nTwitter Dataset Statistics:\")\n",
        "        print(f\"Number of records: {len(twitter_df)}\")\n",
        "        print(\"\\nText length statistics:\")\n",
        "        print(twitter_df['length'].describe())\n",
        "        \n",
        "        # Display sample tweets\n",
        "        print(\"\\nSample Tweets:\")\n",
        "        for i, (_, row) in enumerate(twitter_df.sample(3).iterrows()):\n",
        "            print(f\"Tweet {i+1} ({row['length']} chars): {row['text']}\")\n",
        "else:\n",
        "    print(\"Skipping Twitter dataset comparison\")\n",
        "    twitter_df = pd.DataFrame(columns=['text', 'length'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "text-features-analysis"
      },
      "outputs": [],
      "source": [
        "# 5. Text Features Analysis\n",
        "\n",
        "# Analyze MSMARCO text features\n",
        "if len(msmarco_df) > 0:\n",
        "    # Get text features\n",
        "    print(\"Analyzing MSMARCO text features...\")\n",
        "    msmarco_analyzed = analyze_dataset_text(msmarco_df, 'text', sample_size=5000)\n",
        "    \n",
        "    # Get query features if available\n",
        "    if 'query' in msmarco_df.columns:\n",
        "        print(\"\\nAnalyzing MSMARCO query features...\")\n",
        "        msmarco_query_analyzed = analyze_dataset_text(msmarco_df, 'query', sample_size=5000)\n",
        "    \n",
        "    # Optimize memory usage\n",
        "    msmarco_analyzed = optimize_dataframe_memory(msmarco_analyzed)\n",
        "\n",
        "# Analyze Twitter text features (if available)\n",
        "if len(twitter_df) > 0:\n",
        "    print(\"\\nAnalyzing Twitter text features...\")\n",
        "    twitter_analyzed = analyze_dataset_text(twitter_df, 'text', sample_size=5000)\n",
        "    \n",
        "    # Optimize memory usage\n",
        "    twitter_analyzed = optimize_dataframe_memory(twitter_analyzed)\n",
        "\n",
        "# Compare MSMARCO and Twitter datasets if both available\n",
        "if len(msmarco_df) > 0 and len(twitter_df) > 0:\n",
        "    print(\"\\nComparing MSMARCO passages and Twitter texts:\")\n",
        "    comparison_df = compare_datasets(\n",
        "        msmarco_analyzed, \n",
        "        twitter_analyzed, \n",
        "        name1=\"MSMARCO\", \n",
        "        name2=\"Twitter\",\n",
        "        common_columns=['length', 'word_count', 'avg_word_length', \n",
        "                       'sentence_count', 'avg_sentence_length']\n",
        "    )\n",
        "    print(comparison_df)\n",
        "    \n",
        "    # Compare MSMARCO queries with Twitter if queries available\n",
        "    if 'query' in msmarco_df.columns:\n",
        "        print(\"\\nComparing MSMARCO queries and Twitter texts:\")\n",
        "        query_comparison_df = compare_datasets(\n",
        "            msmarco_query_analyzed, \n",
        "            twitter_analyzed, \n",
        "            name1=\"MSMARCO Queries\", \n",
        "            name2=\"Twitter\",\n",
        "            common_columns=['length', 'word_count', 'avg_word_length']\n",
        "        )\n",
        "        print(query_comparison_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-visualization"
      },
      "outputs": [],
      "source": [
        "# 6. Advanced Data Visualization\n",
        "\n",
        "# Set up visualization parameters\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = [14, 12]\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Function to create comprehensive visualizations\n",
        "def visualize_text_features(df, title_prefix=\"Dataset\", color='blue'):\n",
        "    \"\"\"Create comprehensive visualizations for text features\n",
        "    \n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame with text features\n",
        "        title_prefix (str): Prefix for plot titles\n",
        "        color (str): Base color for plots\n",
        "    \"\"\"\n",
        "    if len(df) == 0:\n",
        "        print(f\"No data to visualize for {title_prefix}\")\n",
        "        return\n",
        "    \n",
        "    # Main text feature plots (2x3 grid)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(f\"{title_prefix} Text Feature Analysis\", fontsize=16)\n",
        "    \n",
        "    # 1. Text Length Distribution\n",
        "    sns.histplot(df['length'], kde=True, color=color, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title(f\"{title_prefix} Text Length Distribution\")\n",
        "    axes[0, 0].set_xlabel(\"Character Count\")\n",
        "    axes[0, 0].set_ylabel(\"Frequency\")\n",
        "    \n",
        "    # 2. Word Count Distribution\n",
        "    sns.histplot(df['word_count'], kde=True, color=color, ax=axes[0, 1])\n",
        "    axes[0, 1].set_title(f\"{title_prefix} Word Count Distribution\")\n",
        "    axes[0, 1].set_xlabel(\"Word Count\")\n",
        "    axes[0, 1].set_ylabel(\"Frequency\")\n",
        "    \n",
        "    # 3. Average Word Length Distribution\n",
        "    sns.histplot(df['avg_word_length'], kde=True, color=color, ax=axes[0, 2])\n",
        "    axes[0, 2].set_title(f\"{title_prefix} Avg Word Length Distribution\")\n",
        "    axes[0, 2].set_xlabel(\"Average Word Length (chars)\")\n",
        "    axes[0, 2].set_ylabel(\"Frequency\")\n",
        "    \n",
        "    # 4. Sentence Count Distribution\n",
        "    if 'sentence_count' in df.columns:\n",
        "        sns.histplot(df['sentence_count'], kde=True, color=color, ax=axes[1, 0])\n",
        "        axes[1, 0].set_title(f\"{title_prefix} Sentence Count Distribution\")\n",
        "        axes[1, 0].set_xlabel(\"Sentence Count\")\n",
        "        axes[1, 0].set_ylabel(\"Frequency\")\n",
        "    else:\n",
        "        axes[1, 0].set_visible(False)\n",
        "    \n",
        "    # 5. Word Count vs Text Length Scatter Plot\n",
        "    axes[1, 1].scatter(df['word_count'], df['length'], alpha=0.5, color=color)\n",
        "    axes[1, 1].set_title(f\"{title_prefix} Word Count vs Text Length\")\n",
        "    axes[1, 1].set_xlabel(\"Word Count\")\n",
        "    axes[1, 1].set_ylabel(\"Text Length (chars)\")\n",
        "    \n",
        "    # Add trend line\n",
        "    z = np.polyfit(df['word_count'], df['length'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[1, 1].plot(df['word_count'], p(df['word_count']), \"--\", color='red')\n",
        "    \n",
        "    # 6. Sentence Count vs Text Length Box Plot (if available)\n",
        "    if 'sentence_count' in df.columns:\n",
        "        # Group by sentence count for box plot (limit to common sentence counts)\n",
        "        sentence_counts = df['sentence_count'].value_counts()\n",
        "        common_counts = sentence_counts[sentence_counts > 10].index.sort_values()\n",
        "        if len(common_counts) > 1:\n",
        "            # Filter for common sentence counts only\n",
        "            plot_data = df[df['sentence_count'].isin(common_counts)]\n",
        "            sns.boxplot(x='sentence_count', y='length', data=plot_data, ax=axes[1, 2], color=color)\n",
        "            axes[1, 2].set_title(f\"{title_prefix} Text Length by Sentence Count\")\n",
        "            axes[1, 2].set_xlabel(\"Sentence Count\")\n",
        "            axes[1, 2].set_ylabel(\"Text Length (chars)\")\n",
        "        else:\n",
        "            axes[1, 2].set_visible(False)\n",
        "    else:\n",
        "        axes[1, 2].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional plots for special character distributions\n",
        "    if 'question_mark_count' in df.columns and 'special_char_count' in df.columns:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        fig.suptitle(f\"{title_prefix} Special Character Analysis\", fontsize=16)\n",
        "        \n",
        "        # 1. Question Mark Count\n",
        "        question_counts = df['question_mark_count'].value_counts().sort_index()\n",
        "        question_counts = question_counts[question_counts.index < 10]  # Limit to 0-9 question marks\n",
        "        axes[0].bar(question_counts.index, question_counts.values, color=color)\n",
        "        axes[0].set_title(f\"{title_prefix} Question Mark Distribution\")\n",
        "        axes[0].set_xlabel(\"Number of Question Marks\")\n",
        "        axes[0].set_ylabel(\"Count\")\n",
        "        axes[0].set_xticks(question_counts.index)\n",
        "        \n",
        "        # 2. Special Character Count Distribution\n",
        "        sns.histplot(df['special_char_count'], kde=True, color=color, ax=axes[1])\n",
        "        axes[1].set_title(f\"{title_prefix} Special Character Count\")\n",
        "        axes[1].set_xlabel(\"Special Character Count\")\n",
        "        axes[1].set_ylabel(\"Frequency\")\n",
        "        \n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "# Create visualizations for MSMARCO dataset\n",
        "if len(msmarco_df) > 0 and 'word_count' in msmarco_analyzed.columns:\n",
        "    print(\"Generating visualizations for MSMARCO passages...\")\n",
        "    visualize_text_features(msmarco_analyzed, title_prefix=\"MSMARCO Passages\", color='blue')\n",
        "    \n",
        "    # Visualize query data if available\n",
        "    if 'query' in msmarco_df.columns:\n",
        "        print(\"Generating visualizations for MSMARCO queries...\")\n",
        "        visualize_text_features(msmarco_query_analyzed, title_prefix=\"MSMARCO Queries\", color='green')\n",
        "\n",
        "# Create visualizations for Twitter dataset (if available)\n",
        "if len(twitter_df) > 0 and 'word_count' in twitter_analyzed.columns:\n",
        "    print(\"Generating visualizations for Twitter dataset...\")\n",
        "    visualize_text_features(twitter_analyzed, title_prefix=\"Twitter\", color='red')\n",
        "\n",
        "# Create comparative visualizations if both datasets available\n",
        "if len(msmarco_df) > 0 and len(twitter_df) > 0:\n",
        "    print(\"Generating comparative visualizations...\")\n",
        "    \n",
        "    # Comparative distribution plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "    fig.suptitle(\"MSMARCO vs Twitter: Text Feature Comparison\", fontsize=16)\n",
        "    \n",
        "    # 1. Text Length Comparison\n",
        "    sns.histplot(msmarco_analyzed['length'], kde=True, color='blue', label='MSMARCO', alpha=0.5, ax=axes[0, 0])\n",
        "    sns.histplot(twitter_analyzed['length'], kde=True, color='red', label='Twitter', alpha=0.5, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title(\"Text Length Distribution Comparison\")\n",
        "    axes[0, 0].set_xlabel(\"Character Count\")\n",
        "    axes[0, 0].set_ylabel(\"Frequency\")\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # 2. Word Count Comparison\n",
        "    sns.histplot(msmarco_analyzed['word_count'], kde=True, color='blue', label='MSMARCO', alpha=0.5, ax=axes[0, 1])\n",
        "    sns.histplot(twitter_analyzed['word_count'], kde=True, color='red', label='Twitter', alpha=0.5, ax=axes[0, 1])\n",
        "    axes[0, 1].set_title(\"Word Count Distribution Comparison\")\n",
        "    axes[0, 1].set_xlabel(\"Word Count\")\n",
        "    axes[0, 1].set_ylabel(\"Frequency\")\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    # 3. Average Word Length Comparison\n",
        "    sns.histplot(msmarco_analyzed['avg_word_length'], kde=True, color='blue', label='MSMARCO', alpha=0.5, ax=axes[1, 0])\n",
        "    sns.histplot(twitter_analyzed['avg_word_length'], kde=True, color='red', label='Twitter', alpha=0.5, ax=axes[1, 0])\n",
        "    axes[1, 0].set_title(\"Average Word Length Comparison\")\n",
        "    axes[1, 0].set_xlabel(\"Average Word Length (chars)\")\n",
        "    axes[1, 0].set_ylabel(\"Frequency\")\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # 4. Box plot comparison of key metrics\n",
        "    comparison_data = pd.DataFrame({\n",
        "        'Text Length (MSMARCO)': msmarco_analyzed['length'],\n",
        "        'Text Length (Twitter)': twitter_analyzed['length'],\n",
        "        'Word Count (MSMARCO)': msmarco_analyzed['word_count'],\n",
        "        'Word Count (Twitter)': twitter_analyzed['word_count'],\n",
        "    })\n",
        "    \n",
        "    melt_data = pd.melt(comparison_data)\n",
        "    sns.boxplot(x='variable', y='value', data=melt_data, ax=axes[1, 1])\n",
        "    axes[1, 1].set_title(\"Key Metrics Box Plot Comparison\")\n",
        "    axes[1, 1].set_xlabel(\"Metric\")\n",
        "    axes[1, 1].set_ylabel(\"Value\")\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-optimization-analysis"
      },
      "outputs": [],
      "source": [
        "# 7. Memory Optimization Analysis\n",
        "\n",
        "def check_dataframe_memory(df, name):\n",
        "    \"\"\"Detailed analysis of DataFrame memory usage with optimization recommendations\n",
        "    \n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame to analyze\n",
        "        name (str): Name for the DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        dict: Memory usage statistics\n",
        "    \"\"\"\n",
        "    if len(df) == 0:\n",
        "        print(f\"DataFrame {name} is empty. No memory analysis possible.\")\n",
        "        return {}\n",
        "    \n",
        "    print(f\"Memory analysis for {name} DataFrame:\")\n",
        "    \n",
        "    # Get memory usage by column\n",
        "    mem_usage = df.memory_usage(deep=True)\n",
        "    total_mem = mem_usage.sum() / 1024**2\n",
        "    print(f\"Total memory usage: {total_mem:.2f} MB\")\n",
        "    \n",
        "    # Analyze by column\n",
        "    column_analysis = {}\n",
        "    print(\"\\nMemory usage by column:\")\n",
        "    for column, mem in mem_usage.items():\n",
        "        if column != 'Index':\n",
        "            dtype = df[column].dtype\n",
        "            mem_mb = mem / 1024**2\n",
        "            column_percent = mem / mem_usage.sum() * 100\n",
        "            print(f\"  {column}: {mem_mb:.2f} MB ({column_percent:.1f}%) - dtype: {dtype}\")\n",
        "            \n",
        "            column_analysis[column] = {\n",
        "                'memory_mb': mem_mb,\n",
        "                'percent': column_percent,\n",
        "                'dtype': str(dtype),\n",
        "                'unique_values': df[column].nunique() if pd.api.types.is_object_dtype(dtype) else None\n",
        "            }\n",
        "    \n",
        "    # Optimization recommendations\n",
        "    print(\"\\nOptimization recommendations:\")\n",
        "    recommendations = []\n",
        "    \n",
        "    # Object columns (strings)\n",
        "    for column in df.select_dtypes(include=['object']).columns:\n",
        "        unique_count = df[column].nunique()\n",
        "        total_count = len(df)\n",
        "        mem_mb = mem_usage[column] / 1024**2\n",
        "        \n",
        "        if unique_count / total_count < 0.5:\n",
        "            recommendation = f\"Convert '{column}' to category type (unique values: {unique_count}/{total_count})\"\n",
        "            print(f\"  {recommendation} - could save up to {mem_mb * 0.7:.2f} MB\")\n",
        "            recommendations.append(recommendation)\n",
        "        elif column != 'text' and mem_mb > 1.0:\n",
        "            recommendation = f\"Consider dictionary encoding for '{column}' ({mem_mb:.2f} MB)\"\n",
        "            print(f\"  {recommendation}\")\n",
        "            recommendations.append(recommendation)\n",
        "    \n",
        "    # Integer columns\n",
        "    for column in df.select_dtypes(include=['int64']).columns:\n",
        "        col_min = df[column].min()\n",
        "        col_max = df[column].max()\n",
        "        mem_mb = mem_usage[column] / 1024**2\n",
        "        \n",
        "        if col_min