{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Social Media Data Exploration and Analysis\n",
    "Implemented based on src/data_preparation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initial-setup"
   },
   "outputs": [],
   "source": [
    "# 1. Environment Setup\n",
    "!pip install -q pandas matplotlib seaborn datasets psutil requests\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy src directory to Colab\n",
    "import os\n",
    "import shutil\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/CS6120_project\"\n",
    "SRC_PATH = os.path.join(PROJECT_PATH, \"src\")\n",
    "if os.path.exists(SRC_PATH):\n",
    "    shutil.copytree(SRC_PATH, \"/content/src\")\n",
    "    import sys\n",
    "    sys.path.append(\"/content\")\n",
    "    print(\"Successfully copied src directory to Colab\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find src directory at {SRC_PATH}\")\n",
    "\n",
    "# Memory monitoring\n",
    "import psutil\n",
    "print(f\"Available memory: {psutil.virtual_memory().available/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msmarco-loading"
   },
   "outputs": [],
   "source": [
    "# 2. MSMARCO Data Loading\n",
    "try:\n",
    "    from src.data_preparation import DataPreprocessor\n",
    "except ImportError:\n",
    "    from content.src.data_preparation import DataPreprocessor\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Load MSMARCO dataset\n",
    "try:\n",
    "    print(\"Attempting to load MSMARCO dataset...\")\n",
    "    \n",
    "    # Try different dataset names/versions\n",
    "    dataset_names = [\"ms_marco\", \"microsoft/ms_marco\", \"msmarco\"]\n",
    "    versions = [\"v1.1\", \"v2.1\", \"v2.0\", None]\n",
    "    \n",
    "    for name in dataset_names:\n",
    "        for version in versions:\n",
    "            try:\n",
    "                config = f\" (version: {version})\" if version else \"\"\n",
    "                print(f\"Trying {name}{config}...\")\n",
    "                \n",
    "                if version:\n",
    "                    msmarco = load_dataset(name, version)\n",
    "                else:\n",
    "                    msmarco = load_dataset(name)\n",
    "                \n",
    "                if len(msmarco['train']) > 0:\n",
    "                    print(f\"Successfully loaded {name}{config}\")\n",
    "                    \n",
    "                    # Version-aware data processing\n",
    "                    docs = []\n",
    "                    for doc in msmarco['train'].select(range(10000)):\n",
    "                        if version == \"v1.1\":\n",
    "                            if 'passages' in doc and 'passage_text' in doc['passages'] and len(doc['passages']['passage_text']) > 0:\n",
    "                                docs.append({\n",
    "                                    'text': doc['passages']['passage_text'][0],\n",
    "                                    'length': len(doc['passages']['passage_text'][0])\n",
    "                                })\n",
    "                        elif version == \"v2.1\":\n",
    "                            if 'passage' in doc and len(doc['passage']) > 0:\n",
    "                                docs.append({\n",
    "                                    'text': doc['passage'],\n",
    "                                    'length': len(doc['passage'])\n",
    "                                })\n",
    "                    \n",
    "                    msmarco_df = pd.DataFrame(docs)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed with {name}{config}: {str(e)[:100]}\")\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    if 'msmarco_df' not in locals():\n",
    "        raise ValueError(\"Could not load any version of MSMARCO dataset\")\n",
    "    \n",
    "    print(f\"MSMARCO statistics:\\n{msmarco_df.describe()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load MSMARCO: {e}\")\n",
    "    # Fallback to local data if available\n",
    "    try:\n",
    "        local_path = os.path.join(PROJECT_PATH, \"data/msmarco_sample.csv\")\n",
    "        if os.path.exists(local_path):\n",
    "            print(\"Attempting to load local data...\")\n",
    "            msmarco_df = pd.read_csv(local_path)\n",
    "            print(\"Loaded local MSMARCO sample data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load local data: {e}\")\n",
    "    finally:\n",
    "        if 'msmarco_df' not in locals():\n",
    "            # Create empty dataframe if all else fails\n",
    "            msmarco_df = pd.DataFrame(columns=['text', 'length'])\n",
    "            print(\"Created empty dataframe as fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twitter-loading"
   },
   "outputs": [],
   "source": [
    "# 3. Twitter Data Loading\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to load Twitter dataset...\")\n",
    "    \n",
    "    # Download Twitter data\n",
    "    twitter_url = \"https://archive.org/download/twitter_cikm_2010/twitter_cikm_2010.zip\"\n",
    "    twitter_zip = \"/content/twitter.zip\"\n",
    "    \n",
    "    if not os.path.exists(twitter_zip):\n",
    "        print(\"Downloading Twitter data...\")\n",
    "        response = requests.get(twitter_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(twitter_zip, 'wb') as f:\n",
    "            for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "                f.write(chunk)\n",
    "    \n",
    "    # Extract and process tweets\n",
    "    print(\"Extracting Twitter data...\")\n",
    "    with zipfile.ZipFile(twitter_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"/content/twitter_data\")\n",
    "    \n",
    "    # Process each file in the extracted directory\n",
    "    tweets = []\n",
    "    for file in os.listdir(\"/content/twitter_data/twitter_cikm_2010\"):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(\"/content/twitter_data/twitter_cikm_2010\", file), 'r', encoding='utf-8') as f:\n",
    "                tweets.extend(f.read().splitlines())\n",
    "    \n",
    "    # Clean and create dataframe\n",
    "    print(\"Cleaning Twitter data...\")\n",
    "    cleaned_tweets = [preprocessor.clean_text(tweet) for tweet in tweets[:10000]]  # Limit to 10k tweets\n",
    "    twitter_df = pd.DataFrame({\n",
    "        'text': cleaned_tweets,\n",
    "        'length': [len(tweet) for tweet in cleaned_tweets]\n",
    "    })\n",
    "    \n",
    "    print(f\"Twitter statistics:\\n{twitter_df.describe()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Twitter data: {e}\")\n",
    "    # Fallback to empty dataframe\n",
    "    twitter_df = pd.DataFrame(columns=['text', 'length'])\n",
    "    print(\"Created empty Twitter dataframe as fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-visualization"
   },
   "outputs": [],
   "source": [
    "# 4. Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# MSMARCO text length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(msmarco_df['length'], bins=50, color='blue')\n",
    "plt.title('MSMARCO Text Length')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Twitter text length distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(twitter_df['length'], bins=50, color='green')\n",
    "plt.title('Twitter Text Length')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory-optimization"
   },
   "outputs": [],
   "source": [
    "# 5. Memory Optimization Check\n",
    "def check_memory_usage(df, name):\n",
    "    mem_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{name} memory usage: {mem_usage:.2f} MB\")\n",
    "    if mem_usage > 500:  # Warning if exceeds 500MB\n",
    "        print(f\"Warning: High memory usage for {name}, consider optimizing data types\")\n",
    "        \n",
    "check_memory_usage(msmarco_df, \"MSMARCO\")\n",
    "check_memory_usage(twitter_df, \"Twitter\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
