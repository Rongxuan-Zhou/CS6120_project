{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YourUsername/YourRepo/blob/main/notebooks/msmarco_data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MSMARCO v1.1 Data Exploration\n",
        "\n",
        "This notebook explores the MS MARCO dataset v1.1, which is a large-scale information retrieval dataset used for text ranking and question answering tasks.\n",
        "\n",
        "Through this notebook, we will:\n",
        "1. Load the MSMARCO dataset\n",
        "2. Explore basic features and statistics of the data\n",
        "3. Analyze text length distribution\n",
        "4. Perform data visualization\n",
        "5. Check memory optimization strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initial-setup"
      },
      "outputs": [],
      "source": [
        "# 1. Environment Setup\n",
        "!pip install -q pandas matplotlib seaborn datasets psutil requests\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy project source code to Colab\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/MSMARCO_Project\"\n",
        "SRC_PATH = os.path.join(PROJECT_PATH, \"src\")\n",
        "if os.path.exists(SRC_PATH):\n",
        "    shutil.copytree(SRC_PATH, \"/content/src\")\n",
        "    sys.path.append(\"/content\")\n",
        "    print(\"Successfully copied src directory to Colab\")\n",
        "else:\n",
        "    # If source directory doesn't exist, create basic structure\n",
        "    os.makedirs(PROJECT_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.join(PROJECT_PATH, \"data\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(PROJECT_PATH, \"src\"), exist_ok=True)\n",
        "    print(f\"Created basic project structure at {PROJECT_PATH}\")\n",
        "\n",
        "# Memory monitoring\n",
        "import psutil\n",
        "print(f\"Available memory: {psutil.virtual_memory().available/1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msmarco-loading"
      },
      "outputs": [],
      "source": [
        "# 2. MSMARCO Dataset Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "try:\n",
        "    # Try to load DataPreprocessor class if it exists\n",
        "    try:\n",
        "        from src.data_preparation import DataPreprocessor\n",
        "        preprocessor = DataPreprocessor()\n",
        "    except ImportError:\n",
        "        print(\"Could not import DataPreprocessor, will process data directly\")\n",
        "\n",
        "    print(\"Attempting to load MSMARCO dataset...\")\n",
        "    \n",
        "    # Load MSMARCO v1.1 dataset\n",
        "    msmarco = load_dataset(\"ms_marco\", \"v1.1\")\n",
        "    print(f\"Successfully loaded MSMARCO v1.1\")\n",
        "    \n",
        "    # View basic dataset information\n",
        "    print(\"\\nMSMARCO dataset structure:\")\n",
        "    print(msmarco)\n",
        "    \n",
        "    # Take samples for analysis to avoid processing the entire dataset\n",
        "    sample_size = 10000\n",
        "    docs = []\n",
        "    for doc in msmarco['train'].select(range(sample_size)):\n",
        "        if 'passages' in doc and 'passage_text' in doc['passages'] and len(doc['passages']['passage_text']) > 0:\n",
        "            docs.append({\n",
        "                'text': doc['passages']['passage_text'][0],\n",
        "                'length': len(doc['passages']['passage_text'][0])\n",
        "            })\n",
        "    \n",
        "    msmarco_df = pd.DataFrame(docs)\n",
        "    print(f\"\\nSample size: {len(msmarco_df)} records\")\n",
        "    print(f\"\\nMSMARCO Statistics:\\n{msmarco_df.describe()}\")\n",
        "    \n",
        "    # Display a few examples\n",
        "    print(\"\\nText examples:\")\n",
        "    for idx, row in msmarco_df.head(3).iterrows():\n",
        "        print(f\"Text {idx+1} (length {row['length']}): {row['text'][:100]}...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to load MSMARCO: {e}\")\n",
        "    # Create empty dataframe as fallback\n",
        "    msmarco_df = pd.DataFrame(columns=['text', 'length'])\n",
        "    print(\"Created empty dataframe as fallback\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "query-analysis"
      },
      "outputs": [],
      "source": [
        "# 3. Query Analysis (if available)\n",
        "try:\n",
        "    # Check if query data is available\n",
        "    if 'queries' in msmarco['train'].features:\n",
        "        # Get query samples\n",
        "        queries = []\n",
        "        for doc in msmarco['train'].select(range(sample_size)):\n",
        "            if 'query' in doc:\n",
        "                queries.append({\n",
        "                    'query': doc['query'],\n",
        "                    'length': len(doc['query'])\n",
        "                })\n",
        "        \n",
        "        query_df = pd.DataFrame(queries)\n",
        "        print(f\"\\nQuery sample size: {len(query_df)} records\")\n",
        "        print(f\"\\nQuery statistics:\\n{query_df.describe()}\")\n",
        "        \n",
        "        # Display a few query examples\n",
        "        print(\"\\nQuery examples:\")\n",
        "        for idx, row in query_df.head(5).iterrows():\n",
        "            print(f\"Query {idx+1} (length {row['length']}): {row['query']}\")\n",
        "    else:\n",
        "        print(\"\\nQuery data not available or stored in a different location\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nFailed to load queries: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization"
      },
      "outputs": [],
      "source": [
        "# 4. Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 4.1 Text Length Distribution\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(msmarco_df['length'], bins=50, kde=True)\n",
        "plt.title('MSMARCO Text Length Distribution')\n",
        "plt.xlabel('Character Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# 4.2 Text Length Box Plot\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.boxplot(y=msmarco_df['length'])\n",
        "plt.title('MSMARCO Text Length Box Plot')\n",
        "plt.ylabel('Character Count')\n",
        "\n",
        "# 4.3 Text Length Kernel Density Estimation\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.kdeplot(msmarco_df['length'], fill=True)\n",
        "plt.title('MSMARCO Text Length Density')\n",
        "plt.xlabel('Character Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# 4.4 Text Length Cumulative Distribution\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.hist(msmarco_df['length'], bins=50, density=True, cumulative=True, histtype='step')\n",
        "plt.title('MSMARCO Text Length Cumulative Distribution')\n",
        "plt.xlabel('Character Count')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Query length analysis (if available)\n",
        "try:\n",
        "    if 'query_df' in locals() and len(query_df) > 0:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # Query length distribution\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.histplot(query_df['length'], bins=30, kde=True)\n",
        "        plt.title('MSMARCO Query Length Distribution')\n",
        "        plt.xlabel('Character Count')\n",
        "        plt.ylabel('Frequency')\n",
        "        \n",
        "        # Query length box plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.boxplot(y=query_df['length'])\n",
        "        plt.title('MSMARCO Query Length Box Plot')\n",
        "        plt.ylabel('Character Count')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "except NameError:\n",
        "    print(\"Query data not available, skipping query visualization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "word-length-analysis"
      },
      "outputs": [],
      "source": [
        "# 5. Word Length Analysis\n",
        "# Calculate average word length and word count for each document\n",
        "def analyze_text_words(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return {'word_count': 0, 'avg_word_length': 0}\n",
        "    \n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    if word_count == 0:\n",
        "        return {'word_count': 0, 'avg_word_length': 0}\n",
        "    \n",
        "    total_length = sum(len(word) for word in words)\n",
        "    avg_length = total_length / word_count\n",
        "    return {'word_count': word_count, 'avg_word_length': avg_length}\n",
        "\n",
        "try:\n",
        "    if len(msmarco_df) > 0:\n",
        "        word_analysis = [analyze_text_words(text) for text in msmarco_df['text']]\n",
        "        msmarco_df['word_count'] = [item['word_count'] for item in word_analysis]\n",
        "        msmarco_df['avg_word_length'] = [item['avg_word_length'] for item in word_analysis]\n",
        "        \n",
        "        # Display word statistics\n",
        "        print(\"Word Statistics:\")\n",
        "        print(f\"Word Count Statistics:\\n{msmarco_df['word_count'].describe()}\")\n",
        "        print(f\"\\nAverage Word Length Statistics:\\n{msmarco_df['avg_word_length'].describe()}\")\n",
        "        \n",
        "        # Visualization\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # Word count distribution\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.histplot(msmarco_df['word_count'], bins=40, kde=True)\n",
        "        plt.title('MSMARCO Document Word Count Distribution')\n",
        "        plt.xlabel('Word Count')\n",
        "        plt.ylabel('Frequency')\n",
        "        \n",
        "        # Average word length distribution\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.histplot(msmarco_df['avg_word_length'], bins=40, kde=True)\n",
        "        plt.title('MSMARCO Average Word Length Distribution')\n",
        "        plt.xlabel('Average Word Length (characters)')\n",
        "        plt.ylabel('Frequency')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Word analysis failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-optimization"
      },
      "outputs": [],
      "source": [
        "# 6. Memory Optimization Check\n",
        "def check_memory_usage(df, name):\n",
        "    if len(df) == 0:\n",
        "        print(f\"{name} is empty, no memory usage\")\n",
        "        return\n",
        "    \n",
        "    mem_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"{name} memory usage: {mem_usage:.2f} MB\")\n",
        "    \n",
        "    # Check memory usage for each column\n",
        "    print(f\"\\n{name} column memory usage:\")\n",
        "    for col in df.columns:\n",
        "        col_mem = df[col].memory_usage(deep=True) / 1024**2\n",
        "        print(f\"- {col}: {col_mem:.2f} MB\")\n",
        "    \n",
        "    # Memory optimization suggestions\n",
        "    print(\"\\nOptimization suggestions:\")\n",
        "    \n",
        "    # Check numeric columns\n",
        "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
        "        current_type = df[col].dtype\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        \n",
        "        # Recommend appropriate data type for integer columns\n",
        "        if pd.api.types.is_integer_dtype(current_type):\n",
        "            if min_val >= 0:\n",
        "                if max_val < 256:\n",
        "                    suggested_type = 'uint8'\n",
        "                elif max_val < 65536:\n",
        "                    suggested_type = 'uint16'\n",
        "                else:\n",
        "                    suggested_type = 'uint32'\n",
        "            else:\n",
        "                if min_val > -128 and max_val < 128:\n",
        "                    suggested_type = 'int8'\n",
        "                elif min_val > -32768 and max_val < 32768:\n",
        "                    suggested_type = 'int16'\n",
        "                else:\n",
        "                    suggested_type = 'int32'\n",
        "            \n",
        "            if current_type != suggested_type:\n",
        "                print(f\"Column '{col}' can be converted from {current_type} to {suggested_type}\")\n",
        "    \n",
        "    # Check if text columns can be categorized\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        if unique_count < len(df) * 0.5:  # If unique values are less than 50% of total rows\n",
        "            print(f\"Column '{col}' could be converted to category type (unique values: {unique_count})\")\n",
        "\n",
        "# Check MSMARCO dataframe\n",
        "check_memory_usage(msmarco_df, \"MSMARCO\")\n",
        "\n",
        "# Perform some optimizations to demonstrate effect\n",
        "if len(msmarco_df) > 0:\n",
        "    # Optimize integer columns\n",
        "    for col in msmarco_df.select_dtypes(include=['int']).columns:\n",
        "        if msmarco_df[col].min() >= 0:\n",
        "            if msmarco_df[col].max() < 256:\n",
        "                msmarco_df[col] = msmarco_df[col].astype('uint8')\n",
        "            elif msmarco_df[col].max() < 65536:\n",
        "                msmarco_df[col] = msmarco_df[col].astype('uint16')\n",
        "    \n",
        "    # Display optimization results\n",
        "    print(\"\\nAfter applying optimizations:\")\n",
        "    check_memory_usage(msmarco_df, \"Optimized MSMARCO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample-exploration"
      },
      "outputs": [],
      "source": [
        "# 7. Explore Text Features\n",
        "if len(msmarco_df) > 0:\n",
        "    # Create more text features\n",
        "    msmarco_df['sentence_count'] = msmarco_df['text'].apply(lambda x: len(x.split('.')) if isinstance(x, str) else 0)\n",
        "    msmarco_df['question_mark_count'] = msmarco_df['text'].apply(lambda x: x.count('?') if isinstance(x, str) else 0)\n",
        "    msmarco_df['number_count'] = msmarco_df['text'].apply(lambda x: sum(c.isdigit() for c in x) if isinstance(x, str) else 0)\n",
        "    \n",
        "    # Display statistics\n",
        "    print(\"Additional Text Feature Statistics:\")\n",
        "    print(f\"Sentence Count Statistics:\\n{msmarco_df['sentence_count'].describe()}\")\n",
        "    print(f\"\\nQuestion Mark Count Statistics:\\n{msmarco_df['question_mark_count'].describe()}\")\n",
        "    print(f\"\\nNumeric Character Statistics:\\n{msmarco_df['number_count'].describe()}\")\n",
        "    \n",
        "    # Sentence length comparison\n",
        "    msmarco_df['avg_sentence_length'] = msmarco_df.apply(\n",
        "        lambda row: row['length'] / row['sentence_count'] if row['sentence_count'] > 0 else 0, \n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nAverage Sentence Length Statistics:\\n{msmarco_df['avg_sentence_length'].describe()}\")\n",
        "    \n",
        "    # Visualization\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Sentence count distribution\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(msmarco_df['sentence_count'], bins=30, kde=True)\n",
        "    plt.title('MSMARCO Sentence Count Distribution')\n",
        "    plt.xlabel('Sentence Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    \n",
        "    # Average sentence length distribution\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(msmarco_df['avg_sentence_length'], bins=30, kde=True)\n",
        "    plt.title('MSMARCO Average Sentence Length Distribution')\n",
        "    plt.xlabel('Average Sentence Length (characters)')\n",
        "    plt.ylabel('Frequency')\n",
        "    \n",
        "    # Question mark count distribution\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.countplot(x='question_mark_count', data=msmarco_df[msmarco_df['question_mark_count'] < 10])\n",
        "    plt.title('MSMARCO Question Mark Count Distribution')\n",
        "    plt.xlabel('Question Mark Count')\n",
        "    plt.ylabel('Document Count')\n",
        "    \n",
        "    # Word count vs text length relationship\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.scatter(msmarco_df['word_count'], msmarco_df['length'], alpha=0.5)\n",
        "    plt.title('Word Count vs Text Length')\n",
        "    plt.xlabel('Word Count')\n",
        "    plt.ylabel('Text Length (characters)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-data"
      },
      "outputs": [],
      "source": [
        "# 8. Save Processed Data\n",
        "# Save locally\n",
        "if len(msmarco_df) > 0:\n",
        "    msmarco_df.to_csv('/content/msmarco_processed.csv', index=False)\n",
        "    print(\"Saved processed dataset to /content/msmarco_processed.csv\")\n",
        "    \n",
        "    # Optional: Save to Google Drive\n",
        "    save_to_drive = False  # Set to True to save to Google Drive\n",
        "    if save_to_drive:\n",
        "        drive_data_path = os.path.join(PROJECT_PATH, \"data\")\n",
        "        os.makedirs(drive_data_path, exist_ok=True)\n",
        "        output_path = os.path.join(drive_data_path, \"msmarco_processed.csv\")\n",
        "        msmarco_df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved processed dataset to Google Drive: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Through this notebook, we've conducted an in-depth analysis of the MSMARCO v1.1 dataset, including:\n",
        "\n",
        "1. Text length distribution and basic statistics\n",
        "2. Word and sentence structure analysis\n",
        "3. Memory usage optimization suggestions\n",
        "4. Visualization of various text features\n",
        "\n",
        "These analyses are valuable for building efficient text retrieval systems and understanding the characteristics of the dataset. Particularly for MSMARCO, which serves as a large-scale information retrieval benchmark, understanding its text length distribution, sentence structure, and vocabulary characteristics is crucial for designing appropriate model architectures and preprocessing strategies."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
