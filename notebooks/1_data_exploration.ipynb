{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSMARCO Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Add project root directory to path\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, we need to load the social media short text dataset. Here we assume the dataset is ready and stored in the project's data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data processing functions from src module\n",
    "from src.data_preparation import load_dataset, preprocess_text\n",
    "\n",
    "# Load dataset\n",
    "# Note: Modify the data path according to actual situation\n",
    "try:\n",
    "    data = load_dataset('../data/social_media_dataset.csv')\n",
    "    print(f\"Successfully loaded dataset with {len(data)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Data file does not exist, please ensure the data file is placed in the correct location\")\n",
    "    # Create a sample dataset for demonstration\n",
    "    data = pd.DataFrame({\n",
    "        'id': range(1000),\n",
    "        'text': [\n",
    "            f\"This is a sample social media post #{i} #example #data\" \n",
    "            for i in range(1000)\n",
    "        ],\n",
    "        'timestamp': pd.date_range(start='2023-01-01', periods=1000, freq='H'),\n",
    "        'user_id': np.random.randint(1, 100, 1000),\n",
    "        'likes': np.random.randint(0, 1000, 1000),\n",
    "        'shares': np.random.randint(0, 200, 1000)\n",
    "    })\n",
    "    print(\"Created sample dataset for demonstration\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset basic information:\")\n",
    "data.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistical Analysis\n",
    "\n",
    "Next, we perform basic statistical analysis on the dataset to understand the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset descriptive statistics:\")\n",
    "data.describe(include='all')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values statistics:\")\n",
    "missing_values = data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis\n",
    "\n",
    "Analyze the distribution of text lengths to understand the characteristics of social media short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text length\n",
    "data['text_length'] = data['text'].apply(len)\n",
    "data['word_count'] = data['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "# Display basic statistics of text length\n",
    "print(\"Text length statistics:\")\n",
    "print(data[['text_length', 'word_count']].describe())\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data['text_length'], kde=True)\n",
    "plt.title('Text Character Length Distribution')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data['word_count'], kde=True)\n",
    "plt.title('Text Word Count Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Content Analysis\n",
    "\n",
    "Analyze text content, including common vocabulary, topic distribution, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_for_analysis(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['processed_tokens'] = data['text'].apply(preprocess_for_analysis)\n",
    "\n",
    "# Count word frequency\n",
    "all_words = [word for tokens in data['processed_tokens'] for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display most common words\n",
    "print(\"Top 20 most common words:\")\n",
    "print(word_freq.most_common(20))\n",
    "\n",
    "# Plot word frequency\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_words = dict(word_freq.most_common(20))\n",
    "sns.barplot(x=list(top_words.keys()), y=list(top_words.values()))\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Tag Analysis\n",
    "\n",
    "Analyze topic tags in social media text (such as hashtags in Twitter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic tags (hashtags)\n",
    "def extract_hashtags(text):\n",
    "    hashtags = re.findall(r'#(\\w+)', text.lower())\n",
    "    return hashtags\n",
    "\n",
    "data['hashtags'] = data['text'].apply(extract_hashtags)\n",
    "\n",
    "# Count tag frequency\n",
    "all_hashtags = [tag for tags in data['hashtags'] for tag in tags]\n",
    "hashtag_freq = Counter(all_hashtags)\n",
    "\n",
    "# Display most common tags\n",
    "print(\"Top 20 most common topic tags:\")\n",
    "print(hashtag_freq.most_common(20))\n",
    "\n",
    "# Plot tag frequency\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_hashtags = dict(hashtag_freq.most_common(20))\n",
    "sns.barplot(x=list(top_hashtags.keys()), y=list(top_hashtags.values()))\n",
    "plt.title('Top 20 Most Common Topic Tags')\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Distribution Analysis\n",
    "\n",
    "Analyze the time distribution characteristics of social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time features\n",
    "data['date'] = data['timestamp'].dt.date\n",
    "data['hour'] = data['timestamp'].dt.hour\n",
    "data['day_of_week'] = data['timestamp'].dt.day_name()\n",
    "\n",
    "# Count texts by date\n",
    "date_counts = data.groupby('date').size()\n",
    "\n",
    "# Count texts by hour\n",
    "hour_counts = data.groupby('hour').size()\n",
    "\n",
    "# Count texts by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = data.groupby('day_of_week').size().reindex(day_order)\n",
    "\n",
    "# Plot time distribution\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "date_counts.plot()\n",
    "plt.title('Text Count by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Text Count')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "hour_counts.plot(kind='bar')\n",
    "plt.title('Text Count by Hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Text Count')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "day_counts.plot(kind='bar')\n",
    "plt.title('Text Count by Day of Week')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Text Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. User Activity Analysis\n",
    "\n",
    "Analyze user activity and engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count texts by user\n",
    "user_post_counts = data.groupby('user_id').size().sort_values(ascending=False)\n",
    "\n",
    "# Calculate average likes and shares per user\n",
    "user_engagement = data.groupby('user_id').agg({\n",
    "    'likes': 'mean',\n",
    "    'shares': 'mean'\n",
    "}).sort_values(by='likes', ascending=False)\n",
    "\n",
    "# Display most active users\n",
    "print(\"Top 10 users with most posts:\")\n",
    "print(user_post_counts.head(10))\n",
    "\n",
    "print(\"\\nTop 10 users with highest average likes:\")\n",
    "print(user_engagement.head(10))\n",
    "\n",
    "# Plot user activity distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(user_post_counts.values, kde=True)\n",
    "plt.title('User Post Count Distribution')\n",
    "plt.xlabel('Post Count')\n",
    "plt.ylabel('User Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(user_engagement['likes'], user_engagement['shares'], alpha=0.5)\n",
    "plt.title('Relationship Between Average Likes and Shares per User')\n",
    "plt.xlabel('Average Likes')\n",
    "plt.ylabel('Average Shares')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Similarity Analysis\n",
    "\n",
    "Vectorize text using TF-IDF and calculate similarity between texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Randomly select a few texts to calculate similarity\n",
    "sample_size = min(5, len(data))\n",
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "sample_texts = data.iloc[sample_indices]['text'].tolist()\n",
    "\n",
    "print(\"\\nSample texts:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"Text {i+1}: {text[:100]}...\")\n",
    "\n",
    "# Calculate similarity between sample texts\n",
    "sample_vectors = tfidf_vectorizer.transform(sample_texts)\n",
    "similarity_matrix = cosine_similarity(sample_vectors)\n",
    "\n",
    "print(\"\\nSimilarity matrix:\")\n",
    "similarity_df = pd.DataFrame(similarity_matrix, \n",
    "                             index=[f\"Text {i+1}\" for i in range(sample_size)],\n",
    "                             columns=[f\"Text {i+1}\" for i in range(sample_size)])\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Special Feature Analysis of Social Media Text\n",
    "\n",
    "Analyze special features of social media text, such as emojis, @mentions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract @mentions\n",
    "def extract_mentions(text):\n",
    "    mentions = re.findall(r'@(\\w+)', text.lower())\n",
    "    return mentions\n",
    "\n",
    "# Detect emojis (simplified version, only detects some common emojis)\n",
    "def contains_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # geometric symbols\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # supplemental arrows\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols & pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # extended symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # symbols & pictographs extended\n",
    "        \"\\U00002702-\\U000027B0\"  # dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "        \"]\")\n",
    "    return bool(emoji_pattern.search(text))\n",
    "\n",
    "# Apply feature extraction\n",
    "data['mentions'] = data['text'].apply(extract_mentions)\n",
    "data['has_emoji'] = data['text'].apply(contains_emoji)\n",
    "data['has_url'] = data['text'].apply(lambda x: bool(re.search(r'http\\S+', x)))\n",
    "data['has_hashtag'] = data['hashtags'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Count special features\n",
    "special_features = {\n",
    "    'Texts with Emojis': data['has_emoji'].mean(),\n",
    "    'Texts with URLs': data['has_url'].mean(),\n",
    "    'Texts with Hashtags': data['has_hashtag'].mean(),\n",
    "    'Texts with @mentions': data['mentions'].apply(lambda x: len(x) > 0).mean()\n",
    "}\n",
    "\n",
    "print(\"Social media text special feature statistics:\")\n",
    "for feature, value in special_features.items():\n",
    "    print(f\"{feature}: {value:.2%}\")\n",
    "\n",
    "# Plot special feature distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(special_features.keys()), y=list(special_features.values()))\n",
    "plt.title('Social Media Text Special Feature Distribution')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Analysis\n",
    "\n",
    "Analyze data quality issues, such as noise, duplicates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate texts\n",
    "duplicate_count = data.duplicated(subset=['text']).sum()\n",
    "print(f\"Number of duplicate texts: {duplicate_count} ({duplicate_count/len(data):.2%})\")\n",
    "\n",
    "# Check for very short texts (possibly noise)\n",
    "very_short_texts = data[data['text_length'] < 10]\n",
    "print(f\"Number of very short texts (length<10): {len(very_short_texts)} ({len(very_short_texts)/len(data):.2%})\")\n",
    "\n",
    "# Check for all-caps texts (possibly spam or emphasis)\n",
    "all_caps_texts = data[data['text'].apply(lambda x: x.isupper())]\n",
    "print(f\"Number of all-caps texts: {len(all_caps_texts)} ({len(all_caps_texts)/len(data):.2%})\")\n",
    "\n",
    "# Display some examples of very short texts\n",
    "if len(very_short_texts) > 0:\n",
    "    print(\"\\nExamples of very short texts:\")\n",
    "    print(very_short_texts['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration Summary\n",
    "\n",
    "Through the exploration of the social media short text dataset, we have discovered the following characteristics:\n",
    "\n",
    "1. **Text Length Characteristics**: Social media texts are typically short, with average character and word counts of X and Y respectively.\n",
    "2. **Common Vocabulary**: The most common words include...\n",
    "3. **Topic Tags**: The most popular topic tags include...\n",
    "4. **Time Distribution**: Text publication times show clear temporal patterns, such as...\n",
    "5. **User Activity**: User activity is unevenly distributed, with a small number of users contributing a large amount of content.\n",
    "6. **Special Features**: Social media texts contain many special features, such as emojis, URLs, @mentions, etc.\n",
    "7. **Data Quality**: There is a certain percentage of duplicate texts and very short texts that need to be cleaned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
