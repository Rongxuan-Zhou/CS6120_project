{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Retrieval System Evaluation",
    "Implemented based on src/evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initial-setup"
   },
   "outputs": [],
   "source": [
    "# 1. Environment Setup\n",
    "!pip install -q scikit-learn matplotlib\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/CS6120_project\"\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-test-data"
   },
   "outputs": [],
   "source": [
    "# 2. Load test data\n",
    "import json\n",
    "from src.evaluation import Evaluator\n",
    "\n",
    "# Load test set\n",
    "with open(\"data/processed/combined.json\") as f:\n",
    "    test_data = json.load(f)[\"test\"]\n",
    "    queries = [item[\"query\"] for item in test_data]\n",
    "    ground_truth = [item[\"relevant_docs\"] for item in test_data]\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = Evaluator(queries, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation-methods"
   },
   "outputs": [],
   "source": [
    "# 3. Evaluate retrieval methods\n",
    "methods = [\"BM25\", \"Vector\", \"Hybrid\"]\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    start = timer()\n",
    "    \n",
    "    # Simulate retrieval results (should call actual retrieval modules)\n",
    "    if method == \"BM25\":\n",
    "        scores = evaluator.simulate_bm25()\n",
    "    elif method == \"Vector\":\n",
    "        scores = evaluator.simulate_vector()\n",
    "    else:\n",
    "        scores = evaluator.simulate_hybrid()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = evaluator.evaluate(scores)\n",
    "    metrics[\"time\"] = timer() - start\n",
    "    results[method] = metrics\n",
    "    \n",
    "    print(f\"{method} Evaluation Results:\")\n",
    "    print(f\"- MRR: {metrics['mrr']:.4f}\")\n",
    "    print(f\"- Recall@10: {metrics['recall@10']:.4f}\")\n",
    "    print(f\"- Precision@5: {metrics['precision@5']:.4f}\")\n",
    "    print(f\"- Average query time: {metrics['time']:.4f}s\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# 4. Results visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "ax1.bar(x - width, [results[m][\"mrr\"] for m in methods], width, label='MRR')\n",
    "ax1.bar(x, [results[m][\"recall@10\"] for m in methods], width, label='Recall@10')\n",
    "ax1.bar(x + width, [results[m][\"precision@5\"] for m in methods], width, label='Precision@5')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Retrieval Performance Comparison')\n",
    "ax1.legend()\n",
    "\n",
    "# Time comparison\n",
    "ax2.bar(methods, [results[m][\"time\"] for m in methods], color='orange')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Query Latency Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ablation-study"
   },
   "outputs": [],
   "source": [
    "# 5. Ablation study (weight parameter analysis)\n",
    "weight_range = np.linspace(0, 1, 11)\n",
    "mrr_scores = []\n",
    "\n",
    "for w in weight_range:\n",
    "    # Simulate fixed-weight hybrid retrieval\n",
    "    scores = evaluator.simulate_hybrid(fixed_weight=w)\n",
    "    metrics = evaluator.evaluate(scores)\n",
    "    mrr_scores.append(metrics[\"mrr\"])\n",
    "\n",
    "# Plot weight-MRR curve\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(weight_range, mrr_scores, marker='o')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Equal Weight')\n",
    "plt.xlabel('BM25 Weight (1-Vector Weight)')\n",
    "plt.ylabel('MRR')\n",
    "plt.title('Ablation Study: Weight Parameter Analysis')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
