{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rongxuan-Zhou/CS6120_project/blob/index_construction-%26-hybrid_retrieval/notebooks/index-construction%20%26%20hybrid-retrieval-2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Environment Setup\n",
        "!pip install -q faiss-cpu sentence-transformers nltk rank-bm25 hnswlib scikit-learn datasets pytrec_eval tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import rank_bm25\n",
        "from collections import Counter\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Any, Optional, Union\n",
        "\n",
        "# Set project path\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/CS6120_project\"\n",
        "os.chdir(PROJECT_PATH)\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"models/indexes\", exist_ok=True)\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"Available GPU: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU for processing\")\n",
        "\n",
        "# Download NLTK resources\n",
        "import nltk\n",
        "for resource in ['punkt', 'stopwords']:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{resource}')\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7j2qmKKTwXg",
        "outputId": "a997ad85-3204-456b-fe6b-3346e51d7aea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Available GPU: True\n",
            "Using GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Helper Classes & Functions\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Simple timer for benchmarking\"\"\"\n",
        "    def __init__(self, name=\"Operation\"):\n",
        "        self.name = name\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.end = time.time()\n",
        "        self.interval = self.end - self.start\n",
        "        print(f\"{self.name} completed in {self.interval:.2f} seconds\")\n",
        "\n",
        "\n",
        "def preprocess_text(text: str) -> List[str]:\n",
        "    \"\"\"Preprocess text for BM25 indexing\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalnum()]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def normalize_scores(scores: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Normalize scores to [0,1] range with safe handling of edge cases\"\"\"\n",
        "    if len(scores) == 0:\n",
        "        return scores\n",
        "\n",
        "    min_val = np.min(scores)\n",
        "    max_val = np.max(scores)\n",
        "\n",
        "    if max_val == min_val:\n",
        "        return np.ones_like(scores)\n",
        "\n",
        "    return (scores - min_val) / (max_val - min_val + 1e-8)\n",
        "\n",
        "\n",
        "def load_models(model_paths: Dict[str, str]) -> Tuple[Dict[str, SentenceTransformer], Dict[str, int]]:\n",
        "    \"\"\"Load all models and return them with their embedding dimensions\"\"\"\n",
        "    models = {}\n",
        "    dimensions = {}\n",
        "\n",
        "    for model_name, model_path in model_paths.items():\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        try:\n",
        "            models[model_name] = SentenceTransformer(model_path)\n",
        "            models[model_name].to(device)\n",
        "            dimensions[model_name] = models[model_name].get_sentence_embedding_dimension()\n",
        "\n",
        "            print(f\"  - Model path: {model_path}\")\n",
        "            print(f\"  - Model architecture: {dimensions[model_name]}d embedding dimension\")\n",
        "            print(f\"  - Model details: {models[model_name]}\")\n",
        "            print(\"\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_name} from {model_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not models:\n",
        "        raise ValueError(\"No models could be loaded. Please check model paths.\")\n",
        "\n",
        "    return models, dimensions"
      ],
      "metadata": {
        "id": "1oaTKTI4T4mF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. MS MARCO Data Loading\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_msmarco_data(max_samples: int = 5000, seed: int = 42) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, Dict[str, int]]]:\n",
        "    \"\"\"Load MS MARCO dataset with proper format for MRR evaluation\n",
        "\n",
        "    Args:\n",
        "        max_samples: Maximum samples to load\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        corpus: Dictionary {doc_id: document_text}\n",
        "        queries: Dictionary {query_id: query_text}\n",
        "        qrels: Dictionary {query_id: {doc_id: relevance}}\n",
        "    \"\"\"\n",
        "    print(\"Loading MS MARCO dataset...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"ms_marco\", \"v1.1\")\n",
        "        dev_data = dataset[\"validation\"].shuffle(seed=seed).select(range(max_samples))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    queries = {}\n",
        "    corpus = {}\n",
        "    qrels = {}\n",
        "\n",
        "    # Process each sample\n",
        "    for example in dev_data:\n",
        "        # Get query_id and query text\n",
        "        qid = str(example[\"query_id\"])\n",
        "        query_text = example[\"query\"]\n",
        "        queries[qid] = query_text\n",
        "\n",
        "        # Get passages information\n",
        "        passages_info = example[\"passages\"]\n",
        "        passage_texts = passages_info.get(\"passage_text\", [])\n",
        "        is_selecteds = passages_info.get(\"is_selected\", [])\n",
        "\n",
        "        # Process each passage\n",
        "        for i, (text, is_sel) in enumerate(zip(passage_texts, is_selecteds)):\n",
        "            # Generate unique document ID as \"qid_i\"\n",
        "            doc_id = f\"{qid}_{i}\"\n",
        "            corpus[doc_id] = text\n",
        "\n",
        "            # If passage is relevant, add to qrels\n",
        "            if is_sel == 1:\n",
        "                if qid not in qrels:\n",
        "                    qrels[qid] = {}\n",
        "                qrels[qid][doc_id] = 1\n",
        "\n",
        "    # Check positive counts\n",
        "    check_positive_counts(queries, qrels)\n",
        "\n",
        "    print(f\"Loaded {len(corpus)} documents, {len(queries)} queries, {len(qrels)} qrels.\")\n",
        "    return corpus, queries, qrels\n",
        "\n",
        "\n",
        "def check_positive_counts(queries: Dict[str, str], qrels: Dict[str, Dict[str, int]]):\n",
        "    \"\"\"Analyze distribution of relevant documents per query\"\"\"\n",
        "    # Count positive examples per query\n",
        "    positive_counts = []\n",
        "    for qid in queries:\n",
        "        if qid in qrels:\n",
        "            positive_counts.append(len(qrels[qid]))\n",
        "        else:\n",
        "            positive_counts.append(0)\n",
        "\n",
        "    # Count distribution\n",
        "    counter = Counter(positive_counts)\n",
        "    print(\"Positive examples distribution (count: queries):\")\n",
        "    for num_pos, num_queries in sorted(counter.items()):\n",
        "        print(f\"{num_pos} positive examples: {num_queries} queries\")\n",
        "\n",
        "    # Count queries without positives\n",
        "    total_queries = len(queries)\n",
        "    no_positive = counter.get(0, 0)\n",
        "    print(f\"\\nTotal queries: {total_queries}\")\n",
        "    print(f\"Queries without positives: {no_positive} ({no_positive/total_queries*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "wi9sMycpT6y9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load Dataset and Models\n",
        "# Load MS MARCO dataset\n",
        "try:\n",
        "    corpus, queries, qrels = load_msmarco_data(max_samples=5000, seed=42)\n",
        "\n",
        "    # Extract important data structures\n",
        "    corpus_texts = list(corpus.values())\n",
        "    doc_ids = list(corpus.keys())\n",
        "\n",
        "    # Define model paths\n",
        "    model_paths = {\n",
        "        \"msmarco_stsb\": os.path.join(PROJECT_PATH, \"model/msmarco_stsb_finetuned_model\"),\n",
        "        \"stsb\": os.path.join(PROJECT_PATH, \"model/stsb_finetuned_model\")\n",
        "    }\n",
        "\n",
        "    # Load models\n",
        "    models, dimensions = load_models(model_paths)\n",
        "\n",
        "    # Models for primary retrieval and fallback\n",
        "    primary_model = \"msmarco_stsb\"\n",
        "    fallback_model = \"stsb\"\n",
        "\n",
        "    print(f\"Primary model: {primary_model}\")\n",
        "    print(f\"Fallback model: {fallback_model}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during dataset or model loading: {str(e)}\")\n",
        "    print(\"Please check your environment setup and model paths.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs1Si3y1T813",
        "outputId": "eacbfeee-0d52-4e35-a35a-2a75966c648f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MS MARCO dataset...\n",
            "Positive examples distribution (count: queries):\n",
            "0 positive examples: 160 queries\n",
            "1 positive examples: 4371 queries\n",
            "2 positive examples: 425 queries\n",
            "3 positive examples: 38 queries\n",
            "4 positive examples: 6 queries\n",
            "\n",
            "Total queries: 5000\n",
            "Queries without positives: 160 (3.20%)\n",
            "Loaded 41070 documents, 5000 queries, 4840 qrels.\n",
            "Loading model: msmarco_stsb\n",
            "  - Model path: /content/drive/MyDrive/CS6120_project/model/msmarco_stsb_finetuned_model\n",
            "  - Model architecture: 768d embedding dimension\n",
            "  - Model details: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            ")\n",
            "\n",
            "Loading model: stsb\n",
            "  - Model path: /content/drive/MyDrive/CS6120_project/model/stsb_finetuned_model\n",
            "  - Model architecture: 768d embedding dimension\n",
            "  - Model details: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            ")\n",
            "\n",
            "Primary model: msmarco_stsb\n",
            "Fallback model: stsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Build BM25 and FAISS indexes\n",
        "# Clear GPU cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Create BM25 index with improved error handling\n",
        "def build_bm25_index(corpus_texts, k1=0.9, b=0.6):\n",
        "    \"\"\"Build BM25 index with improved error handling\"\"\"\n",
        "    print(\"Creating BM25 index...\")\n",
        "    try:\n",
        "        with Timer(\"BM25 preprocessing\"):\n",
        "            tokenized_corpus = [preprocess_text(text) for text in tqdm(corpus_texts, desc=\"Preprocessing documents\")]\n",
        "\n",
        "        # Filter out empty documents to prevent BM25 errors\n",
        "        empty_docs = [i for i, tokens in enumerate(tokenized_corpus) if not tokens]\n",
        "        if empty_docs:\n",
        "            print(f\"Warning: Found {len(empty_docs)} empty documents after preprocessing\")\n",
        "            # Add at least one token to empty documents to prevent BM25 errors\n",
        "            for i in empty_docs:\n",
        "                tokenized_corpus[i] = [\"_empty_\"]\n",
        "\n",
        "        with Timer(\"BM25 index construction\"):\n",
        "            bm25 = rank_bm25.BM25Okapi(tokenized_corpus, k1=k1, b=b)\n",
        "\n",
        "        # Create BM25 related information\n",
        "        avg_doc_len = sum(len(doc) for doc in tokenized_corpus) / len(tokenized_corpus)\n",
        "        bm25_info = {\n",
        "            \"corpus_size\": len(tokenized_corpus),\n",
        "            \"avg_doc_len\": avg_doc_len,\n",
        "            \"idf_avg\": sum(bm25.idf.values()) / len(bm25.idf) if bm25.idf else 0,\n",
        "            \"k1\": k1,\n",
        "            \"b\": b,\n",
        "            \"empty_docs\": len(empty_docs)\n",
        "        }\n",
        "\n",
        "        return bm25, bm25_info, tokenized_corpus\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building BM25 index: {str(e)}\")\n",
        "        # Return a simple BM25 index with dummy data in case of error\n",
        "        dummy_corpus = [[\"dummy\"] for _ in range(len(corpus_texts))]\n",
        "        dummy_bm25 = rank_bm25.BM25Okapi(dummy_corpus)\n",
        "        dummy_info = {\"error\": str(e)}\n",
        "        return dummy_bm25, dummy_info, dummy_corpus\n",
        "\n",
        "# Build BM25 index\n",
        "with Timer(\"Total BM25 index building\"):\n",
        "    bm25, bm25_info, tokenized_corpus = build_bm25_index(corpus_texts)\n",
        "\n",
        "print(\"BM25 index created successfully\")\n",
        "print(f\"Average document length: {bm25_info['avg_doc_len']:.2f} tokens\")\n",
        "\n",
        "# Save BM25 info\n",
        "with open(os.path.join(\"models/indexes\", \"bm25_info.json\"), 'w') as f:\n",
        "    json.dump(bm25_info, f)\n",
        "\n",
        "# Create FAISS indexes for each model\n",
        "def build_faiss_indexes(model, corpus_texts, model_name):\n",
        "    \"\"\"Build all FAISS indexes for a given model\"\"\"\n",
        "    print(f\"\\nProcessing {model_name} model...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    print(f\"Generating {model_name} embeddings...\")\n",
        "    embeddings = None\n",
        "\n",
        "    timer = Timer(f\"{model_name} encoding\")\n",
        "    with timer:\n",
        "        batch_size = 128\n",
        "        embeddings_list = []\n",
        "\n",
        "        for i in tqdm(range(0, len(corpus_texts), batch_size)):\n",
        "            batch = corpus_texts[i:i+batch_size]\n",
        "            try:\n",
        "                batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
        "                embeddings_list.append(batch_embeddings)\n",
        "            except Exception as e:\n",
        "                print(f\"Error encoding batch {i}-{i+batch_size}: {str(e)}\")\n",
        "                # Add zero embeddings for failed batch\n",
        "                dimension = model.get_sentence_embedding_dimension()\n",
        "                zero_embeddings = np.zeros((len(batch), dimension))\n",
        "                embeddings_list.append(zero_embeddings)\n",
        "\n",
        "        embeddings = np.vstack(embeddings_list)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    print(f\"Generated {len(embeddings)} embeddings with dimension {dimension}\")\n",
        "    print(f\"Processing speed: {len(corpus_texts)/timer.interval:.2f} docs/sec\")\n",
        "\n",
        "    # Normalize vectors for cosine similarity\n",
        "    print(f\"Normalizing {model_name} vectors...\")\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    # Create indexes\n",
        "    model_indexes = {}\n",
        "\n",
        "    # 1. Flat index (exact search)\n",
        "    print(f\"Building {model_name} Flat index...\")\n",
        "    index_flat = faiss.IndexFlatIP(dimension)\n",
        "    index_flat.add(embeddings)\n",
        "    print(f\"{model_name} Flat index built with {index_flat.ntotal} vectors\")\n",
        "    model_indexes[\"flat\"] = index_flat\n",
        "\n",
        "    # 2. HNSW index (fast approximate search)\n",
        "    print(f\"Building {model_name} HNSW index...\")\n",
        "    M = 16  # Connections per node\n",
        "    ef_construction = 200  # Search width during construction\n",
        "    index_hnsw = faiss.IndexHNSWFlat(dimension, M)\n",
        "    index_hnsw.hnsw.efConstruction = ef_construction\n",
        "    index_hnsw.add(embeddings)\n",
        "    print(f\"{model_name} HNSW index built with {index_hnsw.ntotal} vectors\")\n",
        "    model_indexes[\"hnsw\"] = index_hnsw\n",
        "\n",
        "    # 3. IVF-PQ index (memory-efficient)\n",
        "    print(f\"Building {model_name} IVF-PQ index...\")\n",
        "    nlist = min(100, len(corpus_texts) // 50)  # Number of cluster centers\n",
        "    m = 8  # Number of subvectors\n",
        "    bits = 8  # Bits per subvector\n",
        "    quantizer = faiss.IndexFlatL2(dimension)\n",
        "    index_ivfpq = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, bits)\n",
        "    index_ivfpq.train(embeddings)\n",
        "    index_ivfpq.add(embeddings)\n",
        "    print(f\"{model_name} IVF-PQ index built with {index_ivfpq.ntotal} vectors\")\n",
        "    model_indexes[\"ivfpq\"] = index_ivfpq\n",
        "\n",
        "    # Create index config\n",
        "    index_config = {\n",
        "        \"model_name\": model_name,\n",
        "        \"dimension\": dimension,\n",
        "        \"flat_index\": {\"type\": \"IndexFlatIP\", \"dimension\": dimension},\n",
        "        \"hnsw_index\": {\"type\": \"IndexHNSWFlat\", \"dimension\": dimension, \"M\": M, \"efConstruction\": ef_construction},\n",
        "        \"ivfpq_index\": {\"type\": \"IndexIVFPQ\", \"dimension\": dimension, \"nlist\": nlist, \"m\": m, \"bits\": bits, \"recommended_nprobe\": 30}\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"embeddings\": embeddings,\n",
        "        \"indexes\": model_indexes,\n",
        "        \"config\": index_config\n",
        "    }\n",
        "\n",
        "# Build FAISS indexes for each model\n",
        "all_embeddings = {}\n",
        "all_indexes = {}\n",
        "all_configs = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    try:\n",
        "        result = build_faiss_indexes(model, corpus_texts, model_name)\n",
        "        all_embeddings[model_name] = result[\"embeddings\"]\n",
        "        all_indexes[model_name] = result[\"indexes\"]\n",
        "        all_configs[model_name] = result[\"config\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error building indexes for model {model_name}: {str(e)}\")\n",
        "\n",
        "# Save hybrid retrieval configuration\n",
        "hybrid_config = {\n",
        "    \"primary_model\": primary_model,\n",
        "    \"fallback_model\": fallback_model,\n",
        "    \"corpus_size\": len(corpus),\n",
        "    \"models\": {\n",
        "        model_name: {\n",
        "            \"dimension\": dimensions[model_name],\n",
        "            \"index_types\": list(all_indexes[model_name].keys()) if model_name in all_indexes else []\n",
        "        } for model_name in models\n",
        "    },\n",
        "    \"bm25_info\": bm25_info\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"models/indexes\", \"hybrid_config.json\"), 'w') as f:\n",
        "    json.dump(hybrid_config, f)\n",
        "\n",
        "print(\"\\nAll model indexes successfully built\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXN_Yr8wUC-P",
        "outputId": "b191e481-0318-4d63-dec6-999c23e23acd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating BM25 index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Preprocessing documents:   0%|          | 0/41070 [00:00<?, ?it/s]\u001b[A\n",
            "Preprocessing documents:   0%|          | 163/41070 [00:00<00:25, 1629.33it/s]\u001b[A\n",
            "Preprocessing documents:   1%|          | 326/41070 [00:00<00:26, 1549.44it/s]\u001b[A\n",
            "Preprocessing documents:   1%|          | 482/41070 [00:00<00:26, 1528.12it/s]\u001b[A\n",
            "Preprocessing documents:   2%|▏         | 644/41070 [00:00<00:25, 1563.20it/s]\u001b[A\n",
            "Preprocessing documents:   2%|▏         | 801/41070 [00:00<00:26, 1543.93it/s]\u001b[A\n",
            "Preprocessing documents:   2%|▏         | 956/41070 [00:00<00:26, 1536.81it/s]\u001b[A\n",
            "Preprocessing documents:   3%|▎         | 1110/41070 [00:00<00:26, 1535.26it/s]\u001b[A\n",
            "Preprocessing documents:   3%|▎         | 1279/41070 [00:00<00:25, 1583.28it/s]\u001b[A\n",
            "Preprocessing documents:   4%|▎         | 1440/41070 [00:00<00:24, 1589.01it/s]\u001b[A\n",
            "Preprocessing documents:   4%|▍         | 1606/41070 [00:01<00:24, 1609.49it/s]\u001b[A\n",
            "Preprocessing documents:   4%|▍         | 1774/41070 [00:01<00:24, 1630.11it/s]\u001b[A\n",
            "Preprocessing documents:   5%|▍         | 1938/41070 [00:01<00:24, 1627.79it/s]\u001b[A\n",
            "Preprocessing documents:   5%|▌         | 2101/41070 [00:01<00:24, 1605.35it/s]\u001b[A\n",
            "Preprocessing documents:   6%|▌         | 2262/41070 [00:01<00:24, 1586.66it/s]\u001b[A\n",
            "Preprocessing documents:   6%|▌         | 2421/41070 [00:01<00:25, 1541.32it/s]\u001b[A\n",
            "Preprocessing documents:   6%|▋         | 2579/41070 [00:01<00:24, 1549.99it/s]\u001b[A\n",
            "Preprocessing documents:   7%|▋         | 2746/41070 [00:01<00:24, 1583.65it/s]\u001b[A\n",
            "Preprocessing documents:   7%|▋         | 2905/41070 [00:01<00:24, 1555.62it/s]\u001b[A\n",
            "Preprocessing documents:   7%|▋         | 3071/41070 [00:01<00:23, 1584.77it/s]\u001b[A\n",
            "Preprocessing documents:   8%|▊         | 3238/41070 [00:02<00:23, 1607.49it/s]\u001b[A\n",
            "Preprocessing documents:   8%|▊         | 3401/41070 [00:02<00:23, 1613.75it/s]\u001b[A\n",
            "Preprocessing documents:   9%|▊         | 3563/41070 [00:02<00:23, 1596.22it/s]\u001b[A\n",
            "Preprocessing documents:   9%|▉         | 3723/41070 [00:02<00:23, 1594.06it/s]\u001b[A\n",
            "Preprocessing documents:   9%|▉         | 3884/41070 [00:02<00:23, 1595.75it/s]\u001b[A\n",
            "Preprocessing documents:  10%|▉         | 4055/41070 [00:02<00:22, 1627.73it/s]\u001b[A\n",
            "Preprocessing documents:  10%|█         | 4218/41070 [00:02<00:22, 1611.73it/s]\u001b[A\n",
            "Preprocessing documents:  11%|█         | 4380/41070 [00:02<00:22, 1601.62it/s]\u001b[A\n",
            "Preprocessing documents:  11%|█         | 4553/41070 [00:02<00:22, 1637.12it/s]\u001b[A\n",
            "Preprocessing documents:  11%|█▏        | 4717/41070 [00:02<00:22, 1620.65it/s]\u001b[A\n",
            "Preprocessing documents:  12%|█▏        | 4890/41070 [00:03<00:21, 1651.91it/s]\u001b[A\n",
            "Preprocessing documents:  12%|█▏        | 5056/41070 [00:03<00:22, 1612.75it/s]\u001b[A\n",
            "Preprocessing documents:  13%|█▎        | 5231/41070 [00:03<00:21, 1651.83it/s]\u001b[A\n",
            "Preprocessing documents:  13%|█▎        | 5407/41070 [00:03<00:21, 1678.41it/s]\u001b[A\n",
            "Preprocessing documents:  14%|█▎        | 5576/41070 [00:03<00:21, 1653.55it/s]\u001b[A\n",
            "Preprocessing documents:  14%|█▍        | 5744/41070 [00:03<00:21, 1658.30it/s]\u001b[A\n",
            "Preprocessing documents:  14%|█▍        | 5910/41070 [00:03<00:21, 1627.06it/s]\u001b[A\n",
            "Preprocessing documents:  15%|█▍        | 6074/41070 [00:03<00:21, 1630.07it/s]\u001b[A\n",
            "Preprocessing documents:  15%|█▌        | 6238/41070 [00:03<00:21, 1618.85it/s]\u001b[A\n",
            "Preprocessing documents:  16%|█▌        | 6400/41070 [00:03<00:21, 1602.28it/s]\u001b[A\n",
            "Preprocessing documents:  16%|█▌        | 6566/41070 [00:04<00:21, 1619.16it/s]\u001b[A\n",
            "Preprocessing documents:  16%|█▋        | 6729/41070 [00:04<00:21, 1602.44it/s]\u001b[A\n",
            "Preprocessing documents:  17%|█▋        | 6890/41070 [00:04<00:21, 1579.76it/s]\u001b[A\n",
            "Preprocessing documents:  17%|█▋        | 7049/41070 [00:04<00:21, 1564.17it/s]\u001b[A\n",
            "Preprocessing documents:  18%|█▊        | 7212/41070 [00:04<00:21, 1580.80it/s]\u001b[A\n",
            "Preprocessing documents:  18%|█▊        | 7371/41070 [00:04<00:21, 1580.25it/s]\u001b[A\n",
            "Preprocessing documents:  18%|█▊        | 7530/41070 [00:04<00:21, 1580.32it/s]\u001b[A\n",
            "Preprocessing documents:  19%|█▊        | 7689/41070 [00:04<00:21, 1574.95it/s]\u001b[A\n",
            "Preprocessing documents:  19%|█▉        | 7853/41070 [00:04<00:20, 1594.15it/s]\u001b[A\n",
            "Preprocessing documents:  20%|█▉        | 8013/41070 [00:05<00:20, 1595.66it/s]\u001b[A\n",
            "Preprocessing documents:  20%|█▉        | 8173/41070 [00:05<00:20, 1590.89it/s]\u001b[A\n",
            "Preprocessing documents:  20%|██        | 8337/41070 [00:05<00:20, 1605.39it/s]\u001b[A\n",
            "Preprocessing documents:  21%|██        | 8498/41070 [00:05<00:20, 1604.86it/s]\u001b[A\n",
            "Preprocessing documents:  21%|██        | 8664/41070 [00:05<00:20, 1619.69it/s]\u001b[A\n",
            "Preprocessing documents:  21%|██▏       | 8826/41070 [00:05<00:20, 1588.28it/s]\u001b[A\n",
            "Preprocessing documents:  22%|██▏       | 8992/41070 [00:05<00:19, 1607.83it/s]\u001b[A\n",
            "Preprocessing documents:  22%|██▏       | 9156/41070 [00:05<00:19, 1614.29it/s]\u001b[A\n",
            "Preprocessing documents:  23%|██▎       | 9329/41070 [00:05<00:19, 1648.38it/s]\u001b[A\n",
            "Preprocessing documents:  23%|██▎       | 9494/41070 [00:05<00:19, 1627.80it/s]\u001b[A\n",
            "Preprocessing documents:  24%|██▎       | 9657/41070 [00:06<00:19, 1615.82it/s]\u001b[A\n",
            "Preprocessing documents:  24%|██▍       | 9819/41070 [00:06<00:19, 1569.25it/s]\u001b[A\n",
            "Preprocessing documents:  24%|██▍       | 9984/41070 [00:06<00:19, 1591.54it/s]\u001b[A\n",
            "Preprocessing documents:  25%|██▍       | 10147/41070 [00:06<00:19, 1602.77it/s]\u001b[A\n",
            "Preprocessing documents:  25%|██▌       | 10316/41070 [00:06<00:18, 1624.53it/s]\u001b[A\n",
            "Preprocessing documents:  26%|██▌       | 10487/41070 [00:06<00:18, 1648.49it/s]\u001b[A\n",
            "Preprocessing documents:  26%|██▌       | 10653/41070 [00:06<00:18, 1636.78it/s]\u001b[A\n",
            "Preprocessing documents:  26%|██▋       | 10817/41070 [00:06<00:18, 1602.37it/s]\u001b[A\n",
            "Preprocessing documents:  27%|██▋       | 10978/41070 [00:06<00:18, 1598.03it/s]\u001b[A\n",
            "Preprocessing documents:  27%|██▋       | 11149/41070 [00:06<00:18, 1629.75it/s]\u001b[A\n",
            "Preprocessing documents:  28%|██▊       | 11315/41070 [00:07<00:18, 1636.21it/s]\u001b[A\n",
            "Preprocessing documents:  28%|██▊       | 11479/41070 [00:07<00:18, 1609.49it/s]\u001b[A\n",
            "Preprocessing documents:  28%|██▊       | 11646/41070 [00:07<00:18, 1625.25it/s]\u001b[A\n",
            "Preprocessing documents:  29%|██▉       | 11814/41070 [00:07<00:17, 1639.73it/s]\u001b[A\n",
            "Preprocessing documents:  29%|██▉       | 11984/41070 [00:07<00:17, 1656.13it/s]\u001b[A\n",
            "Preprocessing documents:  30%|██▉       | 12165/41070 [00:07<00:17, 1700.05it/s]\u001b[A\n",
            "Preprocessing documents:  30%|███       | 12336/41070 [00:07<00:16, 1691.49it/s]\u001b[A\n",
            "Preprocessing documents:  30%|███       | 12506/41070 [00:07<00:17, 1678.39it/s]\u001b[A\n",
            "Preprocessing documents:  31%|███       | 12674/41070 [00:07<00:16, 1678.72it/s]\u001b[A\n",
            "Preprocessing documents:  31%|███▏      | 12842/41070 [00:07<00:16, 1678.47it/s]\u001b[A\n",
            "Preprocessing documents:  32%|███▏      | 13010/41070 [00:08<00:16, 1661.04it/s]\u001b[A\n",
            "Preprocessing documents:  32%|███▏      | 13178/41070 [00:08<00:16, 1665.45it/s]\u001b[A\n",
            "Preprocessing documents:  32%|███▏      | 13345/41070 [00:08<00:17, 1622.58it/s]\u001b[A\n",
            "Preprocessing documents:  33%|███▎      | 13511/41070 [00:08<00:16, 1632.91it/s]\u001b[A\n",
            "Preprocessing documents:  33%|███▎      | 13675/41070 [00:08<00:17, 1607.26it/s]\u001b[A\n",
            "Preprocessing documents:  34%|███▎      | 13836/41070 [00:08<00:17, 1597.10it/s]\u001b[A\n",
            "Preprocessing documents:  34%|███▍      | 14000/41070 [00:08<00:16, 1607.68it/s]\u001b[A\n",
            "Preprocessing documents:  34%|███▍      | 14163/41070 [00:08<00:16, 1611.04it/s]\u001b[A\n",
            "Preprocessing documents:  35%|███▍      | 14329/41070 [00:08<00:16, 1621.68it/s]\u001b[A\n",
            "Preprocessing documents:  35%|███▌      | 14492/41070 [00:08<00:16, 1602.06it/s]\u001b[A\n",
            "Preprocessing documents:  36%|███▌      | 14653/41070 [00:09<00:16, 1574.08it/s]\u001b[A\n",
            "Preprocessing documents:  36%|███▌      | 14819/41070 [00:09<00:16, 1597.60it/s]\u001b[A\n",
            "Preprocessing documents:  36%|███▋      | 14979/41070 [00:09<00:16, 1581.18it/s]\u001b[A\n",
            "Preprocessing documents:  37%|███▋      | 15140/41070 [00:09<00:16, 1586.16it/s]\u001b[A\n",
            "Preprocessing documents:  37%|███▋      | 15299/41070 [00:09<00:16, 1566.38it/s]\u001b[A\n",
            "Preprocessing documents:  38%|███▊      | 15467/41070 [00:09<00:16, 1598.77it/s]\u001b[A\n",
            "Preprocessing documents:  38%|███▊      | 15629/41070 [00:09<00:15, 1602.36it/s]\u001b[A\n",
            "Preprocessing documents:  38%|███▊      | 15790/41070 [00:09<00:15, 1586.66it/s]\u001b[A\n",
            "Preprocessing documents:  39%|███▉      | 15965/41070 [00:09<00:15, 1634.48it/s]\u001b[A\n",
            "Preprocessing documents:  39%|███▉      | 16129/41070 [00:10<00:15, 1635.13it/s]\u001b[A\n",
            "Preprocessing documents:  40%|███▉      | 16302/41070 [00:10<00:14, 1662.67it/s]\u001b[A\n",
            "Preprocessing documents:  40%|████      | 16469/41070 [00:10<00:15, 1636.00it/s]\u001b[A\n",
            "Preprocessing documents:  40%|████      | 16633/41070 [00:10<00:15, 1596.62it/s]\u001b[A\n",
            "Preprocessing documents:  41%|████      | 16808/41070 [00:10<00:14, 1641.07it/s]\u001b[A\n",
            "Preprocessing documents:  41%|████▏     | 16973/41070 [00:10<00:14, 1643.46it/s]\u001b[A\n",
            "Preprocessing documents:  42%|████▏     | 17142/41070 [00:10<00:14, 1656.00it/s]\u001b[A\n",
            "Preprocessing documents:  42%|████▏     | 17310/41070 [00:10<00:14, 1660.65it/s]\u001b[A\n",
            "Preprocessing documents:  43%|████▎     | 17477/41070 [00:10<00:14, 1638.55it/s]\u001b[A\n",
            "Preprocessing documents:  43%|████▎     | 17650/41070 [00:10<00:14, 1663.77it/s]\u001b[A\n",
            "Preprocessing documents:  43%|████▎     | 17817/41070 [00:11<00:14, 1617.04it/s]\u001b[A\n",
            "Preprocessing documents:  44%|████▍     | 17980/41070 [00:11<00:14, 1602.88it/s]\u001b[A\n",
            "Preprocessing documents:  44%|████▍     | 18150/41070 [00:11<00:14, 1630.23it/s]\u001b[A\n",
            "Preprocessing documents:  45%|████▍     | 18317/41070 [00:11<00:13, 1639.97it/s]\u001b[A\n",
            "Preprocessing documents:  45%|████▌     | 18485/41070 [00:11<00:13, 1650.65it/s]\u001b[A\n",
            "Preprocessing documents:  45%|████▌     | 18653/41070 [00:11<00:13, 1659.05it/s]\u001b[A\n",
            "Preprocessing documents:  46%|████▌     | 18821/41070 [00:11<00:13, 1664.42it/s]\u001b[A\n",
            "Preprocessing documents:  46%|████▋     | 19000/41070 [00:11<00:12, 1698.62it/s]\u001b[A\n",
            "Preprocessing documents:  47%|████▋     | 19170/41070 [00:11<00:13, 1668.40it/s]\u001b[A\n",
            "Preprocessing documents:  47%|████▋     | 19337/41070 [00:11<00:13, 1644.46it/s]\u001b[A\n",
            "Preprocessing documents:  47%|████▋     | 19502/41070 [00:12<00:13, 1625.68it/s]\u001b[A\n",
            "Preprocessing documents:  48%|████▊     | 19665/41070 [00:12<00:13, 1591.27it/s]\u001b[A\n",
            "Preprocessing documents:  48%|████▊     | 19825/41070 [00:12<00:13, 1592.37it/s]\u001b[A\n",
            "Preprocessing documents:  49%|████▊     | 19992/41070 [00:12<00:13, 1614.65it/s]\u001b[A\n",
            "Preprocessing documents:  49%|████▉     | 20154/41070 [00:12<00:13, 1608.07it/s]\u001b[A\n",
            "Preprocessing documents:  49%|████▉     | 20319/41070 [00:12<00:12, 1617.72it/s]\u001b[A\n",
            "Preprocessing documents:  50%|████▉     | 20481/41070 [00:12<00:12, 1616.07it/s]\u001b[A\n",
            "Preprocessing documents:  50%|█████     | 20643/41070 [00:12<00:12, 1596.32it/s]\u001b[A\n",
            "Preprocessing documents:  51%|█████     | 20806/41070 [00:12<00:12, 1604.11it/s]\u001b[A\n",
            "Preprocessing documents:  51%|█████     | 20970/41070 [00:12<00:12, 1613.85it/s]\u001b[A\n",
            "Preprocessing documents:  51%|█████▏    | 21142/41070 [00:13<00:12, 1645.12it/s]\u001b[A\n",
            "Preprocessing documents:  52%|█████▏    | 21307/41070 [00:13<00:12, 1630.07it/s]\u001b[A\n",
            "Preprocessing documents:  52%|█████▏    | 21471/41070 [00:13<00:12, 1624.65it/s]\u001b[A\n",
            "Preprocessing documents:  53%|█████▎    | 21639/41070 [00:13<00:11, 1638.78it/s]\u001b[A\n",
            "Preprocessing documents:  53%|█████▎    | 21815/41070 [00:13<00:11, 1673.64it/s]\u001b[A\n",
            "Preprocessing documents:  54%|█████▎    | 21983/41070 [00:13<00:11, 1655.29it/s]\u001b[A\n",
            "Preprocessing documents:  54%|█████▍    | 22149/41070 [00:13<00:11, 1636.37it/s]\u001b[A\n",
            "Preprocessing documents:  54%|█████▍    | 22320/41070 [00:13<00:11, 1656.46it/s]\u001b[A\n",
            "Preprocessing documents:  55%|█████▍    | 22486/41070 [00:13<00:11, 1656.39it/s]\u001b[A\n",
            "Preprocessing documents:  55%|█████▌    | 22652/41070 [00:13<00:11, 1654.13it/s]\u001b[A\n",
            "Preprocessing documents:  56%|█████▌    | 22818/41070 [00:14<00:11, 1655.23it/s]\u001b[A\n",
            "Preprocessing documents:  56%|█████▌    | 22984/41070 [00:14<00:11, 1637.60it/s]\u001b[A\n",
            "Preprocessing documents:  56%|█████▋    | 23148/41070 [00:14<00:11, 1608.45it/s]\u001b[A\n",
            "Preprocessing documents:  57%|█████▋    | 23316/41070 [00:14<00:10, 1627.42it/s]\u001b[A\n",
            "Preprocessing documents:  57%|█████▋    | 23487/41070 [00:14<00:10, 1651.08it/s]\u001b[A\n",
            "Preprocessing documents:  58%|█████▊    | 23653/41070 [00:14<00:10, 1642.29it/s]\u001b[A\n",
            "Preprocessing documents:  58%|█████▊    | 23821/41070 [00:14<00:10, 1650.34it/s]\u001b[A\n",
            "Preprocessing documents:  58%|█████▊    | 23988/41070 [00:14<00:10, 1653.77it/s]\u001b[A\n",
            "Preprocessing documents:  59%|█████▉    | 24161/41070 [00:14<00:10, 1674.21it/s]\u001b[A\n",
            "Preprocessing documents:  59%|█████▉    | 24329/41070 [00:15<00:10, 1657.76it/s]\u001b[A\n",
            "Preprocessing documents:  60%|█████▉    | 24495/41070 [00:15<00:10, 1637.75it/s]\u001b[A\n",
            "Preprocessing documents:  60%|██████    | 24659/41070 [00:15<00:10, 1605.61it/s]\u001b[A\n",
            "Preprocessing documents:  60%|██████    | 24820/41070 [00:15<00:10, 1580.56it/s]\u001b[A\n",
            "Preprocessing documents:  61%|██████    | 24985/41070 [00:15<00:10, 1599.69it/s]\u001b[A\n",
            "Preprocessing documents:  61%|██████    | 25149/41070 [00:15<00:09, 1610.43it/s]\u001b[A\n",
            "Preprocessing documents:  62%|██████▏   | 25313/41070 [00:15<00:09, 1618.61it/s]\u001b[A\n",
            "Preprocessing documents:  62%|██████▏   | 25478/41070 [00:15<00:09, 1625.41it/s]\u001b[A\n",
            "Preprocessing documents:  62%|██████▏   | 25641/41070 [00:15<00:09, 1618.18it/s]\u001b[A\n",
            "Preprocessing documents:  63%|██████▎   | 25808/41070 [00:15<00:09, 1630.94it/s]\u001b[A\n",
            "Preprocessing documents:  63%|██████▎   | 25972/41070 [00:16<00:09, 1568.38it/s]\u001b[A\n",
            "Preprocessing documents:  64%|██████▎   | 26138/41070 [00:16<00:09, 1592.05it/s]\u001b[A\n",
            "Preprocessing documents:  64%|██████▍   | 26310/41070 [00:16<00:09, 1627.53it/s]\u001b[A\n",
            "Preprocessing documents:  64%|██████▍   | 26474/41070 [00:16<00:08, 1627.67it/s]\u001b[A\n",
            "Preprocessing documents:  65%|██████▍   | 26638/41070 [00:16<00:08, 1629.48it/s]\u001b[A\n",
            "Preprocessing documents:  65%|██████▌   | 26803/41070 [00:16<00:08, 1634.25it/s]\u001b[A\n",
            "Preprocessing documents:  66%|██████▌   | 26967/41070 [00:16<00:08, 1625.71it/s]\u001b[A\n",
            "Preprocessing documents:  66%|██████▌   | 27130/41070 [00:16<00:08, 1612.70it/s]\u001b[A\n",
            "Preprocessing documents:  66%|██████▋   | 27292/41070 [00:16<00:08, 1605.12it/s]\u001b[A\n",
            "Preprocessing documents:  67%|██████▋   | 27456/41070 [00:16<00:08, 1612.34it/s]\u001b[A\n",
            "Preprocessing documents:  67%|██████▋   | 27620/41070 [00:17<00:08, 1618.71it/s]\u001b[A\n",
            "Preprocessing documents:  68%|██████▊   | 27791/41070 [00:17<00:08, 1641.44it/s]\u001b[A\n",
            "Preprocessing documents:  68%|██████▊   | 27957/41070 [00:17<00:07, 1645.96it/s]\u001b[A\n",
            "Preprocessing documents:  68%|██████▊   | 28123/41070 [00:17<00:07, 1649.54it/s]\u001b[A\n",
            "Preprocessing documents:  69%|██████▉   | 28291/41070 [00:17<00:07, 1657.38it/s]\u001b[A\n",
            "Preprocessing documents:  69%|██████▉   | 28457/41070 [00:17<00:07, 1651.81it/s]\u001b[A\n",
            "Preprocessing documents:  70%|██████▉   | 28623/41070 [00:17<00:07, 1638.31it/s]\u001b[A\n",
            "Preprocessing documents:  70%|███████   | 28787/41070 [00:17<00:07, 1627.19it/s]\u001b[A\n",
            "Preprocessing documents:  71%|███████   | 28955/41070 [00:17<00:07, 1641.23it/s]\u001b[A\n",
            "Preprocessing documents:  71%|███████   | 29120/41070 [00:17<00:07, 1628.90it/s]\u001b[A\n",
            "Preprocessing documents:  71%|███████▏  | 29283/41070 [00:18<00:07, 1617.53it/s]\u001b[A\n",
            "Preprocessing documents:  72%|███████▏  | 29445/41070 [00:18<00:07, 1603.99it/s]\u001b[A\n",
            "Preprocessing documents:  72%|███████▏  | 29608/41070 [00:18<00:07, 1611.63it/s]\u001b[A\n",
            "Preprocessing documents:  72%|███████▏  | 29775/41070 [00:18<00:06, 1627.48it/s]\u001b[A\n",
            "Preprocessing documents:  73%|███████▎  | 29938/41070 [00:18<00:06, 1611.54it/s]\u001b[A\n",
            "Preprocessing documents:  73%|███████▎  | 30100/41070 [00:18<00:06, 1608.14it/s]\u001b[A\n",
            "Preprocessing documents:  74%|███████▎  | 30262/41070 [00:18<00:06, 1609.87it/s]\u001b[A\n",
            "Preprocessing documents:  74%|███████▍  | 30424/41070 [00:18<00:06, 1612.32it/s]\u001b[A\n",
            "Preprocessing documents:  75%|███████▍  | 30600/41070 [00:18<00:06, 1654.81it/s]\u001b[A\n",
            "Preprocessing documents:  75%|███████▍  | 30766/41070 [00:18<00:06, 1616.57it/s]\u001b[A\n",
            "Preprocessing documents:  75%|███████▌  | 30944/41070 [00:19<00:06, 1662.72it/s]\u001b[A\n",
            "Preprocessing documents:  76%|███████▌  | 31114/41070 [00:19<00:05, 1671.63it/s]\u001b[A\n",
            "Preprocessing documents:  76%|███████▌  | 31282/41070 [00:19<00:05, 1637.33it/s]\u001b[A\n",
            "Preprocessing documents:  77%|███████▋  | 31446/41070 [00:19<00:05, 1617.40it/s]\u001b[A\n",
            "Preprocessing documents:  77%|███████▋  | 31608/41070 [00:19<00:05, 1592.07it/s]\u001b[A\n",
            "Preprocessing documents:  77%|███████▋  | 31768/41070 [00:19<00:05, 1566.75it/s]\u001b[A\n",
            "Preprocessing documents:  78%|███████▊  | 31928/41070 [00:19<00:05, 1575.23it/s]\u001b[A\n",
            "Preprocessing documents:  78%|███████▊  | 32086/41070 [00:19<00:05, 1573.02it/s]\u001b[A\n",
            "Preprocessing documents:  79%|███████▊  | 32245/41070 [00:19<00:05, 1577.26it/s]\u001b[A\n",
            "Preprocessing documents:  79%|███████▉  | 32406/41070 [00:20<00:05, 1584.28it/s]\u001b[A\n",
            "Preprocessing documents:  79%|███████▉  | 32578/41070 [00:20<00:05, 1623.79it/s]\u001b[A\n",
            "Preprocessing documents:  80%|███████▉  | 32741/41070 [00:20<00:05, 1609.53it/s]\u001b[A\n",
            "Preprocessing documents:  80%|████████  | 32903/41070 [00:20<00:05, 1580.02it/s]\u001b[A\n",
            "Preprocessing documents:  81%|████████  | 33085/41070 [00:20<00:04, 1648.52it/s]\u001b[A\n",
            "Preprocessing documents:  81%|████████  | 33251/41070 [00:20<00:04, 1625.77it/s]\u001b[A\n",
            "Preprocessing documents:  81%|████████▏ | 33414/41070 [00:20<00:04, 1590.82it/s]\u001b[A\n",
            "Preprocessing documents:  82%|████████▏ | 33577/41070 [00:20<00:04, 1600.82it/s]\u001b[A\n",
            "Preprocessing documents:  82%|████████▏ | 33743/41070 [00:20<00:04, 1618.00it/s]\u001b[A\n",
            "Preprocessing documents:  83%|████████▎ | 33905/41070 [00:20<00:04, 1614.72it/s]\u001b[A\n",
            "Preprocessing documents:  83%|████████▎ | 34067/41070 [00:21<00:04, 1569.86it/s]\u001b[A\n",
            "Preprocessing documents:  83%|████████▎ | 34232/41070 [00:21<00:04, 1592.13it/s]\u001b[A\n",
            "Preprocessing documents:  84%|████████▎ | 34392/41070 [00:21<00:04, 1580.38it/s]\u001b[A\n",
            "Preprocessing documents:  84%|████████▍ | 34551/41070 [00:21<00:04, 1570.16it/s]\u001b[A\n",
            "Preprocessing documents:  85%|████████▍ | 34709/41070 [00:21<00:04, 1567.97it/s]\u001b[A\n",
            "Preprocessing documents:  85%|████████▍ | 34866/41070 [00:21<00:03, 1556.17it/s]\u001b[A\n",
            "Preprocessing documents:  85%|████████▌ | 35022/41070 [00:21<00:04, 1496.31it/s]\u001b[A\n",
            "Preprocessing documents:  86%|████████▌ | 35190/41070 [00:21<00:03, 1548.26it/s]\u001b[A\n",
            "Preprocessing documents:  86%|████████▌ | 35348/41070 [00:21<00:03, 1557.34it/s]\u001b[A\n",
            "Preprocessing documents:  86%|████████▋ | 35514/41070 [00:21<00:03, 1586.54it/s]\u001b[A\n",
            "Preprocessing documents:  87%|████████▋ | 35673/41070 [00:22<00:03, 1580.78it/s]\u001b[A\n",
            "Preprocessing documents:  87%|████████▋ | 35836/41070 [00:22<00:03, 1593.81it/s]\u001b[A\n",
            "Preprocessing documents:  88%|████████▊ | 35999/41070 [00:22<00:03, 1604.45it/s]\u001b[A\n",
            "Preprocessing documents:  88%|████████▊ | 36170/41070 [00:22<00:03, 1632.54it/s]\u001b[A\n",
            "Preprocessing documents:  88%|████████▊ | 36334/41070 [00:22<00:02, 1624.04it/s]\u001b[A\n",
            "Preprocessing documents:  89%|████████▉ | 36497/41070 [00:22<00:02, 1605.99it/s]\u001b[A\n",
            "Preprocessing documents:  89%|████████▉ | 36658/41070 [00:22<00:02, 1517.87it/s]\u001b[A\n",
            "Preprocessing documents:  90%|████████▉ | 36823/41070 [00:22<00:02, 1555.07it/s]\u001b[A\n",
            "Preprocessing documents:  90%|█████████ | 36984/41070 [00:22<00:02, 1570.88it/s]\u001b[A\n",
            "Preprocessing documents:  90%|█████████ | 37144/41070 [00:23<00:02, 1577.31it/s]\u001b[A\n",
            "Preprocessing documents:  91%|█████████ | 37303/41070 [00:23<00:02, 1571.17it/s]\u001b[A\n",
            "Preprocessing documents:  91%|█████████ | 37461/41070 [00:23<00:02, 1551.19it/s]\u001b[A\n",
            "Preprocessing documents:  92%|█████████▏| 37626/41070 [00:23<00:02, 1577.85it/s]\u001b[A\n",
            "Preprocessing documents:  92%|█████████▏| 37795/41070 [00:23<00:02, 1610.84it/s]\u001b[A\n",
            "Preprocessing documents:  92%|█████████▏| 37957/41070 [00:23<00:01, 1610.89it/s]\u001b[A\n",
            "Preprocessing documents:  93%|█████████▎| 38129/41070 [00:23<00:01, 1643.10it/s]\u001b[A\n",
            "Preprocessing documents:  93%|█████████▎| 38294/41070 [00:23<00:01, 1638.38it/s]\u001b[A\n",
            "Preprocessing documents:  94%|█████████▎| 38463/41070 [00:23<00:01, 1652.55it/s]\u001b[A\n",
            "Preprocessing documents:  94%|█████████▍| 38629/41070 [00:23<00:01, 1648.05it/s]\u001b[A\n",
            "Preprocessing documents:  94%|█████████▍| 38795/41070 [00:24<00:01, 1651.23it/s]\u001b[A\n",
            "Preprocessing documents:  95%|█████████▍| 38961/41070 [00:24<00:01, 1648.69it/s]\u001b[A\n",
            "Preprocessing documents:  95%|█████████▌| 39132/41070 [00:24<00:01, 1664.12it/s]\u001b[A\n",
            "Preprocessing documents:  96%|█████████▌| 39299/41070 [00:24<00:01, 1624.59it/s]\u001b[A\n",
            "Preprocessing documents:  96%|█████████▌| 39478/41070 [00:24<00:00, 1671.98it/s]\u001b[A\n",
            "Preprocessing documents:  97%|█████████▋| 39646/41070 [00:24<00:00, 1647.10it/s]\u001b[A\n",
            "Preprocessing documents:  97%|█████████▋| 39811/41070 [00:24<00:00, 1633.43it/s]\u001b[A\n",
            "Preprocessing documents:  97%|█████████▋| 39975/41070 [00:24<00:00, 1609.68it/s]\u001b[A\n",
            "Preprocessing documents:  98%|█████████▊| 40144/41070 [00:24<00:00, 1631.30it/s]\u001b[A\n",
            "Preprocessing documents:  98%|█████████▊| 40308/41070 [00:24<00:00, 1616.25it/s]\u001b[A\n",
            "Preprocessing documents:  99%|█████████▊| 40476/41070 [00:25<00:00, 1633.82it/s]\u001b[A\n",
            "Preprocessing documents:  99%|█████████▉| 40640/41070 [00:25<00:00, 1616.71it/s]\u001b[A\n",
            "Preprocessing documents:  99%|█████████▉| 40812/41070 [00:25<00:00, 1646.85it/s]\u001b[A\n",
            "Preprocessing documents: 100%|██████████| 41070/41070 [00:25<00:00, 1616.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 preprocessing completed in 25.41 seconds\n",
            "BM25 index construction completed in 0.59 seconds\n",
            "Total BM25 index building completed in 26.02 seconds\n",
            "BM25 index created successfully\n",
            "Average document length: 40.22 tokens\n",
            "\n",
            "Processing msmarco_stsb model...\n",
            "Generating msmarco_stsb embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/321 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/321 [00:00<01:29,  3.57it/s]\u001b[A\n",
            "  1%|          | 2/321 [00:00<01:14,  4.30it/s]\u001b[A\n",
            "  1%|          | 3/321 [00:00<01:08,  4.67it/s]\u001b[A\n",
            "  1%|          | 4/321 [00:00<01:06,  4.79it/s]\u001b[A\n",
            "  2%|▏         | 5/321 [00:01<01:04,  4.92it/s]\u001b[A\n",
            "  2%|▏         | 6/321 [00:01<01:03,  4.93it/s]\u001b[A\n",
            "  2%|▏         | 7/321 [00:01<01:02,  5.00it/s]\u001b[A\n",
            "  2%|▏         | 8/321 [00:01<01:01,  5.05it/s]\u001b[A\n",
            "  3%|▎         | 9/321 [00:01<01:01,  5.08it/s]\u001b[A\n",
            "  3%|▎         | 10/321 [00:02<01:00,  5.12it/s]\u001b[A\n",
            "  3%|▎         | 11/321 [00:02<01:01,  5.08it/s]\u001b[A\n",
            "  4%|▎         | 12/321 [00:02<00:59,  5.16it/s]\u001b[A\n",
            "  4%|▍         | 13/321 [00:02<00:59,  5.21it/s]\u001b[A\n",
            "  4%|▍         | 14/321 [00:02<00:58,  5.26it/s]\u001b[A\n",
            "  5%|▍         | 15/321 [00:02<00:58,  5.27it/s]\u001b[A\n",
            "  5%|▍         | 16/321 [00:03<00:58,  5.23it/s]\u001b[A\n",
            "  5%|▌         | 17/321 [00:03<00:58,  5.20it/s]\u001b[A\n",
            "  6%|▌         | 18/321 [00:03<00:58,  5.18it/s]\u001b[A\n",
            "  6%|▌         | 19/321 [00:03<00:58,  5.15it/s]\u001b[A\n",
            "  6%|▌         | 20/321 [00:03<00:58,  5.15it/s]\u001b[A\n",
            "  7%|▋         | 21/321 [00:04<00:58,  5.15it/s]\u001b[A\n",
            "  7%|▋         | 22/321 [00:04<00:58,  5.14it/s]\u001b[A\n",
            "  7%|▋         | 23/321 [00:04<00:58,  5.11it/s]\u001b[A\n",
            "  7%|▋         | 24/321 [00:04<00:57,  5.14it/s]\u001b[A\n",
            "  8%|▊         | 25/321 [00:04<00:56,  5.23it/s]\u001b[A\n",
            "  8%|▊         | 26/321 [00:05<00:55,  5.32it/s]\u001b[A\n",
            "  8%|▊         | 27/321 [00:05<00:55,  5.31it/s]\u001b[A\n",
            "  9%|▊         | 28/321 [00:05<00:56,  5.21it/s]\u001b[A\n",
            "  9%|▉         | 29/321 [00:05<00:56,  5.15it/s]\u001b[A\n",
            "  9%|▉         | 30/321 [00:05<00:56,  5.11it/s]\u001b[A\n",
            " 10%|▉         | 31/321 [00:06<00:56,  5.16it/s]\u001b[A\n",
            " 10%|▉         | 32/321 [00:06<00:55,  5.23it/s]\u001b[A\n",
            " 10%|█         | 33/321 [00:06<00:55,  5.17it/s]\u001b[A\n",
            " 11%|█         | 34/321 [00:06<00:55,  5.14it/s]\u001b[A\n",
            " 11%|█         | 35/321 [00:06<00:55,  5.15it/s]\u001b[A\n",
            " 11%|█         | 36/321 [00:07<00:54,  5.27it/s]\u001b[A\n",
            " 12%|█▏        | 37/321 [00:07<00:54,  5.18it/s]\u001b[A\n",
            " 12%|█▏        | 38/321 [00:07<00:54,  5.17it/s]\u001b[A\n",
            " 12%|█▏        | 39/321 [00:07<00:54,  5.22it/s]\u001b[A\n",
            " 12%|█▏        | 40/321 [00:07<00:53,  5.22it/s]\u001b[A\n",
            " 13%|█▎        | 41/321 [00:08<00:53,  5.20it/s]\u001b[A\n",
            " 13%|█▎        | 42/321 [00:08<00:53,  5.22it/s]\u001b[A\n",
            " 13%|█▎        | 43/321 [00:08<00:52,  5.29it/s]\u001b[A\n",
            " 14%|█▎        | 44/321 [00:08<00:53,  5.17it/s]\u001b[A\n",
            " 14%|█▍        | 45/321 [00:08<00:52,  5.21it/s]\u001b[A\n",
            " 14%|█▍        | 46/321 [00:08<00:53,  5.19it/s]\u001b[A\n",
            " 15%|█▍        | 47/321 [00:09<00:53,  5.08it/s]\u001b[A\n",
            " 15%|█▍        | 48/321 [00:09<00:53,  5.13it/s]\u001b[A\n",
            " 15%|█▌        | 49/321 [00:09<00:53,  5.06it/s]\u001b[A\n",
            " 16%|█▌        | 50/321 [00:09<00:53,  5.10it/s]\u001b[A\n",
            " 16%|█▌        | 51/321 [00:09<00:53,  5.09it/s]\u001b[A\n",
            " 16%|█▌        | 52/321 [00:10<00:52,  5.10it/s]\u001b[A\n",
            " 17%|█▋        | 53/321 [00:10<00:53,  5.04it/s]\u001b[A\n",
            " 17%|█▋        | 54/321 [00:10<00:52,  5.12it/s]\u001b[A\n",
            " 17%|█▋        | 55/321 [00:10<00:52,  5.09it/s]\u001b[A\n",
            " 17%|█▋        | 56/321 [00:10<00:52,  5.09it/s]\u001b[A\n",
            " 18%|█▊        | 57/321 [00:11<00:51,  5.08it/s]\u001b[A\n",
            " 18%|█▊        | 58/321 [00:11<00:51,  5.14it/s]\u001b[A\n",
            " 18%|█▊        | 59/321 [00:11<00:51,  5.10it/s]\u001b[A\n",
            " 19%|█▊        | 60/321 [00:11<00:50,  5.16it/s]\u001b[A\n",
            " 19%|█▉        | 61/321 [00:11<00:51,  5.08it/s]\u001b[A\n",
            " 19%|█▉        | 62/321 [00:12<00:50,  5.10it/s]\u001b[A\n",
            " 20%|█▉        | 63/321 [00:12<00:50,  5.11it/s]\u001b[A\n",
            " 20%|█▉        | 64/321 [00:12<00:50,  5.12it/s]\u001b[A\n",
            " 20%|██        | 65/321 [00:12<00:49,  5.18it/s]\u001b[A\n",
            " 21%|██        | 66/321 [00:12<00:49,  5.17it/s]\u001b[A\n",
            " 21%|██        | 67/321 [00:13<00:48,  5.21it/s]\u001b[A\n",
            " 21%|██        | 68/321 [00:13<00:48,  5.24it/s]\u001b[A\n",
            " 21%|██▏       | 69/321 [00:13<00:48,  5.21it/s]\u001b[A\n",
            " 22%|██▏       | 70/321 [00:13<00:48,  5.20it/s]\u001b[A\n",
            " 22%|██▏       | 71/321 [00:13<00:47,  5.25it/s]\u001b[A\n",
            " 22%|██▏       | 72/321 [00:14<00:48,  5.19it/s]\u001b[A\n",
            " 23%|██▎       | 73/321 [00:14<00:46,  5.28it/s]\u001b[A\n",
            " 23%|██▎       | 74/321 [00:14<00:46,  5.26it/s]\u001b[A\n",
            " 23%|██▎       | 75/321 [00:14<00:47,  5.14it/s]\u001b[A\n",
            " 24%|██▎       | 76/321 [00:14<00:47,  5.16it/s]\u001b[A\n",
            " 24%|██▍       | 77/321 [00:15<00:47,  5.17it/s]\u001b[A\n",
            " 24%|██▍       | 78/321 [00:15<00:46,  5.21it/s]\u001b[A\n",
            " 25%|██▍       | 79/321 [00:15<00:46,  5.19it/s]\u001b[A\n",
            " 25%|██▍       | 80/321 [00:15<00:46,  5.21it/s]\u001b[A\n",
            " 25%|██▌       | 81/321 [00:15<00:45,  5.24it/s]\u001b[A\n",
            " 26%|██▌       | 82/321 [00:15<00:44,  5.35it/s]\u001b[A\n",
            " 26%|██▌       | 83/321 [00:16<00:45,  5.25it/s]\u001b[A\n",
            " 26%|██▌       | 84/321 [00:16<00:45,  5.18it/s]\u001b[A\n",
            " 26%|██▋       | 85/321 [00:16<00:45,  5.17it/s]\u001b[A\n",
            " 27%|██▋       | 86/321 [00:16<00:44,  5.25it/s]\u001b[A\n",
            " 27%|██▋       | 87/321 [00:16<00:44,  5.28it/s]\u001b[A\n",
            " 27%|██▋       | 88/321 [00:17<00:43,  5.33it/s]\u001b[A\n",
            " 28%|██▊       | 89/321 [00:17<00:44,  5.25it/s]\u001b[A\n",
            " 28%|██▊       | 90/321 [00:17<00:43,  5.27it/s]\u001b[A\n",
            " 28%|██▊       | 91/321 [00:17<00:43,  5.30it/s]\u001b[A\n",
            " 29%|██▊       | 92/321 [00:17<00:42,  5.34it/s]\u001b[A\n",
            " 29%|██▉       | 93/321 [00:18<00:42,  5.38it/s]\u001b[A\n",
            " 29%|██▉       | 94/321 [00:18<00:42,  5.36it/s]\u001b[A\n",
            " 30%|██▉       | 95/321 [00:18<00:41,  5.49it/s]\u001b[A\n",
            " 30%|██▉       | 96/321 [00:18<00:40,  5.53it/s]\u001b[A\n",
            " 30%|███       | 97/321 [00:18<00:41,  5.45it/s]\u001b[A\n",
            " 31%|███       | 98/321 [00:18<00:41,  5.35it/s]\u001b[A\n",
            " 31%|███       | 99/321 [00:19<00:41,  5.38it/s]\u001b[A\n",
            " 31%|███       | 100/321 [00:19<00:41,  5.36it/s]\u001b[A\n",
            " 31%|███▏      | 101/321 [00:19<00:40,  5.44it/s]\u001b[A\n",
            " 32%|███▏      | 102/321 [00:19<00:40,  5.38it/s]\u001b[A\n",
            " 32%|███▏      | 103/321 [00:19<00:40,  5.36it/s]\u001b[A\n",
            " 32%|███▏      | 104/321 [00:20<00:41,  5.21it/s]\u001b[A\n",
            " 33%|███▎      | 105/321 [00:20<00:41,  5.18it/s]\u001b[A\n",
            " 33%|███▎      | 106/321 [00:20<00:41,  5.14it/s]\u001b[A\n",
            " 33%|███▎      | 107/321 [00:20<00:41,  5.12it/s]\u001b[A\n",
            " 34%|███▎      | 108/321 [00:20<00:41,  5.14it/s]\u001b[A\n",
            " 34%|███▍      | 109/321 [00:21<00:40,  5.17it/s]\u001b[A\n",
            " 34%|███▍      | 110/321 [00:21<00:41,  5.13it/s]\u001b[A\n",
            " 35%|███▍      | 111/321 [00:21<00:40,  5.17it/s]\u001b[A\n",
            " 35%|███▍      | 112/321 [00:21<00:40,  5.14it/s]\u001b[A\n",
            " 35%|███▌      | 113/321 [00:21<00:39,  5.25it/s]\u001b[A\n",
            " 36%|███▌      | 114/321 [00:22<00:39,  5.21it/s]\u001b[A\n",
            " 36%|███▌      | 115/321 [00:22<00:39,  5.17it/s]\u001b[A\n",
            " 36%|███▌      | 116/321 [00:22<00:39,  5.18it/s]\u001b[A\n",
            " 36%|███▋      | 117/321 [00:22<00:39,  5.22it/s]\u001b[A\n",
            " 37%|███▋      | 118/321 [00:22<00:38,  5.24it/s]\u001b[A\n",
            " 37%|███▋      | 119/321 [00:22<00:38,  5.27it/s]\u001b[A\n",
            " 37%|███▋      | 120/321 [00:23<00:38,  5.21it/s]\u001b[A\n",
            " 38%|███▊      | 121/321 [00:23<00:38,  5.25it/s]\u001b[A\n",
            " 38%|███▊      | 122/321 [00:23<00:38,  5.23it/s]\u001b[A\n",
            " 38%|███▊      | 123/321 [00:23<00:37,  5.24it/s]\u001b[A\n",
            " 39%|███▊      | 124/321 [00:23<00:37,  5.24it/s]\u001b[A\n",
            " 39%|███▉      | 125/321 [00:24<00:36,  5.42it/s]\u001b[A\n",
            " 39%|███▉      | 126/321 [00:24<00:36,  5.33it/s]\u001b[A\n",
            " 40%|███▉      | 127/321 [00:24<00:35,  5.39it/s]\u001b[A\n",
            " 40%|███▉      | 128/321 [00:24<00:35,  5.39it/s]\u001b[A\n",
            " 40%|████      | 129/321 [00:24<00:36,  5.31it/s]\u001b[A\n",
            " 40%|████      | 130/321 [00:25<00:36,  5.24it/s]\u001b[A\n",
            " 41%|████      | 131/321 [00:25<00:36,  5.20it/s]\u001b[A\n",
            " 41%|████      | 132/321 [00:25<00:35,  5.36it/s]\u001b[A\n",
            " 41%|████▏     | 133/321 [00:25<00:35,  5.26it/s]\u001b[A\n",
            " 42%|████▏     | 134/321 [00:25<00:34,  5.42it/s]\u001b[A\n",
            " 42%|████▏     | 135/321 [00:25<00:34,  5.40it/s]\u001b[A\n",
            " 42%|████▏     | 136/321 [00:26<00:34,  5.33it/s]\u001b[A\n",
            " 43%|████▎     | 137/321 [00:26<00:34,  5.29it/s]\u001b[A\n",
            " 43%|████▎     | 138/321 [00:26<00:34,  5.33it/s]\u001b[A\n",
            " 43%|████▎     | 139/321 [00:26<00:34,  5.27it/s]\u001b[A\n",
            " 44%|████▎     | 140/321 [00:26<00:34,  5.29it/s]\u001b[A\n",
            " 44%|████▍     | 141/321 [00:27<00:33,  5.30it/s]\u001b[A\n",
            " 44%|████▍     | 142/321 [00:27<00:33,  5.42it/s]\u001b[A\n",
            " 45%|████▍     | 143/321 [00:27<00:33,  5.35it/s]\u001b[A\n",
            " 45%|████▍     | 144/321 [00:27<00:33,  5.28it/s]\u001b[A\n",
            " 45%|████▌     | 145/321 [00:27<00:33,  5.28it/s]\u001b[A\n",
            " 45%|████▌     | 146/321 [00:28<00:32,  5.35it/s]\u001b[A\n",
            " 46%|████▌     | 147/321 [00:28<00:32,  5.38it/s]\u001b[A\n",
            " 46%|████▌     | 148/321 [00:28<00:32,  5.35it/s]\u001b[A\n",
            " 46%|████▋     | 149/321 [00:28<00:31,  5.42it/s]\u001b[A\n",
            " 47%|████▋     | 150/321 [00:28<00:32,  5.30it/s]\u001b[A\n",
            " 47%|████▋     | 151/321 [00:28<00:32,  5.26it/s]\u001b[A\n",
            " 47%|████▋     | 152/321 [00:29<00:31,  5.29it/s]\u001b[A\n",
            " 48%|████▊     | 153/321 [00:29<00:32,  5.19it/s]\u001b[A\n",
            " 48%|████▊     | 154/321 [00:29<00:32,  5.20it/s]\u001b[A\n",
            " 48%|████▊     | 155/321 [00:29<00:31,  5.20it/s]\u001b[A\n",
            " 49%|████▊     | 156/321 [00:29<00:31,  5.20it/s]\u001b[A\n",
            " 49%|████▉     | 157/321 [00:30<00:31,  5.17it/s]\u001b[A\n",
            " 49%|████▉     | 158/321 [00:30<00:31,  5.16it/s]\u001b[A\n",
            " 50%|████▉     | 159/321 [00:30<00:31,  5.13it/s]\u001b[A\n",
            " 50%|████▉     | 160/321 [00:30<00:31,  5.11it/s]\u001b[A\n",
            " 50%|█████     | 161/321 [00:30<00:30,  5.23it/s]\u001b[A\n",
            " 50%|█████     | 162/321 [00:31<00:30,  5.17it/s]\u001b[A\n",
            " 51%|█████     | 163/321 [00:31<00:30,  5.17it/s]\u001b[A\n",
            " 51%|█████     | 164/321 [00:31<00:29,  5.27it/s]\u001b[A\n",
            " 51%|█████▏    | 165/321 [00:31<00:29,  5.24it/s]\u001b[A\n",
            " 52%|█████▏    | 166/321 [00:31<00:29,  5.17it/s]\u001b[A\n",
            " 52%|█████▏    | 167/321 [00:32<00:30,  5.13it/s]\u001b[A\n",
            " 52%|█████▏    | 168/321 [00:32<00:29,  5.15it/s]\u001b[A\n",
            " 53%|█████▎    | 169/321 [00:32<00:29,  5.18it/s]\u001b[A\n",
            " 53%|█████▎    | 170/321 [00:32<00:29,  5.19it/s]\u001b[A\n",
            " 53%|█████▎    | 171/321 [00:32<00:28,  5.18it/s]\u001b[A\n",
            " 54%|█████▎    | 172/321 [00:33<00:28,  5.16it/s]\u001b[A\n",
            " 54%|█████▍    | 173/321 [00:33<00:28,  5.12it/s]\u001b[A\n",
            " 54%|█████▍    | 174/321 [00:33<00:28,  5.16it/s]\u001b[A\n",
            " 55%|█████▍    | 175/321 [00:33<00:28,  5.13it/s]\u001b[A\n",
            " 55%|█████▍    | 176/321 [00:33<00:27,  5.26it/s]\u001b[A\n",
            " 55%|█████▌    | 177/321 [00:34<00:27,  5.27it/s]\u001b[A\n",
            " 55%|█████▌    | 178/321 [00:34<00:27,  5.26it/s]\u001b[A\n",
            " 56%|█████▌    | 179/321 [00:34<00:27,  5.25it/s]\u001b[A\n",
            " 56%|█████▌    | 180/321 [00:34<00:27,  5.21it/s]\u001b[A\n",
            " 56%|█████▋    | 181/321 [00:34<00:26,  5.20it/s]\u001b[A\n",
            " 57%|█████▋    | 182/321 [00:34<00:26,  5.20it/s]\u001b[A\n",
            " 57%|█████▋    | 183/321 [00:35<00:26,  5.24it/s]\u001b[A\n",
            " 57%|█████▋    | 184/321 [00:35<00:25,  5.28it/s]\u001b[A\n",
            " 58%|█████▊    | 185/321 [00:35<00:25,  5.30it/s]\u001b[A\n",
            " 58%|█████▊    | 186/321 [00:35<00:25,  5.30it/s]\u001b[A\n",
            " 58%|█████▊    | 187/321 [00:35<00:25,  5.35it/s]\u001b[A\n",
            " 59%|█████▊    | 188/321 [00:36<00:24,  5.37it/s]\u001b[A\n",
            " 59%|█████▉    | 189/321 [00:36<00:24,  5.36it/s]\u001b[A\n",
            " 59%|█████▉    | 190/321 [00:36<00:24,  5.33it/s]\u001b[A\n",
            " 60%|█████▉    | 191/321 [00:36<00:24,  5.29it/s]\u001b[A\n",
            " 60%|█████▉    | 192/321 [00:36<00:24,  5.24it/s]\u001b[A\n",
            " 60%|██████    | 193/321 [00:37<00:24,  5.20it/s]\u001b[A\n",
            " 60%|██████    | 194/321 [00:37<00:24,  5.15it/s]\u001b[A\n",
            " 61%|██████    | 195/321 [00:37<00:24,  5.13it/s]\u001b[A\n",
            " 61%|██████    | 196/321 [00:37<00:24,  5.12it/s]\u001b[A\n",
            " 61%|██████▏   | 197/321 [00:37<00:23,  5.20it/s]\u001b[A\n",
            " 62%|██████▏   | 198/321 [00:38<00:23,  5.21it/s]\u001b[A\n",
            " 62%|██████▏   | 199/321 [00:38<00:23,  5.22it/s]\u001b[A\n",
            " 62%|██████▏   | 200/321 [00:38<00:23,  5.24it/s]\u001b[A\n",
            " 63%|██████▎   | 201/321 [00:38<00:22,  5.22it/s]\u001b[A\n",
            " 63%|██████▎   | 202/321 [00:38<00:22,  5.23it/s]\u001b[A\n",
            " 63%|██████▎   | 203/321 [00:38<00:22,  5.16it/s]\u001b[A\n",
            " 64%|██████▎   | 204/321 [00:39<00:22,  5.13it/s]\u001b[A\n",
            " 64%|██████▍   | 205/321 [00:39<00:22,  5.16it/s]\u001b[A\n",
            " 64%|██████▍   | 206/321 [00:39<00:22,  5.13it/s]\u001b[A\n",
            " 64%|██████▍   | 207/321 [00:39<00:21,  5.22it/s]\u001b[A\n",
            " 65%|██████▍   | 208/321 [00:39<00:21,  5.27it/s]\u001b[A\n",
            " 65%|██████▌   | 209/321 [00:40<00:21,  5.23it/s]\u001b[A\n",
            " 65%|██████▌   | 210/321 [00:40<00:20,  5.29it/s]\u001b[A\n",
            " 66%|██████▌   | 211/321 [00:40<00:20,  5.24it/s]\u001b[A\n",
            " 66%|██████▌   | 212/321 [00:40<00:20,  5.22it/s]\u001b[A\n",
            " 66%|██████▋   | 213/321 [00:40<00:20,  5.17it/s]\u001b[A\n",
            " 67%|██████▋   | 214/321 [00:41<00:20,  5.10it/s]\u001b[A\n",
            " 67%|██████▋   | 215/321 [00:41<00:20,  5.15it/s]\u001b[A\n",
            " 67%|██████▋   | 216/321 [00:41<00:20,  5.14it/s]\u001b[A\n",
            " 68%|██████▊   | 217/321 [00:41<00:20,  5.18it/s]\u001b[A\n",
            " 68%|██████▊   | 218/321 [00:41<00:20,  5.14it/s]\u001b[A\n",
            " 68%|██████▊   | 219/321 [00:42<00:20,  5.10it/s]\u001b[A\n",
            " 69%|██████▊   | 220/321 [00:42<00:19,  5.13it/s]\u001b[A\n",
            " 69%|██████▉   | 221/321 [00:42<00:19,  5.22it/s]\u001b[A\n",
            " 69%|██████▉   | 222/321 [00:42<00:18,  5.22it/s]\u001b[A\n",
            " 69%|██████▉   | 223/321 [00:42<00:18,  5.22it/s]\u001b[A\n",
            " 70%|██████▉   | 224/321 [00:43<00:19,  5.10it/s]\u001b[A\n",
            " 70%|███████   | 225/321 [00:43<00:18,  5.10it/s]\u001b[A\n",
            " 70%|███████   | 226/321 [00:43<00:18,  5.20it/s]\u001b[A\n",
            " 71%|███████   | 227/321 [00:43<00:17,  5.28it/s]\u001b[A\n",
            " 71%|███████   | 228/321 [00:43<00:18,  5.15it/s]\u001b[A\n",
            " 71%|███████▏  | 229/321 [00:43<00:17,  5.18it/s]\u001b[A\n",
            " 72%|███████▏  | 230/321 [00:44<00:17,  5.10it/s]\u001b[A\n",
            " 72%|███████▏  | 231/321 [00:44<00:17,  5.14it/s]\u001b[A\n",
            " 72%|███████▏  | 232/321 [00:44<00:17,  5.16it/s]\u001b[A\n",
            " 73%|███████▎  | 233/321 [00:44<00:17,  5.13it/s]\u001b[A\n",
            " 73%|███████▎  | 234/321 [00:44<00:16,  5.15it/s]\u001b[A\n",
            " 73%|███████▎  | 235/321 [00:45<00:16,  5.13it/s]\u001b[A\n",
            " 74%|███████▎  | 236/321 [00:45<00:16,  5.11it/s]\u001b[A\n",
            " 74%|███████▍  | 237/321 [00:45<00:16,  5.10it/s]\u001b[A\n",
            " 74%|███████▍  | 238/321 [00:45<00:16,  5.12it/s]\u001b[A\n",
            " 74%|███████▍  | 239/321 [00:45<00:15,  5.16it/s]\u001b[A\n",
            " 75%|███████▍  | 240/321 [00:46<00:15,  5.20it/s]\u001b[A\n",
            " 75%|███████▌  | 241/321 [00:46<00:15,  5.14it/s]\u001b[A\n",
            " 75%|███████▌  | 242/321 [00:46<00:15,  5.18it/s]\u001b[A\n",
            " 76%|███████▌  | 243/321 [00:46<00:14,  5.24it/s]\u001b[A\n",
            " 76%|███████▌  | 244/321 [00:46<00:14,  5.17it/s]\u001b[A\n",
            " 76%|███████▋  | 245/321 [00:47<00:14,  5.09it/s]\u001b[A\n",
            " 77%|███████▋  | 246/321 [00:47<00:14,  5.07it/s]\u001b[A\n",
            " 77%|███████▋  | 247/321 [00:47<00:14,  5.02it/s]\u001b[A\n",
            " 77%|███████▋  | 248/321 [00:47<00:14,  5.03it/s]\u001b[A\n",
            " 78%|███████▊  | 249/321 [00:47<00:14,  4.99it/s]\u001b[A\n",
            " 78%|███████▊  | 250/321 [00:48<00:13,  5.13it/s]\u001b[A\n",
            " 78%|███████▊  | 251/321 [00:48<00:13,  5.14it/s]\u001b[A\n",
            " 79%|███████▊  | 252/321 [00:48<00:13,  5.18it/s]\u001b[A\n",
            " 79%|███████▉  | 253/321 [00:48<00:13,  5.18it/s]\u001b[A\n",
            " 79%|███████▉  | 254/321 [00:48<00:12,  5.16it/s]\u001b[A\n",
            " 79%|███████▉  | 255/321 [00:49<00:12,  5.15it/s]\u001b[A\n",
            " 80%|███████▉  | 256/321 [00:49<00:12,  5.15it/s]\u001b[A\n",
            " 80%|████████  | 257/321 [00:49<00:12,  5.15it/s]\u001b[A\n",
            " 80%|████████  | 258/321 [00:49<00:11,  5.30it/s]\u001b[A\n",
            " 81%|████████  | 259/321 [00:49<00:11,  5.24it/s]\u001b[A\n",
            " 81%|████████  | 260/321 [00:50<00:11,  5.10it/s]\u001b[A\n",
            " 81%|████████▏ | 261/321 [00:50<00:11,  5.11it/s]\u001b[A\n",
            " 82%|████████▏ | 262/321 [00:50<00:11,  5.18it/s]\u001b[A\n",
            " 82%|████████▏ | 263/321 [00:50<00:11,  5.18it/s]\u001b[A\n",
            " 82%|████████▏ | 264/321 [00:50<00:10,  5.22it/s]\u001b[A\n",
            " 83%|████████▎ | 265/321 [00:50<00:10,  5.30it/s]\u001b[A\n",
            " 83%|████████▎ | 266/321 [00:51<00:10,  5.22it/s]\u001b[A\n",
            " 83%|████████▎ | 267/321 [00:51<00:10,  5.19it/s]\u001b[A\n",
            " 83%|████████▎ | 268/321 [00:51<00:10,  5.22it/s]\u001b[A\n",
            " 84%|████████▍ | 269/321 [00:51<00:10,  5.15it/s]\u001b[A\n",
            " 84%|████████▍ | 270/321 [00:51<00:09,  5.20it/s]\u001b[A\n",
            " 84%|████████▍ | 271/321 [00:52<00:09,  5.22it/s]\u001b[A\n",
            " 85%|████████▍ | 272/321 [00:52<00:09,  5.13it/s]\u001b[A\n",
            " 85%|████████▌ | 273/321 [00:52<00:09,  5.10it/s]\u001b[A\n",
            " 85%|████████▌ | 274/321 [00:52<00:09,  5.18it/s]\u001b[A\n",
            " 86%|████████▌ | 275/321 [00:52<00:08,  5.17it/s]\u001b[A\n",
            " 86%|████████▌ | 276/321 [00:53<00:08,  5.26it/s]\u001b[A\n",
            " 86%|████████▋ | 277/321 [00:53<00:08,  5.32it/s]\u001b[A\n",
            " 87%|████████▋ | 278/321 [00:53<00:08,  5.33it/s]\u001b[A\n",
            " 87%|████████▋ | 279/321 [00:53<00:08,  5.23it/s]\u001b[A\n",
            " 87%|████████▋ | 280/321 [00:53<00:07,  5.18it/s]\u001b[A\n",
            " 88%|████████▊ | 281/321 [00:54<00:07,  5.18it/s]\u001b[A\n",
            " 88%|████████▊ | 282/321 [00:54<00:07,  5.25it/s]\u001b[A\n",
            " 88%|████████▊ | 283/321 [00:54<00:07,  5.32it/s]\u001b[A\n",
            " 88%|████████▊ | 284/321 [00:54<00:07,  5.25it/s]\u001b[A\n",
            " 89%|████████▉ | 285/321 [00:54<00:06,  5.20it/s]\u001b[A\n",
            " 89%|████████▉ | 286/321 [00:55<00:06,  5.13it/s]\u001b[A\n",
            " 89%|████████▉ | 287/321 [00:55<00:06,  5.20it/s]\u001b[A\n",
            " 90%|████████▉ | 288/321 [00:55<00:06,  5.33it/s]\u001b[A\n",
            " 90%|█████████ | 289/321 [00:55<00:05,  5.39it/s]\u001b[A\n",
            " 90%|█████████ | 290/321 [00:55<00:05,  5.36it/s]\u001b[A\n",
            " 91%|█████████ | 291/321 [00:55<00:05,  5.33it/s]\u001b[A\n",
            " 91%|█████████ | 292/321 [00:56<00:05,  5.24it/s]\u001b[A\n",
            " 91%|█████████▏| 293/321 [00:56<00:05,  5.17it/s]\u001b[A\n",
            " 92%|█████████▏| 294/321 [00:56<00:05,  5.27it/s]\u001b[A\n",
            " 92%|█████████▏| 295/321 [00:56<00:04,  5.24it/s]\u001b[A\n",
            " 92%|█████████▏| 296/321 [00:56<00:04,  5.27it/s]\u001b[A\n",
            " 93%|█████████▎| 297/321 [00:57<00:04,  5.14it/s]\u001b[A\n",
            " 93%|█████████▎| 298/321 [00:57<00:04,  5.26it/s]\u001b[A\n",
            " 93%|█████████▎| 299/321 [00:57<00:04,  5.26it/s]\u001b[A\n",
            " 93%|█████████▎| 300/321 [00:57<00:03,  5.39it/s]\u001b[A\n",
            " 94%|█████████▍| 301/321 [00:57<00:03,  5.25it/s]\u001b[A\n",
            " 94%|█████████▍| 302/321 [00:58<00:03,  5.25it/s]\u001b[A\n",
            " 94%|█████████▍| 303/321 [00:58<00:03,  5.39it/s]\u001b[A\n",
            " 95%|█████████▍| 304/321 [00:58<00:03,  5.32it/s]\u001b[A\n",
            " 95%|█████████▌| 305/321 [00:58<00:03,  5.30it/s]\u001b[A\n",
            " 95%|█████████▌| 306/321 [00:58<00:02,  5.26it/s]\u001b[A\n",
            " 96%|█████████▌| 307/321 [00:59<00:02,  5.16it/s]\u001b[A\n",
            " 96%|█████████▌| 308/321 [00:59<00:02,  5.28it/s]\u001b[A\n",
            " 96%|█████████▋| 309/321 [00:59<00:02,  5.32it/s]\u001b[A\n",
            " 97%|█████████▋| 310/321 [00:59<00:02,  5.32it/s]\u001b[A\n",
            " 97%|█████████▋| 311/321 [00:59<00:01,  5.36it/s]\u001b[A\n",
            " 97%|█████████▋| 312/321 [00:59<00:01,  5.28it/s]\u001b[A\n",
            " 98%|█████████▊| 313/321 [01:00<00:01,  5.26it/s]\u001b[A\n",
            " 98%|█████████▊| 314/321 [01:00<00:01,  5.23it/s]\u001b[A\n",
            " 98%|█████████▊| 315/321 [01:00<00:01,  5.18it/s]\u001b[A\n",
            " 98%|█████████▊| 316/321 [01:00<00:00,  5.20it/s]\u001b[A\n",
            " 99%|█████████▉| 317/321 [01:00<00:00,  5.24it/s]\u001b[A\n",
            " 99%|█████████▉| 318/321 [01:01<00:00,  5.28it/s]\u001b[A\n",
            " 99%|█████████▉| 319/321 [01:01<00:00,  5.34it/s]\u001b[A\n",
            "100%|█████████▉| 320/321 [01:01<00:00,  5.34it/s]\u001b[A\n",
            "100%|██████████| 321/321 [01:01<00:00,  5.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "msmarco_stsb encoding completed in 61.65 seconds\n",
            "Generated 41070 embeddings with dimension 768\n",
            "Processing speed: 666.15 docs/sec\n",
            "Normalizing msmarco_stsb vectors...\n",
            "Building msmarco_stsb Flat index...\n",
            "msmarco_stsb Flat index built with 41070 vectors\n",
            "Building msmarco_stsb HNSW index...\n",
            "msmarco_stsb HNSW index built with 41070 vectors\n",
            "Building msmarco_stsb IVF-PQ index...\n",
            "msmarco_stsb IVF-PQ index built with 41070 vectors\n",
            "\n",
            "Processing stsb model...\n",
            "Generating stsb embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/321 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/321 [00:00<01:11,  4.48it/s]\u001b[A\n",
            "  1%|          | 2/321 [00:00<01:07,  4.75it/s]\u001b[A\n",
            "  1%|          | 3/321 [00:00<01:04,  4.94it/s]\u001b[A\n",
            "  1%|          | 4/321 [00:00<01:03,  4.97it/s]\u001b[A\n",
            "  2%|▏         | 5/321 [00:01<01:02,  5.04it/s]\u001b[A\n",
            "  2%|▏         | 6/321 [00:01<01:02,  5.01it/s]\u001b[A\n",
            "  2%|▏         | 7/321 [00:01<01:02,  5.05it/s]\u001b[A\n",
            "  2%|▏         | 8/321 [00:01<01:01,  5.08it/s]\u001b[A\n",
            "  3%|▎         | 9/321 [00:01<01:01,  5.10it/s]\u001b[A\n",
            "  3%|▎         | 10/321 [00:01<01:00,  5.13it/s]\u001b[A\n",
            "  3%|▎         | 11/321 [00:02<01:00,  5.10it/s]\u001b[A\n",
            "  4%|▎         | 12/321 [00:02<00:59,  5.18it/s]\u001b[A\n",
            "  4%|▍         | 13/321 [00:02<00:58,  5.23it/s]\u001b[A\n",
            "  4%|▍         | 14/321 [00:02<00:58,  5.28it/s]\u001b[A\n",
            "  5%|▍         | 15/321 [00:02<00:57,  5.30it/s]\u001b[A\n",
            "  5%|▍         | 16/321 [00:03<00:57,  5.28it/s]\u001b[A\n",
            "  5%|▌         | 17/321 [00:03<00:57,  5.25it/s]\u001b[A\n",
            "  6%|▌         | 18/321 [00:03<00:58,  5.22it/s]\u001b[A\n",
            "  6%|▌         | 19/321 [00:03<00:57,  5.21it/s]\u001b[A\n",
            "  6%|▌         | 20/321 [00:03<00:57,  5.21it/s]\u001b[A\n",
            "  7%|▋         | 21/321 [00:04<00:57,  5.20it/s]\u001b[A\n",
            "  7%|▋         | 22/321 [00:04<00:57,  5.20it/s]\u001b[A\n",
            "  7%|▋         | 23/321 [00:04<00:57,  5.17it/s]\u001b[A\n",
            "  7%|▋         | 24/321 [00:04<00:57,  5.19it/s]\u001b[A\n",
            "  8%|▊         | 25/321 [00:04<00:56,  5.28it/s]\u001b[A\n",
            "  8%|▊         | 26/321 [00:05<00:54,  5.36it/s]\u001b[A\n",
            "  8%|▊         | 27/321 [00:05<00:54,  5.37it/s]\u001b[A\n",
            "  9%|▊         | 28/321 [00:05<00:55,  5.28it/s]\u001b[A\n",
            "  9%|▉         | 29/321 [00:05<00:55,  5.22it/s]\u001b[A\n",
            "  9%|▉         | 30/321 [00:05<00:56,  5.18it/s]\u001b[A\n",
            " 10%|▉         | 31/321 [00:05<00:55,  5.22it/s]\u001b[A\n",
            " 10%|▉         | 32/321 [00:06<00:54,  5.28it/s]\u001b[A\n",
            " 10%|█         | 33/321 [00:06<00:55,  5.20it/s]\u001b[A\n",
            " 11%|█         | 34/321 [00:06<00:55,  5.17it/s]\u001b[A\n",
            " 11%|█         | 35/321 [00:06<00:55,  5.17it/s]\u001b[A\n",
            " 11%|█         | 36/321 [00:06<00:53,  5.29it/s]\u001b[A\n",
            " 12%|█▏        | 37/321 [00:07<00:54,  5.21it/s]\u001b[A\n",
            " 12%|█▏        | 38/321 [00:07<00:54,  5.21it/s]\u001b[A\n",
            " 12%|█▏        | 39/321 [00:07<00:53,  5.23it/s]\u001b[A\n",
            " 12%|█▏        | 40/321 [00:07<00:53,  5.20it/s]\u001b[A\n",
            " 13%|█▎        | 41/321 [00:07<00:53,  5.20it/s]\u001b[A\n",
            " 13%|█▎        | 42/321 [00:08<00:53,  5.20it/s]\u001b[A\n",
            " 13%|█▎        | 43/321 [00:08<00:52,  5.25it/s]\u001b[A\n",
            " 14%|█▎        | 44/321 [00:08<00:53,  5.14it/s]\u001b[A\n",
            " 14%|█▍        | 45/321 [00:08<00:53,  5.18it/s]\u001b[A\n",
            " 14%|█▍        | 46/321 [00:08<00:53,  5.17it/s]\u001b[A\n",
            " 15%|█▍        | 47/321 [00:09<00:54,  5.07it/s]\u001b[A\n",
            " 15%|█▍        | 48/321 [00:09<00:53,  5.11it/s]\u001b[A\n",
            " 15%|█▌        | 49/321 [00:09<00:54,  5.02it/s]\u001b[A\n",
            " 16%|█▌        | 50/321 [00:09<00:53,  5.06it/s]\u001b[A\n",
            " 16%|█▌        | 51/321 [00:09<00:53,  5.05it/s]\u001b[A\n",
            " 16%|█▌        | 52/321 [00:10<00:53,  5.05it/s]\u001b[A\n",
            " 17%|█▋        | 53/321 [00:10<00:53,  5.01it/s]\u001b[A\n",
            " 17%|█▋        | 54/321 [00:10<00:52,  5.10it/s]\u001b[A\n",
            " 17%|█▋        | 55/321 [00:10<00:52,  5.07it/s]\u001b[A\n",
            " 17%|█▋        | 56/321 [00:10<00:52,  5.08it/s]\u001b[A\n",
            " 18%|█▊        | 57/321 [00:11<00:51,  5.10it/s]\u001b[A\n",
            " 18%|█▊        | 58/321 [00:11<00:50,  5.17it/s]\u001b[A\n",
            " 18%|█▊        | 59/321 [00:11<00:51,  5.12it/s]\u001b[A\n",
            " 19%|█▊        | 60/321 [00:11<00:50,  5.17it/s]\u001b[A\n",
            " 19%|█▉        | 61/321 [00:11<00:51,  5.09it/s]\u001b[A\n",
            " 19%|█▉        | 62/321 [00:12<00:50,  5.09it/s]\u001b[A\n",
            " 20%|█▉        | 63/321 [00:12<00:50,  5.11it/s]\u001b[A\n",
            " 20%|█▉        | 64/321 [00:12<00:50,  5.14it/s]\u001b[A\n",
            " 20%|██        | 65/321 [00:12<00:49,  5.19it/s]\u001b[A\n",
            " 21%|██        | 66/321 [00:12<00:49,  5.16it/s]\u001b[A\n",
            " 21%|██        | 67/321 [00:12<00:48,  5.21it/s]\u001b[A\n",
            " 21%|██        | 68/321 [00:13<00:48,  5.23it/s]\u001b[A\n",
            " 21%|██▏       | 69/321 [00:13<00:48,  5.21it/s]\u001b[A\n",
            " 22%|██▏       | 70/321 [00:13<00:48,  5.20it/s]\u001b[A\n",
            " 22%|██▏       | 71/321 [00:13<00:47,  5.23it/s]\u001b[A\n",
            " 22%|██▏       | 72/321 [00:13<00:48,  5.16it/s]\u001b[A\n",
            " 23%|██▎       | 73/321 [00:14<00:47,  5.25it/s]\u001b[A\n",
            " 23%|██▎       | 74/321 [00:14<00:47,  5.23it/s]\u001b[A\n",
            " 23%|██▎       | 75/321 [00:14<00:47,  5.13it/s]\u001b[A\n",
            " 24%|██▎       | 76/321 [00:14<00:47,  5.15it/s]\u001b[A\n",
            " 24%|██▍       | 77/321 [00:14<00:47,  5.17it/s]\u001b[A\n",
            " 24%|██▍       | 78/321 [00:15<00:46,  5.22it/s]\u001b[A\n",
            " 25%|██▍       | 79/321 [00:15<00:46,  5.19it/s]\u001b[A\n",
            " 25%|██▍       | 80/321 [00:15<00:46,  5.23it/s]\u001b[A\n",
            " 25%|██▌       | 81/321 [00:15<00:45,  5.25it/s]\u001b[A\n",
            " 26%|██▌       | 82/321 [00:15<00:44,  5.38it/s]\u001b[A\n",
            " 26%|██▌       | 83/321 [00:16<00:45,  5.28it/s]\u001b[A\n",
            " 26%|██▌       | 84/321 [00:16<00:45,  5.21it/s]\u001b[A\n",
            " 26%|██▋       | 85/321 [00:16<00:45,  5.20it/s]\u001b[A\n",
            " 27%|██▋       | 86/321 [00:16<00:44,  5.28it/s]\u001b[A\n",
            " 27%|██▋       | 87/321 [00:16<00:44,  5.29it/s]\u001b[A\n",
            " 27%|██▋       | 88/321 [00:16<00:43,  5.33it/s]\u001b[A\n",
            " 28%|██▊       | 89/321 [00:17<00:44,  5.26it/s]\u001b[A\n",
            " 28%|██▊       | 90/321 [00:17<00:43,  5.27it/s]\u001b[A\n",
            " 28%|██▊       | 91/321 [00:17<00:43,  5.30it/s]\u001b[A\n",
            " 29%|██▊       | 92/321 [00:17<00:42,  5.35it/s]\u001b[A\n",
            " 29%|██▉       | 93/321 [00:17<00:42,  5.37it/s]\u001b[A\n",
            " 29%|██▉       | 94/321 [00:18<00:42,  5.35it/s]\u001b[A\n",
            " 30%|██▉       | 95/321 [00:18<00:41,  5.47it/s]\u001b[A\n",
            " 30%|██▉       | 96/321 [00:18<00:40,  5.52it/s]\u001b[A\n",
            " 30%|███       | 97/321 [00:18<00:41,  5.44it/s]\u001b[A\n",
            " 31%|███       | 98/321 [00:18<00:41,  5.35it/s]\u001b[A\n",
            " 31%|███       | 99/321 [00:19<00:41,  5.38it/s]\u001b[A\n",
            " 31%|███       | 100/321 [00:19<00:41,  5.36it/s]\u001b[A\n",
            " 31%|███▏      | 101/321 [00:19<00:40,  5.47it/s]\u001b[A\n",
            " 32%|███▏      | 102/321 [00:19<00:40,  5.40it/s]\u001b[A\n",
            " 32%|███▏      | 103/321 [00:19<00:40,  5.36it/s]\u001b[A\n",
            " 32%|███▏      | 104/321 [00:19<00:41,  5.22it/s]\u001b[A\n",
            " 33%|███▎      | 105/321 [00:20<00:41,  5.16it/s]\u001b[A\n",
            " 33%|███▎      | 106/321 [00:20<00:41,  5.12it/s]\u001b[A\n",
            " 33%|███▎      | 107/321 [00:20<00:41,  5.12it/s]\u001b[A\n",
            " 34%|███▎      | 108/321 [00:20<00:41,  5.13it/s]\u001b[A\n",
            " 34%|███▍      | 109/321 [00:20<00:41,  5.17it/s]\u001b[A\n",
            " 34%|███▍      | 110/321 [00:21<00:40,  5.15it/s]\u001b[A\n",
            " 35%|███▍      | 111/321 [00:21<00:40,  5.19it/s]\u001b[A\n",
            " 35%|███▍      | 112/321 [00:21<00:40,  5.16it/s]\u001b[A\n",
            " 35%|███▌      | 113/321 [00:21<00:39,  5.25it/s]\u001b[A\n",
            " 36%|███▌      | 114/321 [00:21<00:39,  5.22it/s]\u001b[A\n",
            " 36%|███▌      | 115/321 [00:22<00:39,  5.18it/s]\u001b[A\n",
            " 36%|███▌      | 116/321 [00:22<00:39,  5.18it/s]\u001b[A\n",
            " 36%|███▋      | 117/321 [00:22<00:39,  5.22it/s]\u001b[A\n",
            " 37%|███▋      | 118/321 [00:22<00:38,  5.24it/s]\u001b[A\n",
            " 37%|███▋      | 119/321 [00:22<00:38,  5.25it/s]\u001b[A\n",
            " 37%|███▋      | 120/321 [00:23<00:38,  5.17it/s]\u001b[A\n",
            " 38%|███▊      | 121/321 [00:23<00:38,  5.21it/s]\u001b[A\n",
            " 38%|███▊      | 122/321 [00:23<00:38,  5.19it/s]\u001b[A\n",
            " 38%|███▊      | 123/321 [00:23<00:38,  5.21it/s]\u001b[A\n",
            " 39%|███▊      | 124/321 [00:23<00:37,  5.19it/s]\u001b[A\n",
            " 39%|███▉      | 125/321 [00:24<00:36,  5.36it/s]\u001b[A\n",
            " 39%|███▉      | 126/321 [00:24<00:37,  5.26it/s]\u001b[A\n",
            " 40%|███▉      | 127/321 [00:24<00:36,  5.32it/s]\u001b[A\n",
            " 40%|███▉      | 128/321 [00:24<00:36,  5.32it/s]\u001b[A\n",
            " 40%|████      | 129/321 [00:24<00:36,  5.27it/s]\u001b[A\n",
            " 40%|████      | 130/321 [00:24<00:36,  5.21it/s]\u001b[A\n",
            " 41%|████      | 131/321 [00:25<00:36,  5.17it/s]\u001b[A\n",
            " 41%|████      | 132/321 [00:25<00:35,  5.32it/s]\u001b[A\n",
            " 41%|████▏     | 133/321 [00:25<00:35,  5.22it/s]\u001b[A\n",
            " 42%|████▏     | 134/321 [00:25<00:34,  5.39it/s]\u001b[A\n",
            " 42%|████▏     | 135/321 [00:25<00:34,  5.38it/s]\u001b[A\n",
            " 42%|████▏     | 136/321 [00:26<00:34,  5.30it/s]\u001b[A\n",
            " 43%|████▎     | 137/321 [00:26<00:35,  5.24it/s]\u001b[A\n",
            " 43%|████▎     | 138/321 [00:26<00:34,  5.28it/s]\u001b[A\n",
            " 43%|████▎     | 139/321 [00:26<00:34,  5.23it/s]\u001b[A\n",
            " 44%|████▎     | 140/321 [00:26<00:34,  5.26it/s]\u001b[A\n",
            " 44%|████▍     | 141/321 [00:27<00:34,  5.26it/s]\u001b[A\n",
            " 44%|████▍     | 142/321 [00:27<00:33,  5.37it/s]\u001b[A\n",
            " 45%|████▍     | 143/321 [00:27<00:33,  5.31it/s]\u001b[A\n",
            " 45%|████▍     | 144/321 [00:27<00:33,  5.25it/s]\u001b[A\n",
            " 45%|████▌     | 145/321 [00:27<00:33,  5.23it/s]\u001b[A\n",
            " 45%|████▌     | 146/321 [00:28<00:33,  5.28it/s]\u001b[A\n",
            " 46%|████▌     | 147/321 [00:28<00:32,  5.33it/s]\u001b[A\n",
            " 46%|████▌     | 148/321 [00:28<00:32,  5.30it/s]\u001b[A\n",
            " 46%|████▋     | 149/321 [00:28<00:32,  5.37it/s]\u001b[A\n",
            " 47%|████▋     | 150/321 [00:28<00:32,  5.26it/s]\u001b[A\n",
            " 47%|████▋     | 151/321 [00:28<00:32,  5.23it/s]\u001b[A\n",
            " 47%|████▋     | 152/321 [00:29<00:32,  5.25it/s]\u001b[A\n",
            " 48%|████▊     | 153/321 [00:29<00:32,  5.15it/s]\u001b[A\n",
            " 48%|████▊     | 154/321 [00:29<00:32,  5.16it/s]\u001b[A\n",
            " 48%|████▊     | 155/321 [00:29<00:32,  5.17it/s]\u001b[A\n",
            " 49%|████▊     | 156/321 [00:29<00:31,  5.17it/s]\u001b[A\n",
            " 49%|████▉     | 157/321 [00:30<00:31,  5.14it/s]\u001b[A\n",
            " 49%|████▉     | 158/321 [00:30<00:31,  5.12it/s]\u001b[A\n",
            " 50%|████▉     | 159/321 [00:30<00:31,  5.10it/s]\u001b[A\n",
            " 50%|████▉     | 160/321 [00:30<00:31,  5.08it/s]\u001b[A\n",
            " 50%|█████     | 161/321 [00:30<00:30,  5.21it/s]\u001b[A\n",
            " 50%|█████     | 162/321 [00:31<00:30,  5.14it/s]\u001b[A\n",
            " 51%|█████     | 163/321 [00:31<00:30,  5.14it/s]\u001b[A\n",
            " 51%|█████     | 164/321 [00:31<00:29,  5.25it/s]\u001b[A\n",
            " 51%|█████▏    | 165/321 [00:31<00:29,  5.25it/s]\u001b[A\n",
            " 52%|█████▏    | 166/321 [00:31<00:30,  5.15it/s]\u001b[A\n",
            " 52%|█████▏    | 167/321 [00:32<00:30,  5.10it/s]\u001b[A\n",
            " 52%|█████▏    | 168/321 [00:32<00:29,  5.12it/s]\u001b[A\n",
            " 53%|█████▎    | 169/321 [00:32<00:29,  5.15it/s]\u001b[A\n",
            " 53%|█████▎    | 170/321 [00:32<00:29,  5.15it/s]\u001b[A\n",
            " 53%|█████▎    | 171/321 [00:32<00:29,  5.15it/s]\u001b[A\n",
            " 54%|█████▎    | 172/321 [00:33<00:29,  5.09it/s]\u001b[A\n",
            " 54%|█████▍    | 173/321 [00:33<00:29,  5.07it/s]\u001b[A\n",
            " 54%|█████▍    | 174/321 [00:33<00:28,  5.12it/s]\u001b[A\n",
            " 55%|█████▍    | 175/321 [00:33<00:28,  5.12it/s]\u001b[A\n",
            " 55%|█████▍    | 176/321 [00:33<00:27,  5.22it/s]\u001b[A\n",
            " 55%|█████▌    | 177/321 [00:33<00:27,  5.24it/s]\u001b[A\n",
            " 55%|█████▌    | 178/321 [00:34<00:27,  5.22it/s]\u001b[A\n",
            " 56%|█████▌    | 179/321 [00:34<00:27,  5.21it/s]\u001b[A\n",
            " 56%|█████▌    | 180/321 [00:34<00:27,  5.15it/s]\u001b[A\n",
            " 56%|█████▋    | 181/321 [00:34<00:27,  5.16it/s]\u001b[A\n",
            " 57%|█████▋    | 182/321 [00:34<00:26,  5.16it/s]\u001b[A\n",
            " 57%|█████▋    | 183/321 [00:35<00:26,  5.19it/s]\u001b[A\n",
            " 57%|█████▋    | 184/321 [00:35<00:26,  5.23it/s]\u001b[A\n",
            " 58%|█████▊    | 185/321 [00:35<00:25,  5.25it/s]\u001b[A\n",
            " 58%|█████▊    | 186/321 [00:35<00:25,  5.26it/s]\u001b[A\n",
            " 58%|█████▊    | 187/321 [00:35<00:25,  5.31it/s]\u001b[A\n",
            " 59%|█████▊    | 188/321 [00:36<00:24,  5.35it/s]\u001b[A\n",
            " 59%|█████▉    | 189/321 [00:36<00:24,  5.33it/s]\u001b[A\n",
            " 59%|█████▉    | 190/321 [00:36<00:24,  5.30it/s]\u001b[A\n",
            " 60%|█████▉    | 191/321 [00:36<00:24,  5.27it/s]\u001b[A\n",
            " 60%|█████▉    | 192/321 [00:36<00:24,  5.23it/s]\u001b[A\n",
            " 60%|██████    | 193/321 [00:37<00:24,  5.19it/s]\u001b[A\n",
            " 60%|██████    | 194/321 [00:37<00:24,  5.14it/s]\u001b[A\n",
            " 61%|██████    | 195/321 [00:37<00:24,  5.13it/s]\u001b[A\n",
            " 61%|██████    | 196/321 [00:37<00:24,  5.12it/s]\u001b[A\n",
            " 61%|██████▏   | 197/321 [00:37<00:23,  5.20it/s]\u001b[A\n",
            " 62%|██████▏   | 198/321 [00:38<00:23,  5.22it/s]\u001b[A\n",
            " 62%|██████▏   | 199/321 [00:38<00:23,  5.23it/s]\u001b[A\n",
            " 62%|██████▏   | 200/321 [00:38<00:23,  5.25it/s]\u001b[A\n",
            " 63%|██████▎   | 201/321 [00:38<00:22,  5.22it/s]\u001b[A\n",
            " 63%|██████▎   | 202/321 [00:38<00:22,  5.23it/s]\u001b[A\n",
            " 63%|██████▎   | 203/321 [00:38<00:22,  5.16it/s]\u001b[A\n",
            " 64%|██████▎   | 204/321 [00:39<00:22,  5.14it/s]\u001b[A\n",
            " 64%|██████▍   | 205/321 [00:39<00:22,  5.17it/s]\u001b[A\n",
            " 64%|██████▍   | 206/321 [00:39<00:22,  5.13it/s]\u001b[A\n",
            " 64%|██████▍   | 207/321 [00:39<00:21,  5.21it/s]\u001b[A\n",
            " 65%|██████▍   | 208/321 [00:39<00:21,  5.26it/s]\u001b[A\n",
            " 65%|██████▌   | 209/321 [00:40<00:21,  5.22it/s]\u001b[A\n",
            " 65%|██████▌   | 210/321 [00:40<00:21,  5.28it/s]\u001b[A\n",
            " 66%|██████▌   | 211/321 [00:40<00:21,  5.24it/s]\u001b[A\n",
            " 66%|██████▌   | 212/321 [00:40<00:20,  5.21it/s]\u001b[A\n",
            " 66%|██████▋   | 213/321 [00:40<00:20,  5.19it/s]\u001b[A\n",
            " 67%|██████▋   | 214/321 [00:41<00:20,  5.12it/s]\u001b[A\n",
            " 67%|██████▋   | 215/321 [00:41<00:20,  5.18it/s]\u001b[A\n",
            " 67%|██████▋   | 216/321 [00:41<00:20,  5.16it/s]\u001b[A\n",
            " 68%|██████▊   | 217/321 [00:41<00:19,  5.21it/s]\u001b[A\n",
            " 68%|██████▊   | 218/321 [00:41<00:19,  5.18it/s]\u001b[A\n",
            " 68%|██████▊   | 219/321 [00:42<00:19,  5.15it/s]\u001b[A\n",
            " 69%|██████▊   | 220/321 [00:42<00:19,  5.18it/s]\u001b[A\n",
            " 69%|██████▉   | 221/321 [00:42<00:19,  5.25it/s]\u001b[A\n",
            " 69%|██████▉   | 222/321 [00:42<00:18,  5.24it/s]\u001b[A\n",
            " 69%|██████▉   | 223/321 [00:42<00:18,  5.22it/s]\u001b[A\n",
            " 70%|██████▉   | 224/321 [00:43<00:18,  5.11it/s]\u001b[A\n",
            " 70%|███████   | 225/321 [00:43<00:18,  5.11it/s]\u001b[A\n",
            " 70%|███████   | 226/321 [00:43<00:18,  5.19it/s]\u001b[A\n",
            " 71%|███████   | 227/321 [00:43<00:17,  5.26it/s]\u001b[A\n",
            " 71%|███████   | 228/321 [00:43<00:17,  5.19it/s]\u001b[A\n",
            " 71%|███████▏  | 229/321 [00:43<00:17,  5.21it/s]\u001b[A\n",
            " 72%|███████▏  | 230/321 [00:44<00:17,  5.13it/s]\u001b[A\n",
            " 72%|███████▏  | 231/321 [00:44<00:17,  5.17it/s]\u001b[A\n",
            " 72%|███████▏  | 232/321 [00:44<00:17,  5.20it/s]\u001b[A\n",
            " 73%|███████▎  | 233/321 [00:44<00:17,  5.15it/s]\u001b[A\n",
            " 73%|███████▎  | 234/321 [00:44<00:16,  5.15it/s]\u001b[A\n",
            " 73%|███████▎  | 235/321 [00:45<00:16,  5.15it/s]\u001b[A\n",
            " 74%|███████▎  | 236/321 [00:45<00:16,  5.15it/s]\u001b[A\n",
            " 74%|███████▍  | 237/321 [00:45<00:16,  5.15it/s]\u001b[A\n",
            " 74%|███████▍  | 238/321 [00:45<00:16,  5.18it/s]\u001b[A\n",
            " 74%|███████▍  | 239/321 [00:45<00:15,  5.23it/s]\u001b[A\n",
            " 75%|███████▍  | 240/321 [00:46<00:15,  5.23it/s]\u001b[A\n",
            " 75%|███████▌  | 241/321 [00:46<00:15,  5.17it/s]\u001b[A\n",
            " 75%|███████▌  | 242/321 [00:46<00:15,  5.22it/s]\u001b[A\n",
            " 76%|███████▌  | 243/321 [00:46<00:14,  5.28it/s]\u001b[A\n",
            " 76%|███████▌  | 244/321 [00:46<00:14,  5.20it/s]\u001b[A\n",
            " 76%|███████▋  | 245/321 [00:47<00:14,  5.13it/s]\u001b[A\n",
            " 77%|███████▋  | 246/321 [00:47<00:14,  5.10it/s]\u001b[A\n",
            " 77%|███████▋  | 247/321 [00:47<00:14,  5.03it/s]\u001b[A\n",
            " 77%|███████▋  | 248/321 [00:47<00:14,  5.05it/s]\u001b[A\n",
            " 78%|███████▊  | 249/321 [00:47<00:14,  4.99it/s]\u001b[A\n",
            " 78%|███████▊  | 250/321 [00:48<00:13,  5.11it/s]\u001b[A\n",
            " 78%|███████▊  | 251/321 [00:48<00:13,  5.12it/s]\u001b[A\n",
            " 79%|███████▊  | 252/321 [00:48<00:13,  5.15it/s]\u001b[A\n",
            " 79%|███████▉  | 253/321 [00:48<00:13,  5.16it/s]\u001b[A\n",
            " 79%|███████▉  | 254/321 [00:48<00:13,  5.14it/s]\u001b[A\n",
            " 79%|███████▉  | 255/321 [00:49<00:12,  5.11it/s]\u001b[A\n",
            " 80%|███████▉  | 256/321 [00:49<00:12,  5.12it/s]\u001b[A\n",
            " 80%|████████  | 257/321 [00:49<00:12,  5.13it/s]\u001b[A\n",
            " 80%|████████  | 258/321 [00:49<00:11,  5.29it/s]\u001b[A\n",
            " 81%|████████  | 259/321 [00:49<00:11,  5.21it/s]\u001b[A\n",
            " 81%|████████  | 260/321 [00:50<00:12,  5.08it/s]\u001b[A\n",
            " 81%|████████▏ | 261/321 [00:50<00:11,  5.10it/s]\u001b[A\n",
            " 82%|████████▏ | 262/321 [00:50<00:11,  5.16it/s]\u001b[A\n",
            " 82%|████████▏ | 263/321 [00:50<00:11,  5.14it/s]\u001b[A\n",
            " 82%|████████▏ | 264/321 [00:50<00:10,  5.19it/s]\u001b[A\n",
            " 83%|████████▎ | 265/321 [00:50<00:10,  5.29it/s]\u001b[A\n",
            " 83%|████████▎ | 266/321 [00:51<00:10,  5.22it/s]\u001b[A\n",
            " 83%|████████▎ | 267/321 [00:51<00:10,  5.20it/s]\u001b[A\n",
            " 83%|████████▎ | 268/321 [00:51<00:10,  5.26it/s]\u001b[A\n",
            " 84%|████████▍ | 269/321 [00:51<00:10,  5.20it/s]\u001b[A\n",
            " 84%|████████▍ | 270/321 [00:51<00:09,  5.24it/s]\u001b[A\n",
            " 84%|████████▍ | 271/321 [00:52<00:09,  5.25it/s]\u001b[A\n",
            " 85%|████████▍ | 272/321 [00:52<00:09,  5.15it/s]\u001b[A\n",
            " 85%|████████▌ | 273/321 [00:52<00:09,  5.11it/s]\u001b[A\n",
            " 85%|████████▌ | 274/321 [00:52<00:09,  5.18it/s]\u001b[A\n",
            " 86%|████████▌ | 275/321 [00:52<00:08,  5.17it/s]\u001b[A\n",
            " 86%|████████▌ | 276/321 [00:53<00:08,  5.24it/s]\u001b[A\n",
            " 86%|████████▋ | 277/321 [00:53<00:08,  5.32it/s]\u001b[A\n",
            " 87%|████████▋ | 278/321 [00:53<00:08,  5.32it/s]\u001b[A\n",
            " 87%|████████▋ | 279/321 [00:53<00:08,  5.21it/s]\u001b[A\n",
            " 87%|████████▋ | 280/321 [00:53<00:07,  5.15it/s]\u001b[A\n",
            " 88%|████████▊ | 281/321 [00:54<00:07,  5.15it/s]\u001b[A\n",
            " 88%|████████▊ | 282/321 [00:54<00:07,  5.21it/s]\u001b[A\n",
            " 88%|████████▊ | 283/321 [00:54<00:07,  5.28it/s]\u001b[A\n",
            " 88%|████████▊ | 284/321 [00:54<00:07,  5.21it/s]\u001b[A\n",
            " 89%|████████▉ | 285/321 [00:54<00:06,  5.16it/s]\u001b[A\n",
            " 89%|████████▉ | 286/321 [00:55<00:06,  5.09it/s]\u001b[A\n",
            " 89%|████████▉ | 287/321 [00:55<00:06,  5.16it/s]\u001b[A\n",
            " 90%|████████▉ | 288/321 [00:55<00:06,  5.30it/s]\u001b[A\n",
            " 90%|█████████ | 289/321 [00:55<00:05,  5.37it/s]\u001b[A\n",
            " 90%|█████████ | 290/321 [00:55<00:05,  5.33it/s]\u001b[A\n",
            " 91%|█████████ | 291/321 [00:55<00:05,  5.33it/s]\u001b[A\n",
            " 91%|█████████ | 292/321 [00:56<00:05,  5.24it/s]\u001b[A\n",
            " 91%|█████████▏| 293/321 [00:56<00:05,  5.16it/s]\u001b[A\n",
            " 92%|█████████▏| 294/321 [00:56<00:05,  5.24it/s]\u001b[A\n",
            " 92%|█████████▏| 295/321 [00:56<00:04,  5.21it/s]\u001b[A\n",
            " 92%|█████████▏| 296/321 [00:56<00:04,  5.21it/s]\u001b[A\n",
            " 93%|█████████▎| 297/321 [00:57<00:04,  5.08it/s]\u001b[A\n",
            " 93%|█████████▎| 298/321 [00:57<00:04,  5.21it/s]\u001b[A\n",
            " 93%|█████████▎| 299/321 [00:57<00:04,  5.20it/s]\u001b[A\n",
            " 93%|█████████▎| 300/321 [00:57<00:03,  5.35it/s]\u001b[A\n",
            " 94%|█████████▍| 301/321 [00:57<00:03,  5.23it/s]\u001b[A\n",
            " 94%|█████████▍| 302/321 [00:58<00:03,  5.21it/s]\u001b[A\n",
            " 94%|█████████▍| 303/321 [00:58<00:03,  5.34it/s]\u001b[A\n",
            " 95%|█████████▍| 304/321 [00:58<00:03,  5.28it/s]\u001b[A\n",
            " 95%|█████████▌| 305/321 [00:58<00:03,  5.29it/s]\u001b[A\n",
            " 95%|█████████▌| 306/321 [00:58<00:02,  5.25it/s]\u001b[A\n",
            " 96%|█████████▌| 307/321 [00:59<00:02,  5.14it/s]\u001b[A\n",
            " 96%|█████████▌| 308/321 [00:59<00:02,  5.25it/s]\u001b[A\n",
            " 96%|█████████▋| 309/321 [00:59<00:02,  5.29it/s]\u001b[A\n",
            " 97%|█████████▋| 310/321 [00:59<00:02,  5.29it/s]\u001b[A\n",
            " 97%|█████████▋| 311/321 [00:59<00:01,  5.33it/s]\u001b[A\n",
            " 97%|█████████▋| 312/321 [00:59<00:01,  5.24it/s]\u001b[A\n",
            " 98%|█████████▊| 313/321 [01:00<00:01,  5.20it/s]\u001b[A\n",
            " 98%|█████████▊| 314/321 [01:00<00:01,  5.18it/s]\u001b[A\n",
            " 98%|█████████▊| 315/321 [01:00<00:01,  5.13it/s]\u001b[A\n",
            " 98%|█████████▊| 316/321 [01:00<00:00,  5.14it/s]\u001b[A\n",
            " 99%|█████████▉| 317/321 [01:00<00:00,  5.19it/s]\u001b[A\n",
            " 99%|█████████▉| 318/321 [01:01<00:00,  5.23it/s]\u001b[A\n",
            " 99%|█████████▉| 319/321 [01:01<00:00,  5.29it/s]\u001b[A\n",
            "100%|█████████▉| 320/321 [01:01<00:00,  5.30it/s]\u001b[A\n",
            "100%|██████████| 321/321 [01:01<00:00,  5.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stsb encoding completed in 61.69 seconds\n",
            "Generated 41070 embeddings with dimension 768\n",
            "Processing speed: 665.73 docs/sec\n",
            "Normalizing stsb vectors...\n",
            "Building stsb Flat index...\n",
            "stsb Flat index built with 41070 vectors\n",
            "Building stsb HNSW index...\n",
            "stsb HNSW index built with 41070 vectors\n",
            "Building stsb IVF-PQ index...\n",
            "stsb IVF-PQ index built with 41070 vectors\n",
            "\n",
            "All model indexes successfully built\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Save indexes and corpus information\n",
        "print(\"Saving all model indexes...\")\n",
        "for model_name in models:\n",
        "    if model_name not in all_indexes:\n",
        "        print(f\"Skipping {model_name} - no indexes built\")\n",
        "        continue\n",
        "\n",
        "    model_dir = os.path.join(PROJECT_PATH, \"models/indexes\", model_name)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    model_indexes = all_indexes[model_name]\n",
        "\n",
        "    # Save all index types\n",
        "    print(f\"\\nSaving {model_name} indexes...\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Saving {model_name} Flat index...\")\n",
        "        faiss.write_index(model_indexes[\"flat\"], os.path.join(model_dir, \"flat_index.faiss\"))\n",
        "\n",
        "        print(f\"Saving {model_name} HNSW index...\")\n",
        "        faiss.write_index(model_indexes[\"hnsw\"], os.path.join(model_dir, \"hnsw_index.faiss\"))\n",
        "\n",
        "        print(f\"Saving {model_name} IVF-PQ index...\")\n",
        "        faiss.write_index(model_indexes[\"ivfpq\"], os.path.join(model_dir, \"ivfpq_index.faiss\"))\n",
        "\n",
        "        # Save index configuration information\n",
        "        with open(os.path.join(model_dir, \"index_config.json\"), 'w') as f:\n",
        "            json.dump(all_configs[model_name], f)\n",
        "\n",
        "        print(f\"{model_name} indexes successfully saved to: {model_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving indexes for {model_name}: {str(e)}\")\n",
        "\n",
        "# Save the corpus and document IDs\n",
        "corpus_dir = os.path.join(PROJECT_PATH, \"models/indexes\")\n",
        "print(\"\\nSaving corpus data...\")\n",
        "\n",
        "try:\n",
        "    # Save corpus\n",
        "    with open(os.path.join(corpus_dir, \"corpus.json\"), 'w') as f:\n",
        "        json.dump(corpus, f)\n",
        "\n",
        "    # Save doc IDs\n",
        "    with open(os.path.join(corpus_dir, \"doc_ids.json\"), 'w') as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    # Save BM25 model using pickle\n",
        "    print(\"Saving BM25 model...\")\n",
        "    with open(os.path.join(corpus_dir, \"bm25_model.pkl\"), 'wb') as f:\n",
        "        pickle.dump(bm25, f)\n",
        "\n",
        "    # Also save tokenized corpus for future use\n",
        "    with open(os.path.join(corpus_dir, \"tokenized_corpus.pkl\"), 'wb') as f:\n",
        "        pickle.dump(tokenized_corpus, f)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving corpus data: {str(e)}\")\n",
        "\n",
        "print(\"\\nAll indexes and data successfully saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k66b5r2HUgd_",
        "outputId": "3c4100d5-c0c0-47a3-8d22-b091a06f3603"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all model indexes...\n",
            "\n",
            "Saving msmarco_stsb indexes...\n",
            "Saving msmarco_stsb Flat index...\n",
            "Saving msmarco_stsb HNSW index...\n",
            "Saving msmarco_stsb IVF-PQ index...\n",
            "msmarco_stsb indexes successfully saved to: /content/drive/MyDrive/CS6120_project/models/indexes/msmarco_stsb\n",
            "\n",
            "Saving stsb indexes...\n",
            "Saving stsb Flat index...\n",
            "Saving stsb HNSW index...\n",
            "Saving stsb IVF-PQ index...\n",
            "stsb indexes successfully saved to: /content/drive/MyDrive/CS6120_project/models/indexes/stsb\n",
            "\n",
            "Saving corpus data...\n",
            "Saving BM25 model...\n",
            "\n",
            "All indexes and data successfully saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Enhanced Retrieval Functions\n",
        "\n",
        "def bm25_retrieve(query: str, bm25_index, doc_ids: List[str], k: int = 10) -> Tuple[List[str], List[float]]:\n",
        "    \"\"\"Retrieve documents using BM25 with improved error handling\n",
        "\n",
        "    Args:\n",
        "        query: Query string\n",
        "        bm25_index: BM25 index\n",
        "        doc_ids: List of document IDs\n",
        "        k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        top_doc_ids: List of document IDs\n",
        "        top_scores: List of corresponding BM25 scores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Process query\n",
        "        query_tokens = preprocess_text(query)\n",
        "        if not query_tokens:\n",
        "            print(f\"Warning: Empty query tokens for query: '{query}'\")\n",
        "            query_tokens = [\"_empty_\"]\n",
        "\n",
        "        # Get BM25 scores\n",
        "        bm25_scores = bm25_index.get_scores(query_tokens)\n",
        "\n",
        "        # Ensure length compatibility\n",
        "        if len(bm25_scores) != len(doc_ids):\n",
        "            print(f\"Warning: BM25 scores count ({len(bm25_scores)}) does not match doc_ids count ({len(doc_ids)})\")\n",
        "            if len(bm25_scores) < len(doc_ids):\n",
        "                # Extend with zeros\n",
        "                bm25_scores = np.concatenate([bm25_scores, np.zeros(len(doc_ids) - len(bm25_scores))])\n",
        "            else:\n",
        "                # Truncate\n",
        "                bm25_scores = bm25_scores[:len(doc_ids)]\n",
        "\n",
        "        # Get top k results\n",
        "        if len(bm25_scores) == 0:\n",
        "            print(\"Warning: No BM25 scores available\")\n",
        "            return [], []\n",
        "\n",
        "        # Sort by score in descending order\n",
        "        top_indices = np.argsort(bm25_scores)[::-1][:k]\n",
        "        top_scores = bm25_scores[top_indices]\n",
        "\n",
        "        # Map indices to document IDs\n",
        "        top_doc_ids = [doc_ids[idx] for idx in top_indices]\n",
        "\n",
        "        return top_doc_ids, top_scores\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in BM25 retrieval: {str(e)}\")\n",
        "        # Return empty results in case of error\n",
        "        return [], []\n",
        "\n",
        "\n",
        "# Query classifier for dynamic weighting\n",
        "def train_query_classifier():\n",
        "    \"\"\"Train a simple query classifier for dynamic strategy selection\"\"\"\n",
        "    # Sample queries for training\n",
        "    test_queries = [\n",
        "        \"How does social media affect mental health?\",\n",
        "        \"Best programming languages to learn\",\n",
        "        \"Artificial intelligence applications\",\n",
        "        \"Climate change solutions and mitigation strategies\",\n",
        "        \"Nutrition advice for athletes performance\"\n",
        "    ]\n",
        "\n",
        "    # Feature extraction function\n",
        "    def extract_query_features(query):\n",
        "        features = []\n",
        "        # Length features\n",
        "        features.append(len(query))  # Raw character length\n",
        "        features.append(len(query.split()))  # Word count\n",
        "\n",
        "        # Question features\n",
        "        features.append(1 if \"?\" in query else 0)  # Is it a question\n",
        "        features.append(1 if query.lower().startswith(\"how\") else 0)  # How question\n",
        "        features.append(1 if query.lower().startswith(\"what\") else 0)  # What question\n",
        "        features.append(1 if query.lower().startswith(\"why\") else 0)  # Why question\n",
        "\n",
        "        # Structure features\n",
        "        features.append(1 if \",\" in query else 0)  # Has comma\n",
        "        features.append(1 if \":\" in query else 0)  # Has colon\n",
        "\n",
        "        # Topic features (simple keyword checking)\n",
        "        features.append(1 if any(word in query.lower() for word in [\"programming\", \"code\", \"software\"]) else 0)  # Tech\n",
        "        features.append(1 if any(word in query.lower() for word in [\"health\", \"medical\", \"disease\"]) else 0)  # Health\n",
        "\n",
        "        return features\n",
        "\n",
        "    # Build training features\n",
        "    X_train = np.array([extract_query_features(q) for q in test_queries])\n",
        "    # Dummy labels (which model is better)\n",
        "    # 0 for primary model (semantic-heavy queries)\n",
        "    # 1 for fallback model (simpler queries that might work better with BM25)\n",
        "    y_train = np.array([0, 1, 0, 0, 1])  # Simulated labels\n",
        "\n",
        "    # Train a simple query classifier\n",
        "    classifier = LogisticRegression(random_state=42)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    print(\"Query classifier trained\")\n",
        "\n",
        "    # Add feature extractor to classifier object for convenience\n",
        "    classifier.extract_features = extract_query_features\n",
        "\n",
        "    return classifier\n",
        "\n",
        "# Train the classifier\n",
        "query_classifier = train_query_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPCq6Y4kVHe_",
        "outputId": "f3f42cde-fc46-468f-bf67-bbeaf5857158"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classifier trained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Main Enhanced Hybrid Retrieval Function\n",
        "def hybrid_retrieve_documents(\n",
        "    query: str,\n",
        "    query_id: str,\n",
        "    top_k: int = 100,\n",
        "    strategy: str = \"sbert_bm25\",\n",
        "    alpha: float = 0.7,\n",
        "    debug: bool = False\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"Enhanced hybrid retrieval function with improved error handling\n",
        "\n",
        "    Args:\n",
        "        query: Query string\n",
        "        query_id: Query ID\n",
        "        top_k: Number of results to return\n",
        "        strategy: Retrieval strategy\n",
        "        alpha: Weight for semantic score (1-alpha for BM25)\n",
        "        debug: Whether to print debug information\n",
        "\n",
        "    Returns:\n",
        "        Dictionary {query_id: {doc_id: score}} for MRR evaluation\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if not query:\n",
        "        print(\"Error: Empty query\")\n",
        "        return {query_id: {}}  # Return empty results\n",
        "\n",
        "    if strategy not in [\"single\", \"dynamic\", \"fallback\", \"ensemble\", \"sbert_bm25\"]:\n",
        "        print(f\"Error: Unknown strategy '{strategy}', falling back to 'single' strategy\")\n",
        "        strategy = \"single\"\n",
        "\n",
        "    # Initialize result format for MRR evaluation\n",
        "    result = {query_id: {}}\n",
        "\n",
        "    # Start retrieval process with full error handling\n",
        "    try:\n",
        "        # Initialize model list based on strategy\n",
        "        if strategy in [\"single\", \"fallback\"]:\n",
        "            model_list = [primary_model]\n",
        "        else:\n",
        "            model_list = [primary_model, fallback_model]\n",
        "\n",
        "        # Encode query for each model\n",
        "        query_embeddings = {}\n",
        "        for model_name in model_list:\n",
        "            if model_name not in models:\n",
        "                print(f\"Error: Model {model_name} not loaded\")\n",
        "                continue\n",
        "\n",
        "            query_emb = models[model_name].encode([query])\n",
        "            faiss.normalize_L2(query_emb)\n",
        "            query_embeddings[model_name] = query_emb\n",
        "\n",
        "        # If no query embeddings could be generated, return empty results\n",
        "        if not query_embeddings:\n",
        "            print(\"Error: No query embeddings could be generated\")\n",
        "            return {query_id: {}}\n",
        "\n",
        "        # Process based on strategy\n",
        "        if strategy == \"single\":\n",
        "            # Just use primary model\n",
        "            if primary_model not in all_indexes:\n",
        "                print(f\"Error: No indexes for {primary_model}\")\n",
        "                return {query_id: {}}\n",
        "\n",
        "            index = all_indexes[primary_model][\"hnsw\"]\n",
        "            D, I = index.search(query_embeddings[primary_model], top_k)\n",
        "\n",
        "            # Build results in required format\n",
        "            for i in range(min(top_k, len(I[0]))):\n",
        "                if I[0][i] >= 0 and I[0][i] < len(doc_ids):  # Ensure valid index\n",
        "                    doc_id = doc_ids[I[0][i]]\n",
        "                    score = float(D[0][i])\n",
        "                    result[query_id][doc_id] = score\n",
        "\n",
        "        elif strategy == \"dynamic\":\n",
        "            # Use query classifier to determine best model\n",
        "            features = query_classifier.extract_features(query)\n",
        "            features = np.array([features])\n",
        "\n",
        "            # Predict which model to use\n",
        "            model_idx = query_classifier.predict(features)[0]\n",
        "            model_to_use = primary_model if model_idx == 0 else fallback_model\n",
        "\n",
        "            if debug:\n",
        "                print(f\"Dynamic strategy selected model: {model_to_use}\")\n",
        "\n",
        "            if model_to_use not in all_indexes or model_to_use not in query_embeddings:\n",
        "                # Fall back to primary model if selected model is not available\n",
        "                print(f\"Warning: Selected model {model_to_use} not available, using {primary_model} instead\")\n",
        "                model_to_use = primary_model\n",
        "\n",
        "            # Use the selected model\n",
        "            index = all_indexes[model_to_use][\"hnsw\"]\n",
        "            D, I = index.search(query_embeddings[model_to_use], top_k)\n",
        "\n",
        "            # Build results in required format\n",
        "            for i in range(min(top_k, len(I[0]))):\n",
        "                if I[0][i] >= 0 and I[0][i] < len(doc_ids):\n",
        "                    doc_id = doc_ids[I[0][i]]\n",
        "                    score = float(D[0][i])\n",
        "                    result[query_id][doc_id] = score\n",
        "\n",
        "        elif strategy == \"fallback\":\n",
        "            # First try with primary model\n",
        "            primary_index = all_indexes[primary_model][\"hnsw\"]\n",
        "            D_primary, I_primary = primary_index.search(query_embeddings[primary_model], top_k)\n",
        "\n",
        "            # Check confidence (average similarity score)\n",
        "            confidence = np.mean(D_primary[0]) if len(D_primary[0]) > 0 else 0\n",
        "            threshold = 0.3  # Confidence threshold\n",
        "\n",
        "            if debug:\n",
        "                print(f\"Primary model confidence: {confidence:.4f} (threshold: {threshold})\")\n",
        "\n",
        "            # Decide which results to use\n",
        "            if confidence > threshold:\n",
        "                # Use primary model results\n",
        "                D, I = D_primary, I_primary\n",
        "                if debug:\n",
        "                    print(\"Using primary model results\")\n",
        "            else:\n",
        "                # Switch to fallback model\n",
        "                if debug:\n",
        "                    print(f\"Switching to fallback model ({fallback_model})\")\n",
        "\n",
        "                fallback_index = all_indexes[fallback_model][\"hnsw\"]\n",
        "                D, I = fallback_index.search(query_embeddings[fallback_model], top_k)\n",
        "\n",
        "            # Build results in required format\n",
        "            for i in range(min(top_k, len(I[0]))):\n",
        "                if I[0][i] >= 0 and I[0][i] < len(doc_ids):\n",
        "                    doc_id = doc_ids[I[0][i]]\n",
        "                    score = float(D[0][i])\n",
        "                    result[query_id][doc_id] = score\n",
        "\n",
        "        elif strategy == \"ensemble\":\n",
        "            # Get results from each model\n",
        "            all_results = {}\n",
        "\n",
        "            for model_name in model_list:\n",
        "                if model_name not in all_indexes or model_name not in query_embeddings:\n",
        "                    continue\n",
        "\n",
        "                model_index = all_indexes[model_name][\"hnsw\"]\n",
        "                D, I = model_index.search(query_embeddings[model_name], top_k * 2)  # Get more candidates\n",
        "\n",
        "                # Save score for each document ID\n",
        "                for j in range(len(I[0])):\n",
        "                    idx = int(I[0][j])\n",
        "                    if idx < 0 or idx >= len(doc_ids):  # Skip invalid indices\n",
        "                        continue\n",
        "\n",
        "                    doc_id = doc_ids[idx]\n",
        "                    score = float(D[0][j])\n",
        "\n",
        "                    if doc_id not in all_results:\n",
        "                        all_results[doc_id] = {}\n",
        "\n",
        "                    all_results[doc_id][model_name] = score\n",
        "\n",
        "            # Compute combined scores using weights\n",
        "            weights = {primary_model: alpha, fallback_model: 1.0-alpha}\n",
        "            final_scores = {}\n",
        "\n",
        "            for doc_id in all_results:\n",
        "                final_scores[doc_id] = 0\n",
        "                for model_name, weight in weights.items():\n",
        "                    if model_name in all_results[doc_id]:\n",
        "                        final_scores[doc_id] += all_results[doc_id][model_name] * weight\n",
        "\n",
        "            # Sort and select top_k results\n",
        "            sorted_results = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "            # Build results in required format\n",
        "            for doc_id, score in sorted_results:\n",
        "                result[query_id][doc_id] = float(score)\n",
        "\n",
        "        elif strategy == \"sbert_bm25\":\n",
        "            # SBERT + BM25 hybrid approach - most thorough option\n",
        "            if debug:\n",
        "                print(f\"Running sbert_bm25 strategy with alpha={alpha:.2f}\")\n",
        "\n",
        "            # 1. Get SBERT results\n",
        "            sbert_index = all_indexes[primary_model][\"hnsw\"]\n",
        "            D_sbert, I_sbert = sbert_index.search(query_embeddings[primary_model], top_k*2)\n",
        "\n",
        "            # Get SBERT results in dictionary format\n",
        "            sbert_results = {}\n",
        "            for j in range(len(I_sbert[0])):\n",
        "                idx = int(I_sbert[0][j])\n",
        "                if idx < 0 or idx >= len(doc_ids):\n",
        "                    continue\n",
        "\n",
        "                doc_id = doc_ids[idx]\n",
        "                score = float(D_sbert[0][j])\n",
        "                sbert_results[doc_id] = score\n",
        "\n",
        "            # 2. Get BM25 results\n",
        "            bm25_doc_ids, bm25_scores = bm25_retrieve(query, bm25, doc_ids, k=top_k*2)\n",
        "\n",
        "            # Get BM25 results in dictionary format\n",
        "            bm25_results = {}\n",
        "            for doc_id, score in zip(bm25_doc_ids, bm25_scores):\n",
        "                bm25_results[doc_id] = float(score)\n",
        "\n",
        "            if debug:\n",
        "                print(f\"SBERT found {len(sbert_results)} results\")\n",
        "                print(f\"BM25 found {len(bm25_results)} results\")\n",
        "\n",
        "            # 3. Check if we have results\n",
        "            if not sbert_results and not bm25_results:\n",
        "                print(\"Warning: No results found from either SBERT or BM25\")\n",
        "                return {query_id: {}}\n",
        "\n",
        "            # 4. Normalize scores\n",
        "            if sbert_results and len(sbert_results) > 0:\n",
        "                sbert_docs = list(sbert_results.keys())\n",
        "                sbert_scores = np.array(list(sbert_results.values()))\n",
        "                sbert_scores_norm = normalize_scores(sbert_scores)\n",
        "                sbert_results = dict(zip(sbert_docs, sbert_scores_norm))\n",
        "\n",
        "            if bm25_results and len(bm25_results) > 0:\n",
        "                bm25_docs = list(bm25_results.keys())\n",
        "                bm25_scores = np.array(list(bm25_results.values()))\n",
        "                bm25_scores_norm = normalize_scores(bm25_scores)\n",
        "                bm25_results = dict(zip(bm25_docs, bm25_scores_norm))\n",
        "\n",
        "            # 5. Combine unique candidates\n",
        "            all_candidates = set(sbert_results.keys()) | set(bm25_results.keys())\n",
        "\n",
        "            if debug:\n",
        "                print(f\"Total unique candidates: {len(all_candidates)}\")\n",
        "\n",
        "            # 6. Calculate combined scores\n",
        "            combined_scores = {}\n",
        "            for doc_id in all_candidates:\n",
        "                sbert_score = sbert_results.get(doc_id, 0.0)\n",
        "                bm25_score = bm25_results.get(doc_id, 0.0)\n",
        "                # Apply scaling factor and combine\n",
        "                combined_scores[doc_id] = alpha * sbert_score + (1-alpha) * bm25_score\n",
        "\n",
        "            # 7. Sort and take top k\n",
        "            ranked_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "            # 8. Build results in required format\n",
        "            for doc_id, score in ranked_results:\n",
        "                result[query_id][doc_id] = float(score)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in hybrid retrieval: {str(e)}\")\n",
        "        # Return empty results in case of error\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "81Q5_fQdVJv2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Enhanced Evaluation Functions - 修改版\n",
        "\n",
        "def compute_mrr_at_k(run: Dict[str, Dict[str, float]],\n",
        "                     qrels: Dict[str, Dict[str, int]],\n",
        "                     k: int = 100,\n",
        "                     verbose: bool = False) -> float:\n",
        "    \"\"\"Compute MRR@k with improved error handling\n",
        "\n",
        "    Args:\n",
        "        run: Search results {query_id: {doc_id: score}}\n",
        "        qrels: Ground truth {query_id: {doc_id: relevance}}\n",
        "        k: Cutoff value\n",
        "        verbose: Whether to print detailed information\n",
        "\n",
        "    Returns:\n",
        "        MRR value\n",
        "    \"\"\"\n",
        "    total_rr = 0.0\n",
        "    num_queries = 0\n",
        "    queries_with_relevant_docs = 0\n",
        "    queries_with_relevant_retrieved = 0\n",
        "\n",
        "    for qid, relevant_docs in qrels.items():\n",
        "        # Skip if no relevant docs\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "\n",
        "        # Count this as a query with relevant docs\n",
        "        queries_with_relevant_docs += 1\n",
        "\n",
        "        # Skip if query not in run\n",
        "        if qid not in run:\n",
        "            if verbose:\n",
        "                print(f\"Query {qid} not in run\")\n",
        "            continue\n",
        "\n",
        "        # Skip if run has no results for this query\n",
        "        if not run[qid]:\n",
        "            if verbose:\n",
        "                print(f\"No results for query {qid}\")\n",
        "            continue\n",
        "\n",
        "        # Sort results by score in descending order, take top k\n",
        "        try:\n",
        "            sorted_docs = sorted(run[qid].items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "        except Exception as e:\n",
        "            print(f\"Error sorting results for query {qid}: {str(e)}\")\n",
        "            print(f\"Run[qid] type: {type(run[qid])}, value: {run[qid]}\")\n",
        "            continue\n",
        "\n",
        "        rr = 0.0  # Reciprocal rank for current query\n",
        "        for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
        "            # If document is relevant\n",
        "            if doc_id in relevant_docs and relevant_docs[doc_id] > 0:\n",
        "                rr = 1.0 / rank\n",
        "                queries_with_relevant_retrieved += 1\n",
        "                break  # Only consider first relevant document\n",
        "\n",
        "        total_rr += rr\n",
        "        num_queries += 1\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Queries processed: {num_queries}\")\n",
        "        print(f\"Queries with relevant docs: {queries_with_relevant_docs}\")\n",
        "        print(f\"Queries with relevant docs retrieved: {queries_with_relevant_retrieved}\")\n",
        "\n",
        "    if num_queries == 0:\n",
        "        print(\"Warning: No queries with results were evaluated\")\n",
        "        return 0.0\n",
        "\n",
        "    return total_rr / num_queries\n",
        "\n",
        "\n",
        "def compute_recall_at_k(run: Dict[str, Dict[str, float]],\n",
        "                       qrels: Dict[str, Dict[str, int]],\n",
        "                       k: int = 100,\n",
        "                       verbose: bool = False) -> float:\n",
        "    \"\"\"Compute Recall@K with improved error handling\n",
        "\n",
        "    Args:\n",
        "        run: Search results {query_id: {doc_id: score}}\n",
        "        qrels: Ground truth {query_id: {doc_id: relevance}}\n",
        "        k: Cutoff value\n",
        "        verbose: Whether to print detailed information\n",
        "\n",
        "    Returns:\n",
        "        Recall value\n",
        "    \"\"\"\n",
        "    total_recall = 0.0\n",
        "    num_queries_with_rels = 0  # Only count queries with relevant documents\n",
        "    recall_values = []\n",
        "\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        # Get relevant document set\n",
        "        relevant_docs = {doc_id for doc_id, rel in rel_docs.items() if rel > 0}\n",
        "        if not relevant_docs:\n",
        "            # Skip queries without relevant documents\n",
        "            continue\n",
        "\n",
        "        # Count this query\n",
        "        num_queries_with_rels += 1\n",
        "\n",
        "        # Skip if query not in run\n",
        "        if qid not in run:\n",
        "            recall_values.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Skip if run has no results for this query\n",
        "        if not run[qid]:\n",
        "            recall_values.append(0.0)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Get top k documents by score\n",
        "            top_docs = sorted(run[qid].items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "            top_docs_ids = {doc_id for doc_id, score in top_docs}\n",
        "\n",
        "            # Compute recall: hits / total relevant\n",
        "            hit_count = len(relevant_docs & top_docs_ids)\n",
        "            recall_q = hit_count / len(relevant_docs)\n",
        "\n",
        "            recall_values.append(recall_q)\n",
        "            total_recall += recall_q\n",
        "\n",
        "            if verbose and recall_q == 1.0:\n",
        "                print(f\"Query {qid} has perfect recall\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing recall for query {qid}: {str(e)}\")\n",
        "            recall_values.append(0.0)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Queries with relevant docs: {num_queries_with_rels}\")\n",
        "        if recall_values:\n",
        "            print(f\"Min recall: {min(recall_values):.4f}\")\n",
        "            print(f\"Max recall: {max(recall_values):.4f}\")\n",
        "            print(f\"Median recall: {np.median(recall_values):.4f}\")\n",
        "\n",
        "    if num_queries_with_rels == 0:\n",
        "        print(\"Warning: No queries with relevant documents were evaluated\")\n",
        "        return 0.0\n",
        "\n",
        "    return total_recall / num_queries_with_rels\n",
        "\n",
        "\n",
        "# Check if pytrec_eval is available and define NDCG function\n",
        "try:\n",
        "    import pytrec_eval\n",
        "\n",
        "    def compute_ndcg_at_k(run, qrels, k=100, verbose=False):\n",
        "        \"\"\"Compute NDCG@k using pytrec_eval\"\"\"\n",
        "        try:\n",
        "            # Create evaluator\n",
        "            evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'ndcg'})\n",
        "\n",
        "            # Evaluate\n",
        "            results = evaluator.evaluate(run)\n",
        "\n",
        "            # Extract NDCG scores\n",
        "            ndcg_values = [results[qid].get('ndcg', 0.0) for qid in results]\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Queries evaluated for NDCG: {len(ndcg_values)}\")\n",
        "                if ndcg_values:\n",
        "                    print(f\"Min NDCG: {min(ndcg_values):.4f}\")\n",
        "                    print(f\"Max NDCG: {max(ndcg_values):.4f}\")\n",
        "                    print(f\"Median NDCG: {np.median(ndcg_values):.4f}\")\n",
        "\n",
        "            # Calculate mean NDCG\n",
        "            if not ndcg_values:\n",
        "                return 0.0\n",
        "\n",
        "            return sum(ndcg_values) / len(ndcg_values)\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing NDCG: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    has_pytrec_eval = True\n",
        "    print(\"pytrec_eval loaded successfully for NDCG calculation\")\n",
        "\n",
        "except ImportError:\n",
        "    has_pytrec_eval = False\n",
        "    print(\"pytrec_eval not available, skipping NDCG calculation\")"
      ],
      "metadata": {
        "id": "XV0CB0O8azg7",
        "outputId": "5ac712f7-31a4-4d3a-d24d-74ad08ee08d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytrec_eval loaded successfully for NDCG calculation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试检索功能：运行这个单元格来获取输出结果\n",
        "# 测试单个查询示例\n",
        "print(\"Testing a sample query...\")\n",
        "sample_result = test_sample_query(strategy=\"sbert_bm25\", alpha=0.0, top_k=5)\n",
        "\n",
        "# 测试BM25检索功能\n",
        "print(\"\\n--- Testing BM25 Retrieval ---\")\n",
        "sample_qid = random.choice(list(queries.keys()))\n",
        "sample_query = queries[sample_qid]\n",
        "print(f\"Query: {sample_query}\")\n",
        "\n",
        "bm25_doc_ids, bm25_scores = bm25_retrieve(sample_query, bm25, doc_ids, k=5)\n",
        "print(\"\\nBM25 retrieval results:\")\n",
        "for i, (doc_id, score) in enumerate(zip(bm25_doc_ids, bm25_scores)):\n",
        "    print(f\"Result {i+1}: {doc_id} (Score: {score:.4f})\")\n",
        "    print(f\"Text: {corpus[doc_id][:100]}...\")\n",
        "\n",
        "# 验证MRR计算\n",
        "print(\"\\n--- Verifying MRR Calculation ---\")\n",
        "# 创建一个小型测试集\n",
        "test_qid = sample_qid\n",
        "test_query = sample_query\n",
        "test_run = {}\n",
        "test_run[test_qid] = {}\n",
        "\n",
        "# 添加一些检索结果\n",
        "test_results = hybrid_retrieve_documents(\n",
        "    test_query, test_qid, top_k=10, strategy=\"sbert_bm25\", alpha=0.0\n",
        ")\n",
        "\n",
        "print(f\"Run format for MRR calculation: {type(test_results)}\")\n",
        "print(f\"Example run entry: {list(test_results.items())[0]}\")\n",
        "\n",
        "# 计算MRR\n",
        "if test_qid in qrels:\n",
        "    test_mrr = compute_mrr_at_k({test_qid: test_results[test_qid]}, {test_qid: qrels[test_qid]}, k=10, verbose=True)\n",
        "    print(f\"Test MRR@10: {test_mrr:.4f}\")\n",
        "else:\n",
        "    print(f\"Query {test_qid} has no relevance judgments\")\n",
        "\n",
        "    # 找一个有相关文档的查询\n",
        "    for alt_qid in qrels:\n",
        "        if qrels[alt_qid]:\n",
        "            print(f\"Using alternative query {alt_qid} with {len(qrels[alt_qid])} relevant docs\")\n",
        "            alt_query = queries[alt_qid]\n",
        "            alt_results = hybrid_retrieve_documents(\n",
        "                alt_query, alt_qid, top_k=10, strategy=\"sbert_bm25\", alpha=0.0\n",
        "            )\n",
        "            alt_mrr = compute_mrr_at_k(alt_results, {alt_qid: qrels[alt_qid]}, k=10, verbose=True)\n",
        "            print(f\"Alternative test MRR@10: {alt_mrr:.4f}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "TJT0MLbvaz99",
        "outputId": "d43f3cab-7084-48b1-aa44-a3a5e1066d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing a sample query...\n",
            "Sample query ID: 17999\n",
            "Sample query: does starving yourself cause your belly to bloat\n",
            "Running sbert_bm25 strategy with alpha=0.00\n",
            "SBERT found 10 results\n",
            "BM25 found 10 results\n",
            "Total unique candidates: 17\n",
            "\n",
            "Retrieved documents using sbert_bm25 strategy (alpha=0.0):\n",
            "\n",
            "Rank 1 - Doc ID: 17999_6 - Score: 1.0000 - ✗ NOT RELEVANT\n",
            "Preview: No, you’re not imagining things—you have the dreaded belly bloat! And even if you’re going for daily runs and eating right, many factors in your day can contribute to sudden and unexpected belly bloat...\n",
            "\n",
            "Rank 2 - Doc ID: 17999_8 - Score: 0.9144 - ✗ NOT RELEVANT\n",
            "Preview: Bloat from carbonated water. For one to three hours after drinking carbonated water, you may feel as though your belly has expanded. The carbonation can make your stomach look distended and cause clot...\n",
            "\n",
            "Rank 3 - Doc ID: 17999_1 - Score: 0.8984 - ✗ NOT RELEVANT\n",
            "Preview: The starvation belly stands out in painful relief against emaciated arms, legs, and face, and will appear more taut and inflexible. That’s because the belly on a starving person has nothing to do with...\n",
            "\n",
            "Rank 4 - Doc ID: 17999_0 - Score: 0.7152 - ✗ NOT RELEVANT\n",
            "Preview: In the later stages of malnourishment, the belly swells due to ascites. A starving person's blood protein levels are so low from the malnutrition the osmotic pressure pushes the fluid out of the vascu...\n",
            "\n",
            "Rank 5 - Doc ID: 14602_7 - Score: 0.2769 - ✗ NOT RELEVANT\n",
            "Preview: I have had people write me, concerned that their goat/kids were bloating when in fact they were fine. But, real bloat, if untreated, can cause death, so it is something to be aware of. We have only ha...\n",
            "\n",
            "--- Testing BM25 Retrieval ---\n",
            "Query: an inflammation of the lungs commonly caused by a bacterial or viral infection\n",
            "\n",
            "BM25 retrieval results:\n",
            "Result 1: 10907_3 (Score: 38.3285)\n",
            "Text: Pneumonia (lung inflammation, usually caused by a viral or bacterial infection). Sinusitis (inflamma...\n",
            "Result 2: 10907_1 (Score: 32.8306)\n",
            "Text: Phlegm symptoms can be caused by various viral and bacterial infections including: Acute bronchitis ...\n",
            "Result 3: 18865_2 (Score: 32.3317)\n",
            "Text: Pneumonia is inflammation of the lung usually caused by bacterial or viral infection (rarely, also b...\n",
            "Result 4: 10907_0 (Score: 31.2688)\n",
            "Text: Pneumonia — A condition caused by bacterial or viral infection that is characterized by inflammation...\n",
            "Result 5: 18247_6 (Score: 29.6541)\n",
            "Text: Pneumonia is inflammation of the lung usually caused by bacterial or viral infection (rarely, also b...\n",
            "\n",
            "--- Verifying MRR Calculation ---\n",
            "Run format for MRR calculation: <class 'dict'>\n",
            "Example run entry: ('10907', {'10907_3': 0.9999999993945393, '10907_1': 0.6671295885883667, '18865_2': 0.6369197106144395, '10907_0': 0.5725646892667525, '18247_6': 0.474801926900325, '13639_5': 0.3047857492696342, '10907_4': 0.3047857492696342, '10907_2': 0.2675955833956058, '10907_5': 0.2343421751581222, '18247_3': 0.22323603284948734})\n",
            "Queries processed: 1\n",
            "Queries with relevant docs: 1\n",
            "Queries with relevant docs retrieved: 1\n",
            "Test MRR@10: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估不同检索策略（小样本）\n",
        "print(\"Evaluating retrieval strategies on a small sample...\")\n",
        "evaluation_results = evaluate_strategies(\n",
        "    max_queries=5,  # 使用较小的样本量便于快速运行\n",
        "    k_values=[10, 50, 100],\n",
        "    strategies=[\"single\", \"sbert_bm25\"]  # 只评估两种最重要的策略\n",
        ")\n",
        "\n",
        "# 显示结果摘要\n",
        "print(\"\\nResults summary:\")\n",
        "for strategy, results in evaluation_results.items():\n",
        "    print(f\"\\nStrategy: {strategy}\")\n",
        "    for k, metrics in results.items():\n",
        "        print(f\"  k={k}: MRR={metrics['mrr']:.4f}, Recall={metrics['recall']:.4f}\" +\n",
        "              (f\", NDCG={metrics['ndcg']:.4f}\" if 'ndcg' in metrics and metrics['ndcg'] > 0 else \"\"))"
      ],
      "metadata": {
        "id": "RzJSebrYa26r",
        "outputId": "a87dc165-1c26-4487-84b2-7f7d9852a6d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating retrieval strategies on a small sample...\n",
            "Evaluating 5 queries across 2 strategies\n",
            "Queries with relevance judgments: 5/5\n",
            "\n",
            "Evaluating strategy: single\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with single: 100%|██████████| 5/5 [00:00<00:00, 78.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  k=10: MRR=0.0000, Recall=0.0000, NDCG=0.1588\n",
            "  k=50: MRR=0.0000, Recall=0.0000, NDCG=0.1588\n",
            "  k=100: MRR=0.0107, Recall=1.0000, NDCG=0.1588\n",
            "\n",
            "Evaluating strategy: sbert_bm25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with sbert_bm25: 100%|██████████| 5/5 [00:00<00:00, 14.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  k=10: MRR=0.0000, Recall=0.0000, NDCG=0.0000\n",
            "  k=50: MRR=0.0000, Recall=0.0000, NDCG=0.0000\n",
            "  k=100: MRR=0.0000, Recall=0.0000, NDCG=0.0000\n",
            "\n",
            "Summary of Results:\n",
            "\n",
            "single:\n",
            "  k=10: mrr=0.0000, recall=0.0000, ndcg=0.1588\n",
            "  k=50: mrr=0.0000, recall=0.0000, ndcg=0.1588\n",
            "  k=100: mrr=0.0107, recall=1.0000, ndcg=0.1588\n",
            "\n",
            "sbert_bm25:\n",
            "  k=10: mrr=0.0000, recall=0.0000, ndcg=0.0000\n",
            "  k=50: mrr=0.0000, recall=0.0000, ndcg=0.0000\n",
            "  k=100: mrr=0.0000, recall=0.0000, ndcg=0.0000\n",
            "\n",
            "Results summary:\n",
            "\n",
            "Strategy: single\n",
            "  k=10: MRR=0.0000, Recall=0.0000, NDCG=0.1588\n",
            "  k=50: MRR=0.0000, Recall=0.0000, NDCG=0.1588\n",
            "  k=100: MRR=0.0107, Recall=1.0000, NDCG=0.1588\n",
            "\n",
            "Strategy: sbert_bm25\n",
            "  k=10: MRR=0.0000, Recall=0.0000\n",
            "  k=50: MRR=0.0000, Recall=0.0000\n",
            "  k=100: MRR=0.0000, Recall=0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试alpha参数调优（使用非常小的样本以便快速运行）\n",
        "print(\"Testing alpha parameter tuning on a very small sample...\")\n",
        "mini_alpha_results = tune_alpha_parameter(sample_size=3)"
      ],
      "metadata": {
        "id": "8I-YBvQQa63G",
        "outputId": "02604cf8-2316-4cff-98eb-0784d97960df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing alpha parameter tuning on a very small sample...\n",
            "Testing sbert_bm25 strategy with different alpha values...\n",
            "\n",
            "Alpha = 0.0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=0.0: 100%|██████████| 3/3 [00:00<00:00, 11.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.1944, Recall@100=1.0000\n",
            "\n",
            "Alpha = 0.2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=0.2: 100%|██████████| 3/3 [00:00<00:00, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.1784, Recall@100=1.0000\n",
            "\n",
            "Alpha = 0.4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=0.4: 100%|██████████| 3/3 [00:00<00:00, 13.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.0998, Recall@100=1.0000\n",
            "\n",
            "Alpha = 0.5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=0.5: 100%|██████████| 3/3 [00:00<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.0078, Recall@100=0.3333\n",
            "\n",
            "Alpha = 0.6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=0.6: 100%|██████████| 3/3 [00:00<00:00, 13.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.0000, Recall@100=0.0000\n",
            "\n",
            "Alpha = 0.8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=0.8: 100%|██████████| 3/3 [00:00<00:00, 13.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.0000, Recall@100=0.0000\n",
            "\n",
            "Alpha = 1.0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries with alpha=1.0: 100%|██████████| 3/3 [00:00<00:00, 13.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MRR@100=0.0000, Recall@100=0.0000\n",
            "\n",
            "Summary of Alpha Results for sbert_bm25 strategy:\n",
            "Alpha\tMRR@100\tRecall@100\n",
            "0.0\t0.1944\t1.0000\n",
            "0.2\t0.1784\t1.0000\n",
            "0.4\t0.0998\t1.0000\n",
            "0.5\t0.0078\t0.3333\n",
            "0.6\t0.0000\t0.0000\n",
            "0.8\t0.0000\t0.0000\n",
            "1.0\t0.0000\t0.0000\n",
            "\n",
            "Best alpha value for MRR: 0.0 (MRR@100=0.1944)\n",
            "Best alpha value for Recall: 0.0 (Recall@100=1.0000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jh0qgBwa-4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}