{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# SBERT Fine-tuning for Social Media Retrieval\n",
    "Colab-optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initial-setup"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Environment\n",
    "!pip install -q sentence-transformers torch faiss-gpu\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set project paths\n",
    "import os\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/CS6120_project\"\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# GPU monitoring\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-loading"
   },
   "outputs": [],
   "source": [
    "# 2. Data Preparation\n",
    "from src.data_preparation import DataPreprocessor\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "combined_path = Path(\"data/processed/combined.json\")\n",
    "\n",
    "# Process data if needed\n",
    "if not combined_path.exists():\n",
    "    print(\"Processing datasets...\")\n",
    "    try:\n",
    "        # Load MSMARCO from HuggingFace\n",
    "        from datasets import load_dataset\n",
    "        msmarco = load_dataset(\"microsoft/ms_marco\", \"v1.1\")\n",
    "        msmarco_df = pd.DataFrame([{\n",
    "            'text': doc['passages']['passage_text'][0]\n",
    "        } for doc in msmarco['train'] if 'passages' in doc])\n",
    "        \n",
    "        # Process and combine datasets\n",
    "        preprocessor.process_msmarco(msmarco_df)\n",
    "        preprocessor.process_twitter(Path(\"data/raw/twitter.zip\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load training data\n",
    "with open(combined_path) as f:\n",
    "    train_data = json.load(f)['train']\n",
    "    \n",
    "print(f\"Loaded {len(train_data)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-training"
   },
   "outputs": [],
   "source": [
    "# 3. Model Training (adapted from src/model_training.py)\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"all-mpnet-base-v2\"\n",
    "BATCH_SIZE = 64 if torch.cuda.is_available() else 16\n",
    "EPOCHS = 3\n",
    "\n",
    "# Initialize model\n",
    "model = SentenceTransformer(BASE_MODEL)\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training setup\n",
    "from sentence_transformers import InputExample\n",
    "train_examples = [\n",
    "    InputExample(texts=[text, text], label=1.0) \n",
    "    for text in train_data[:10000]  # Limit for demo\n",
    "]\n",
    "train_dataloader = DataLoader(train_examples, batch_size=BATCH_SIZE)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Training configuration\n",
    "warmup_steps = 100\n",
    "optimizer_params = {'lr': 2e-5}\n",
    "\n",
    "# Training loop with full configuration\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    optimizer_params=optimizer_params,\n",
    "    output_path=\"models/sbert_model\",\n",
    "    checkpoint_path=\"models/checkpoints\",\n",
    "    checkpoint_save_steps=1000,\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fallback-setup"
   },
   "outputs": [],
   "source": [
    "# 4. Fallback Model Setup\n",
    "if torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() > 0.8:\n",
    "    print(\"Switching to RoBERTa-base for memory efficiency\")\n",
    "    model = SentenceTransformer(\"roberta-base\")\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,  # Shorter training for fallback\n",
    "        output_path=\"models/fallback\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation"
   },
   "outputs": [],
   "source": [
    "# 5. Validation\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "# Load test data\n",
    "with open(\"data/processed/combined.json\") as f:\n",
    "    test_data = json.load(f)[\"test\"]\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    sentences1=test_data[:1000],\n",
    "    sentences2=test_data[:1000],\n",
    "    scores=[1.0]*1000  # Perfect similarity for demo\n",
    ")\n",
    "\n",
    "evaluator(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
